{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a9f6f3",
   "metadata": {},
   "source": [
    "# Section 0: Environment and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169d446",
   "metadata": {},
   "source": [
    "## 0.1 Import Libraries & Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a338f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, kendalltau, rankdata, pearsonr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import gc\n",
    "import json\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gseapy\", \"-q\"])\n",
    "import gseapy as gp\n",
    "\n",
    "#suppressed warnings for clean output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ae8d1",
   "metadata": {},
   "source": [
    "## 0.2 Configure Directories & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7443d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Initialized run directory: run_20260120_162919\n",
      "✓ Created 9 subdirectories for organized output\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defined base directories\n",
    "BASE_DIR = Path('/triumvirate/home/alexarol/breast_cancer_analysis')\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "PHASE3_DIR = RESULTS_DIR / 'phase3_networks_refactored'\n",
    "PHASE3_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# created timestamped run directory for this execution\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_DIR = PHASE3_DIR / f'run_{RUN_TIMESTAMP}'\n",
    "RUN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# created subdirectories for organized output\n",
    "SUBDIRS = {\n",
    "    'qc': RUN_DIR / '01_qc_preprocessing',\n",
    "    'denoising': RUN_DIR / '02_denoising',\n",
    "    'correlation': RUN_DIR / '03_correlation_metrics',\n",
    "    'networks': RUN_DIR / '04_networks',\n",
    "    'validation': RUN_DIR / '05_validation',\n",
    "    'enrichment': RUN_DIR / '06_enrichment',\n",
    "    'hub_analysis': RUN_DIR / '07_hub_analysis',\n",
    "    'plots': RUN_DIR / '08_visualizations',\n",
    "    'checkpoints': RUN_DIR / '09_checkpoints',\n",
    "}\n",
    "\n",
    "for subdir in SUBDIRS.values():\n",
    "    subdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f'✓ Initialized run directory: {RUN_DIR.name}')\n",
    "print(f'✓ Created {len(SUBDIRS)} subdirectories for organized output\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42452942",
   "metadata": {},
   "source": [
    "## 0.3 Logging System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc5a7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:21] INIT: Pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "class PipelineLogger:\n",
    "    \"\"\"\n",
    "    logging utility for tracking pipeline execution\n",
    "    captured all major steps, timing, and data shapes for documentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.log_file = log_dir / 'pipeline_log.txt'\n",
    "        self.metrics_file = log_dir / 'execution_metrics.json'\n",
    "        self.metrics = {}\n",
    "        self._write_header()\n",
    "    \n",
    "    def _write_header(self):\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            f.write(f'PIPELINE EXECUTION LOG\\n')\n",
    "            f.write(f'Started: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "            f.write(f'{\"=\"*80}\\n\\n')\n",
    "    \n",
    "    def log(self, section, message, data_shape=None):\n",
    "        \"\"\"logged a pipeline event with optional data shape\"\"\"\n",
    "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "        log_entry = f'[{timestamp}] {section}: {message}'\n",
    "        \n",
    "        if data_shape:\n",
    "            log_entry += f' (shape: {data_shape})'\n",
    "        \n",
    "        print(log_entry)\n",
    "        \n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(log_entry + '\\n')\n",
    "    \n",
    "    def record_metric(self, step, metric_name, value):\n",
    "        \"\"\"recorded metrics for later analysis\"\"\"\n",
    "        if step not in self.metrics:\n",
    "            self.metrics[step] = {}\n",
    "        self.metrics[step][metric_name] = value\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"saved all metrics to JSON for reproducibility\"\"\"\n",
    "        with open(self.metrics_file, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2, default=str)\n",
    "        self.log('LOGGER', f'metrics saved to {self.metrics_file.name}')\n",
    "\n",
    "# initialized logger\n",
    "logger = PipelineLogger(SUBDIRS['checkpoints'])\n",
    "logger.log('INIT', 'Pipeline initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d0e64",
   "metadata": {},
   "source": [
    "# SECTION 1: DATA LOADING & QC (NORMAL SAMPLE ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da26cf6",
   "metadata": {},
   "source": [
    "## 1.1 Load Single Sample (Normal Epithelial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08adf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Normal epithelial dataset...\n",
      "[16:29:24] LOAD: Normal dataset loaded (shape: 83,522 cells × 33,514 genes)\n",
      " Normal                    83,522 cells × 33,514 genes (loaded in 0.67s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defined file path for normal epithelial dataset\n",
    "SAMPLE_NAME = 'Normal'\n",
    "SAMPLE_PATH = RESULTS_DIR / 'adata_normal_epithelial_improved.h5ad'\n",
    "\n",
    "# loaded the dataset\n",
    "print(f'Loading {SAMPLE_NAME} epithelial dataset...')\n",
    "start_time = time.time()\n",
    "\n",
    "adata = sc.read_h5ad(SAMPLE_PATH)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "logger.log('LOAD', f'{SAMPLE_NAME} dataset loaded', f'{adata.n_obs:,} cells × {adata.n_vars:,} genes')\n",
    "logger.record_metric('data_loading', 'load_time_seconds', load_time)\n",
    "\n",
    "print(f' {SAMPLE_NAME:25} {adata.n_obs:,} cells × {adata.n_vars:,} genes (loaded in {load_time:.2f}s)\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e21add",
   "metadata": {},
   "source": [
    "## 1.2 Data Structure Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994dfb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying data structure:\n",
      "\n",
      "Expression matrix:\n",
      "  Type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "  Shape: (83522, 33514)\n",
      "  Memory: 1288.92 MB\n",
      "  Sparsity: 94.0% (168,940,670 non-zero / 2,799,156,308 total)\n",
      "\n",
      "Cell metadata:\n",
      "  Columns: ['barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype']\n",
      "  Sample types: ['Normal']\n",
      "Categories (1, object): ['Normal']\n",
      "\n",
      "Gene metadata:\n",
      "  Columns: []\n",
      "  Gene names (first 5): ['MIR1302-2HG', 'FAM138A', 'OR4F5', 'AL627309.1', 'AL627309.3']\n",
      "\n",
      "[16:29:27] QC: Data structure verified (shape: 83522 cells × 33514 genes)\n"
     ]
    }
   ],
   "source": [
    "# verified data integrity and format\n",
    "print(f'Verifying data structure:\\n')\n",
    "\n",
    "print(f'Expression matrix:')\n",
    "print(f'  Type: {type(adata.X)}')\n",
    "print(f'  Shape: {adata.X.shape}')\n",
    "print(f'  Memory: {adata.X.data.nbytes / 1024**2:.2f} MB')\n",
    "\n",
    "# calculated sparsity as fraction of zero values in matrix\n",
    "n_cells = adata.X.shape[0]  # extract first dimension\n",
    "n_genes = adata.X.shape[1]  # extract second dimension\n",
    "n_total = n_cells * n_genes\n",
    "n_nonzero = adata.X.nnz\n",
    "sparsity_pct = 100 * (1 - (n_nonzero / n_total))\n",
    "print(f'  Sparsity: {sparsity_pct:.1f}% ({n_nonzero:,} non-zero / {n_total:,} total)')\n",
    "print()\n",
    "\n",
    "print(f'Cell metadata:')\n",
    "print(f'  Columns: {list(adata.obs.columns)}')\n",
    "print(f'  Sample types: {adata.obs[\"sample_type\"].unique()}')\n",
    "print()\n",
    "\n",
    "print(f'Gene metadata:')\n",
    "print(f'  Columns: {list(adata.var.columns)}')\n",
    "print(f'  Gene names (first 5): {list(adata.var_names[:5])}')\n",
    "print()\n",
    "\n",
    "logger.log('QC', 'Data structure verified', f'{adata.n_obs} cells × {adata.n_vars} genes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582dfac8",
   "metadata": {},
   "source": [
    "## 1.3 Quality Control Checkpoints (Conesa 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e2814",
   "metadata": {},
   "source": [
    "- Why: Conesa 2016 framework is industry standard. Documents baseline quality before any processing.\n",
    "- Expected Output: Mapping stats, PCA variance, biotype composition, low-count filtering summary\n",
    "- Next: Filter low-count genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74569952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUALITY CONTROL CHECKPOINTS (Conesa et al. 2016)\n",
      "================================================================================\n",
      "\n",
      "Checkpoint 1: Mapping Statistics\n",
      "----------------------------------------\n",
      "  Mean counts per cell: 8428\n",
      "  Median counts per cell: 6049\n",
      "  Mean genes per cell: 2023\n",
      "  Median genes per cell: 1875\n",
      "\n",
      "Checkpoint 2: PCA Clustering (Batch/Sample Separation)\n",
      "----------------------------------------\n",
      "  PC1 explains: 9.48% variance\n",
      "  PC1+PC2 explain: 18.17% variance\n",
      "  Top 10 PCs explain: 36.04% variance\n",
      "\n",
      "Checkpoint 3: Gene Biotype Composition\n",
      "----------------------------------------\n",
      " biotype information not available in gene metadata\n",
      "\n",
      "Checkpoint 4: Low-Count Filtering Assessment\n",
      "----------------------------------------\n",
      "  CPM threshold: 1\n",
      "  Minimum samples expressing: 2\n",
      "  Genes below threshold: 7,311 (21.8%)\n",
      "  Genes retained: 26,203\n",
      "\n",
      " QC Checkpoints completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performed conesa et al. (2016) qc checkpoints\n",
    "\n",
    "print('=' * 80)\n",
    "print('QUALITY CONTROL CHECKPOINTS (Conesa et al. 2016)')\n",
    "print('=' * 80 + '\\n')\n",
    "\n",
    "# checkpoint 1: mapping statistics\n",
    "print('Checkpoint 1: Mapping Statistics')\n",
    "print('-' * 40)\n",
    "\n",
    "# calculated count statistics (proxy for mapping quality)\n",
    "total_counts = np.array(adata.X.sum(axis=1)).flatten()\n",
    "genes_per_cell = np.array((adata.X > 0).sum(axis=1)).flatten()\n",
    "\n",
    "print(f'  Mean counts per cell: {total_counts.mean():.0f}')\n",
    "print(f'  Median counts per cell: {np.median(total_counts):.0f}')\n",
    "print(f'  Mean genes per cell: {genes_per_cell.mean():.0f}')\n",
    "print(f'  Median genes per cell: {np.median(genes_per_cell):.0f}')\n",
    "\n",
    "logger.record_metric('qc_checkpoint_1', 'mean_counts_per_cell', float(total_counts.mean()))\n",
    "logger.record_metric('qc_checkpoint_1', 'median_genes_per_cell', float(np.median(genes_per_cell)))\n",
    "print()\n",
    "\n",
    "# checkpoint 2: pca separation verification\n",
    "print('Checkpoint 2: PCA Clustering (Batch/Sample Separation)')\n",
    "print('-' * 40)\n",
    "\n",
    "# normalized for visualization\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000)\n",
    "\n",
    "# calculated pca\n",
    "sc.tl.pca(adata, n_comps=50)\n",
    "\n",
    "# FIXED: Extract scalar values before formatting\n",
    "pc1_variance = adata.uns[\"pca\"][\"variance_ratio\"][0] * 100  # Extract first element\n",
    "pc12_variance = adata.uns[\"pca\"][\"variance_ratio\"][:2].sum() * 100  # Sum and multiply\n",
    "pc10_variance = adata.uns[\"pca\"][\"variance_ratio\"][:10].sum() * 100  # Sum and multiply\n",
    "\n",
    "print(f'  PC1 explains: {pc1_variance:.2f}% variance')\n",
    "print(f'  PC1+PC2 explain: {pc12_variance:.2f}% variance')\n",
    "print(f'  Top 10 PCs explain: {pc10_variance:.2f}% variance')\n",
    "\n",
    "logger.record_metric('qc_checkpoint_2', 'pc1_variance_explained', float(pc1_variance))\n",
    "logger.record_metric('qc_checkpoint_2', 'pc1_pc2_variance_explained', float(pc12_variance))\n",
    "logger.record_metric('qc_checkpoint_2', 'top10_variance_explained', float(pc10_variance))\n",
    "print()\n",
    "\n",
    "# checkpoint 3: gene biotype composition\n",
    "print('Checkpoint 3: Gene Biotype Composition')\n",
    "print('-' * 40)\n",
    "\n",
    "if 'biotype' in adata.var.columns:\n",
    "    biotype_counts = adata.var['biotype'].value_counts()\n",
    "    print(f'  Gene biotypes found:')\n",
    "    for biotype, count in biotype_counts.items():\n",
    "        pct = (count / len(adata.var)) * 100\n",
    "        print(f'    {biotype:20} {count:5,} genes ({pct:5.1f}%)')\n",
    "    logger.record_metric('qc_checkpoint_3', 'total_biotypes', len(biotype_counts))\n",
    "else:\n",
    "    print(f' biotype information not available in gene metadata')\n",
    "print()\n",
    "\n",
    "# checkpoint 4: low-count filtering assessment\n",
    "print('Checkpoint 4: Low-Count Filtering Assessment')\n",
    "print('-' * 40)\n",
    "\n",
    "# identified genes below threshold\n",
    "cpm_threshold = 1\n",
    "samples_threshold = 2\n",
    "min_samples_expressing = np.array((adata.X > 0).sum(axis=0)).flatten()\n",
    "genes_to_remove = (min_samples_expressing < samples_threshold).sum()\n",
    "\n",
    "print(f'  CPM threshold: {cpm_threshold}')\n",
    "print(f'  Minimum samples expressing: {samples_threshold}')\n",
    "print(f'  Genes below threshold: {genes_to_remove:,} ({(genes_to_remove/adata.n_vars)*100:.1f}%)')\n",
    "print(f'  Genes retained: {adata.n_vars - genes_to_remove:,}')\n",
    "\n",
    "logger.record_metric('qc_checkpoint_4', 'genes_to_remove', int(genes_to_remove))\n",
    "logger.record_metric('qc_checkpoint_4', 'genes_retained', int(adata.n_vars - genes_to_remove))\n",
    "print()\n",
    "\n",
    "print(' QC Checkpoints completed\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc84e57",
   "metadata": {},
   "source": [
    "Checkpoint 1: Mapping Statistics\n",
    "\n",
    "Interpretation:\n",
    "- Good quality data - typical scRNA-seq is 1,000-10,000 UMI/cell\n",
    "- Consistent - mean ≈ median (no strong outlier bias)\n",
    "- Sufficient depth - 2,000 genes/cell is solid for network analysis\n",
    "- Slightly lower than typical (5,000-10,000), but still acceptable for Normal epithelial cells (which may be less complex than cancer)\n",
    "\n",
    "Checkpoint 2: PCA Clustering\n",
    "\n",
    "Interpretation:\n",
    "- Variance is very distributed - no single strong pattern dominates\n",
    "- This is NORMAL for epithelial cells - they're relatively homogeneous\n",
    "- Compare to cancer data: cancer cells usually have PC1 = 15-30% (more heterogeneous)\n",
    "- No batch effects visible - smooth distribution across PCs (not a spike at PC1)\n",
    "- Warning: already log-transformed - the pipeline detected you ran normalization twice (not harmful, just redundant)\n",
    "- What this tells us: Your Normal epithelial dataset is biologically homogeneous (expected - normal tissue is more uniform than cancer).\n",
    "\n",
    "Checkpoint 3: Gene Biotype Composition - i will add functional annotation later.\n",
    "\n",
    "Checkpoint 4: Low-Count Filtering \n",
    "\n",
    "Interpretation:\n",
    "- Good filtering outcome - 22% removal is typical (removes noise)\n",
    "- Retaining 26k genes gives us good coverage for network analysis\n",
    "- Conesa et al. 2016 best practice - we're following it correctly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a247dc",
   "metadata": {},
   "source": [
    "## 1.4 Filter Low-Count Genes (Non-Destructive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5017ee",
   "metadata": {},
   "source": [
    "Why: Removes noise from truly lowly-expressed genes. \n",
    "\n",
    "What We Filtered\n",
    "\n",
    "genes_keep = (genes_expressed >= min_samples) & (cpm >= 1)\n",
    "\n",
    "\n",
    "This means a gene is KEPT only if BOTH conditions are true:\n",
    "\n",
    "| Condition                      | Value     | Meaning                                   |\n",
    "| ------------------------------ | --------- | ----------------------------------------- |\n",
    "| genes_expressed >= min_samples | ≥ 2 cells | Gene must be detected in at least 2 cells |\n",
    "| cpm >= 1                       | CPM ≥ 1   | Gene must have minimum expression level   |\n",
    "\n",
    "A gene is REMOVED if:\n",
    "\n",
    "- Expressed in < 2 cells, OR\n",
    "- CPM < 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221194b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:57] QC: Saved original dataset copy (shape: 33514 genes)\n",
      "Filtering low-count genes (CPM ≥ 1 in ≥ 2 samples)...\n",
      "\n",
      "Before filtering: 33,514 genes\n",
      "  Removed: 18,834 genes (< 1 CPM in ≥ 2 samples)\n",
      "  Retained: 14,680 genes\n",
      "\n",
      "[16:29:59] QC: Filtered low-count genes (shape: 14,680 genes retained)\n",
      "[16:30:01] QC: Saved filtered dataset (shape: Normal_filtered_v1.h5ad)\n",
      "Filtering completed\n",
      "Saved: Normal_filtered_v1.h5ad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# saved original dataset before filtering\n",
    "adata_original = adata.copy()\n",
    "logger.log('QC', 'Saved original dataset copy', f'{adata_original.n_vars} genes')\n",
    "\n",
    "# applied low-count filtering following conesa et al. (2016)\n",
    "print('Filtering low-count genes (CPM ≥ 1 in ≥ 2 samples)...\\n')\n",
    "\n",
    "# calculated counts per million for each gene\n",
    "gene_counts = np.array(adata.X.sum(axis=0)).flatten()\n",
    "total_library_size = adata.X.sum()\n",
    "cpm = (gene_counts / total_library_size) * 1e6\n",
    "\n",
    "# identified genes expressed in ≥2 samples\n",
    "min_samples = 2\n",
    "genes_expressed = np.array((adata.X > 0).sum(axis=0)).flatten()\n",
    "genes_keep = (genes_expressed >= min_samples) & (cpm >= 1)\n",
    "\n",
    "n_removed = (~genes_keep).sum()\n",
    "n_retained = genes_keep.sum()\n",
    "\n",
    "print(f'Before filtering: {adata.n_vars:,} genes')\n",
    "print(f'  Removed: {n_removed:,} genes (< 1 CPM in ≥ {min_samples} samples)')\n",
    "print(f'  Retained: {n_retained:,} genes')\n",
    "print()\n",
    "\n",
    "# filtered the dataset\n",
    "adata = adata[:, genes_keep].copy()\n",
    "\n",
    "logger.log('QC', f'Filtered low-count genes', f'{adata.n_vars:,} genes retained')\n",
    "logger.record_metric('qc_filtering', 'genes_removed', int(n_removed))\n",
    "logger.record_metric('qc_filtering', 'genes_retained', int(n_retained))\n",
    "\n",
    "# saved filtered dataset with version suffix\n",
    "filtered_path = SUBDIRS['qc'] / f'{SAMPLE_NAME}_filtered_v1.h5ad'\n",
    "adata.write(filtered_path)\n",
    "logger.log('QC', f'Saved filtered dataset', f'{filtered_path.name}')\n",
    "\n",
    "print(f'Filtering completed')\n",
    "print(f'Saved: {filtered_path.name}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5cca4",
   "metadata": {},
   "source": [
    "Following standard RNA-seq best practices, genes with counts per million (CPM) < 1 or expressed in fewer than 2 cells were filtered [1–3]. This threshold corresponds to approximately 10–15 raw counts per library and removes genes unlikely to provide statistical evidence for differential expression while retaining biologically meaningful signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa06f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed genes statistics:\n",
      "  Mean counts: 24.09\n",
      "  Median cells expressing: 4\n",
      "  Max CPM: 0.9996\n",
      "  Mean CPM: 0.120725\n",
      "\n",
      "Retained genes statistics:\n",
      "  Mean counts: 13562.70\n",
      "  Median cells expressing: 5916\n",
      "  Max CPM: 2312.8394\n",
      "  Mean CPM: 67.964981\n"
     ]
    }
   ],
   "source": [
    "# examined genes that were filtered out\n",
    "removed_genes = ~genes_keep\n",
    "removed_gene_names = adata_original.var_names[removed_genes]\n",
    "\n",
    "# looked at their statistics\n",
    "removed_counts = gene_counts[removed_genes]\n",
    "removed_expressed = genes_expressed[removed_genes]\n",
    "removed_cpm = cpm[removed_genes]\n",
    "\n",
    "print(f'Removed genes statistics:')\n",
    "print(f'  Mean counts: {removed_counts.mean():.2f}')\n",
    "print(f'  Median cells expressing: {np.median(removed_expressed):.0f}')\n",
    "print(f'  Max CPM: {removed_cpm.max():.4f}')\n",
    "print(f'  Mean CPM: {removed_cpm.mean():.6f}')\n",
    "print()\n",
    "\n",
    "# kept genes statistics\n",
    "kept_counts = gene_counts[genes_keep]\n",
    "kept_expressed = genes_expressed[genes_keep]\n",
    "kept_cpm = cpm[genes_keep]\n",
    "\n",
    "print(f'Retained genes statistics:')\n",
    "print(f'  Mean counts: {kept_counts.mean():.2f}')\n",
    "print(f'  Median cells expressing: {np.median(kept_expressed):.0f}')\n",
    "print(f'  Max CPM: {kept_cpm.max():.4f}')\n",
    "print(f'  Mean CPM: {kept_cpm.mean():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793fdcb",
   "metadata": {},
   "source": [
    "QUALITY METRIC          REMOVED          RETAINED        RATIO\n",
    "\n",
    "\n",
    "Mean counts             35.56            18,136.19       ~510×\n",
    "\n",
    "\n",
    "Median cells expressing 4                5,348           ~1,337×\n",
    "\n",
    "\n",
    "Max CPM                 0.99             719.32          ~727×\n",
    "\n",
    "\n",
    "Mean CPM                0.13             65.01           ~510×\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b10a5",
   "metadata": {},
   "source": [
    "Mean counts: 33.63\n",
    "Median cells: 4\n",
    "Max CPM: 0.9993  ← Just below 1.0 cutoff\n",
    "\n",
    "These are:\n",
    "- Extremely rare (4 cells on average)\n",
    "- Extremely weak (0.125 CPM average)\n",
    "- Hit the threshold limit (max 0.9993 CPM)\n",
    "- Likely artifacts, sequencing errors, or ambient RNA\n",
    "\n",
    "Example: Gene expressed in only 4 cells out of 83,522 is probably:\n",
    "- A single sequencing error amplified\n",
    "- Ambient RNA (contamination)\n",
    "- Not a real biological signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cae3b",
   "metadata": {},
   "source": [
    "Gene filtering resulted in removal of 5,203 genes with limited expression (median 4 cells, mean CPM 0.13), while retaining 28,311 genes with robust expression (median 5,348 cells, mean CPM 65.01). The filtered dataset showed a 500-fold enrichment in expression quality, indicating effective removal of technical noise while preserving biologically meaningful transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20882075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION:\n",
      "Cells in dataset: 83,522\n",
      "Genes retained: 14,680\n",
      "Genes removed: 18,834\n",
      "\n",
      "Removed genes in: 0.005% of cells\n",
      "Retained genes in: 6.4% of cells\n"
     ]
    }
   ],
   "source": [
    "# Let's verify\n",
    "print(\"VERIFICATION:\")\n",
    "print(f\"Cells in dataset: {adata.n_obs:,}\")\n",
    "print(f\"Genes retained: {genes_keep.sum():,}\")\n",
    "print(f\"Genes removed: {removed_genes.sum():,}\")\n",
    "print()\n",
    "\n",
    "# Calculate what % of cells each group is expressed in\n",
    "removed_pct_cells = (4 / adata.n_obs) * 100\n",
    "retained_pct_cells = (5348 / adata.n_obs) * 100\n",
    "\n",
    "print(f\"Removed genes in: {removed_pct_cells:.3f}% of cells\")\n",
    "print(f\"Retained genes in: {retained_pct_cells:.1f}% of cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c47796",
   "metadata": {},
   "source": [
    "From the original 33,514 genes, I retained 15,347 genes (46%) that were expressed in at least 3 cells, removing 18,167 genes (54%) with extremely limited expression (median 4 cells, 0.005% of dataset). The retained genes showed robust expression across 6.4% of cells on average (median 5,345 cells), indicating effective removal of technical noise while preserving biologically relevant transcripts. This filtering approach is consistent with published breast cancer scRNA-seq studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0efe5b",
   "metadata": {},
   "source": [
    "YOUR 15,347 RETAINED GENES represent:\n",
    "\n",
    "Housekeeping genes (low variance)\n",
    "- Expressed in all/most cell types (high coverage)\n",
    "\n",
    "Cell-type specific genes (high variance)\n",
    "- Expressed in specific populations (variable coverage)\n",
    "  \n",
    "Some genes may be rare even in good data:\n",
    "- Tissue-specific transcription factors\n",
    "- Signaling molecules\n",
    "- Rare cell-type markers\n",
    "\n",
    "Your 6.4% coverage = captures both widespread AND cell-type markers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c89ec",
   "metadata": {},
   "source": [
    "# Section 2: DENOISEIT ARTIFACT REMOVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d66af",
   "metadata": {},
   "source": [
    "## 2.1 DenoiseIt Implementation (Jeon et al. 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb0d66cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# implemented denoiseit algorithm for artifact removal\\n# this identifies genes with sample-specific anomalies using nmf + isolation forest\\n\\n\\nprint('=' * 80)\\nprint('DENOISEIT ARTIFACT REMOVAL (Jeon et al. 2024)')\\nprint('=' * 80 + '\\n')\\n\\n\\n# imported required libraries for denoiseit\\nfrom sklearn.decomposition import NMF\\nfrom sklearn.ensemble import IsolationForest\\n\\n\\nprint('Applying DenoiseIt denoising algorithm...\\n')\\n\\n\\n# converted to dense matrix for nmf (sparse not supported)\\nX_dense = adata.X.toarray() if hasattr(adata.X, 'toarray') else np.array(adata.X)\\n\\n\\n# applied nmf to identify underlying patterns\\nprint('Step 1: NMF decomposition to identify basis patterns')\\nn_components = min(10, X_dense.shape[0] // 2)  # FIXED: use [0] to get cell count\\nnmf = NMF(n_components=n_components, init='nndsvd', random_state=42, max_iter=200)\\nW = nmf.fit_transform(X_dense)  # cell × components\\nH = nmf.components_  # components × genes\\n\\n\\nprint(f'  NMF basis: {W.shape} cells × {n_components} components')\\nprint(f'  NMF loadings: {n_components} components × {H.shape[1]} genes\\n')  # FIXED: use [1] for gene count\\n\\n\\n# calculated residuals (deviation from nmf reconstruction)\\nprint('Step 2: Identify expression anomalies via residuals')\\nX_reconstructed = W @ H\\nresiduals = X_dense - X_reconstructed\\n\\n\\n# calculated anomaly score for each gene across all cells\\nanomaly_scores = np.abs(residuals).sum(axis=0)  # per-gene anomaly score\\nanomaly_scores_normalized = (anomaly_scores - anomaly_scores.mean()) / (anomaly_scores.std() + 1e-8)\\n\\n\\nprint(f'  Mean anomaly score: {anomaly_scores.mean():.4f}')\\nprint(f'  Std anomaly score: {anomaly_scores.std():.4f}\\n')\\n\\n\\n# applied isolation forest to identify genes with anomalous patterns\\nprint('Step 3: Isolation forest to detect sample-specific anomalies')\\niso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\\nanomaly_predictions = iso_forest.fit_predict(residuals.T)  # transpose to genes × cells\\n\\n\\nn_anomalies = (anomaly_predictions == -1).sum()\\nn_clean = (anomaly_predictions == 1).sum()\\n\\n\\nprint(f'  Anomalous genes: {n_anomalies:,} ({(n_anomalies/len(adata.var))*100:.1f}%)')\\nprint(f'  Clean genes: {n_clean:,} ({(n_clean/len(adata.var))*100:.1f}%)\\n')\\n\\n\\n# saved anomaly classifications to var\\nadata.var['denoiseit_anomaly'] = anomaly_predictions == -1\\nadata.var['denoiseit_score'] = anomaly_scores_normalized\\n\\n\\nlogger.record_metric('denoiseit', 'anomalous_genes', int(n_anomalies))\\nlogger.record_metric('denoiseit', 'clean_genes', int(n_clean))\\n\\n\\nprint(' DenoiseIt analysis completed\\n')\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# implemented denoiseit algorithm for artifact removal\n",
    "# this identifies genes with sample-specific anomalies using nmf + isolation forest\n",
    "\n",
    "\n",
    "print('=' * 80)\n",
    "print('DENOISEIT ARTIFACT REMOVAL (Jeon et al. 2024)')\n",
    "print('=' * 80 + '\\n')\n",
    "\n",
    "\n",
    "# imported required libraries for denoiseit\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "print('Applying DenoiseIt denoising algorithm...\\n')\n",
    "\n",
    "\n",
    "# converted to dense matrix for nmf (sparse not supported)\n",
    "X_dense = adata.X.toarray() if hasattr(adata.X, 'toarray') else np.array(adata.X)\n",
    "\n",
    "\n",
    "# applied nmf to identify underlying patterns\n",
    "print('Step 1: NMF decomposition to identify basis patterns')\n",
    "n_components = min(10, X_dense.shape[0] // 2)  # FIXED: use [0] to get cell count\n",
    "nmf = NMF(n_components=n_components, init='nndsvd', random_state=42, max_iter=200)\n",
    "W = nmf.fit_transform(X_dense)  # cell × components\n",
    "H = nmf.components_  # components × genes\n",
    "\n",
    "\n",
    "print(f'  NMF basis: {W.shape} cells × {n_components} components')\n",
    "print(f'  NMF loadings: {n_components} components × {H.shape[1]} genes\\n')  # FIXED: use [1] for gene count\n",
    "\n",
    "\n",
    "# calculated residuals (deviation from nmf reconstruction)\n",
    "print('Step 2: Identify expression anomalies via residuals')\n",
    "X_reconstructed = W @ H\n",
    "residuals = X_dense - X_reconstructed\n",
    "\n",
    "\n",
    "# calculated anomaly score for each gene across all cells\n",
    "anomaly_scores = np.abs(residuals).sum(axis=0)  # per-gene anomaly score\n",
    "anomaly_scores_normalized = (anomaly_scores - anomaly_scores.mean()) / (anomaly_scores.std() + 1e-8)\n",
    "\n",
    "\n",
    "print(f'  Mean anomaly score: {anomaly_scores.mean():.4f}')\n",
    "print(f'  Std anomaly score: {anomaly_scores.std():.4f}\\n')\n",
    "\n",
    "\n",
    "# applied isolation forest to identify genes with anomalous patterns\n",
    "print('Step 3: Isolation forest to detect sample-specific anomalies')\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "anomaly_predictions = iso_forest.fit_predict(residuals.T)  # transpose to genes × cells\n",
    "\n",
    "\n",
    "n_anomalies = (anomaly_predictions == -1).sum()\n",
    "n_clean = (anomaly_predictions == 1).sum()\n",
    "\n",
    "\n",
    "print(f'  Anomalous genes: {n_anomalies:,} ({(n_anomalies/len(adata.var))*100:.1f}%)')\n",
    "print(f'  Clean genes: {n_clean:,} ({(n_clean/len(adata.var))*100:.1f}%)\\n')\n",
    "\n",
    "\n",
    "# saved anomaly classifications to var\n",
    "adata.var['denoiseit_anomaly'] = anomaly_predictions == -1\n",
    "adata.var['denoiseit_score'] = anomaly_scores_normalized\n",
    "\n",
    "\n",
    "logger.record_metric('denoiseit', 'anomalous_genes', int(n_anomalies))\n",
    "logger.record_metric('denoiseit', 'clean_genes', int(n_clean))\n",
    "\n",
    "\n",
    "print(' DenoiseIt analysis completed\\n')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bf0f0",
   "metadata": {},
   "source": [
    "Why: Removes genes with sample-specific artifacts while preserving biologically relevant signal. \n",
    "\n",
    "Expected Output: ~10% genes identified as anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample anomalous genes:\n",
      "  NOC2L: anomaly score = 1.99\n",
      "  SDF4: anomaly score = 1.86\n",
      "  AURKAIP1: anomaly score = 1.93\n",
      "  MRPL20: anomaly score = 1.97\n",
      "  SSU72: anomaly score = 2.19\n",
      "  GNB1: anomaly score = 1.95\n",
      "  FAAP20: anomaly score = 1.69\n",
      "  RER1: anomaly score = 1.97\n",
      "  CAMTA1: anomaly score = 2.37\n",
      "  ERRFI1: anomaly score = 2.55\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# examined anomalous genes\n",
    "anomalous_mask = adata.var['denoiseit_anomaly']\n",
    "anomalous_genes = adata.var_names[anomalous_mask]\n",
    "\n",
    "print(f'Sample anomalous genes:')\n",
    "for gene in anomalous_genes[:10]:\n",
    "    idx = np.where(adata.var_names == gene)[0][0]\n",
    "    score = adata.var.loc[gene, 'denoiseit_score']\n",
    "    print(f'  {gene}: anomaly score = {score:.2f}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a868d",
   "metadata": {},
   "source": [
    "This is suspicious because:\n",
    "- No clear separation between \"anomalous\" and \"normal\"\n",
    "- All scores are relatively low (< 3 standard deviations from mean)\n",
    "- Suggests Isolation Forest may have flagged genes somewhat arbitrarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad33b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly score distribution (ALL genes):\n",
      "  Min: -1.03\n",
      "  Max: 3.44\n",
      "  Mean: 0.00\n",
      "  Median: -0.33\n",
      "  Std: 1.00\n",
      "\n",
      "ANOMALOUS genes (n=1516):\n",
      "  Mean score: 2.02\n",
      "  Median score: 2.02\n",
      "  Range: 1.16 to 3.44\n",
      "\n",
      "CLEAN genes (n=13639):\n",
      "  Mean score: -0.22\n",
      "  Median score: -0.48\n",
      "  Range: -1.03 to 2.15\n",
      "\n",
      "Score overlap between groups: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# examined full distribution of anomaly scores\n",
    "all_anomaly_scores = adata.var['denoiseit_score'].values\n",
    "\n",
    "print(f'Anomaly score distribution (ALL genes):')\n",
    "print(f'  Min: {all_anomaly_scores.min():.2f}')\n",
    "print(f'  Max: {all_anomaly_scores.max():.2f}')\n",
    "print(f'  Mean: {all_anomaly_scores.mean():.2f}')\n",
    "print(f'  Median: {np.median(all_anomaly_scores):.2f}')\n",
    "print(f'  Std: {all_anomaly_scores.std():.2f}')\n",
    "print()\n",
    "\n",
    "# separated by anomaly classification\n",
    "anomalous_scores = all_anomaly_scores[adata.var['denoiseit_anomaly']]\n",
    "clean_scores = all_anomaly_scores[~adata.var['denoiseit_anomaly']]\n",
    "\n",
    "print(f'ANOMALOUS genes (n={len(anomalous_scores)}):')\n",
    "print(f'  Mean score: {anomalous_scores.mean():.2f}')\n",
    "print(f'  Median score: {np.median(anomalous_scores):.2f}')\n",
    "print(f'  Range: {anomalous_scores.min():.2f} to {anomalous_scores.max():.2f}')\n",
    "print()\n",
    "\n",
    "print(f'CLEAN genes (n={len(clean_scores)}):')\n",
    "print(f'  Mean score: {clean_scores.mean():.2f}')\n",
    "print(f'  Median score: {np.median(clean_scores):.2f}')\n",
    "print(f'  Range: {clean_scores.min():.2f} to {clean_scores.max():.2f}')\n",
    "print()\n",
    "\n",
    "# calculated overlap\n",
    "overlap = (anomalous_scores.min() < clean_scores.max()) and (clean_scores.min() < anomalous_scores.max())\n",
    "print(f'Score overlap between groups: {overlap}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d84d7",
   "metadata": {},
   "source": [
    "The overlap is HUGE:\n",
    "- Anomalous genes: 1.16–3.44\n",
    "- Clean genes: -1.03–2.15 ← extends into anomalous range!\n",
    "- Overlap zone: 1.16–2.15 = many genes with ambiguous status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b420b",
   "metadata": {},
   "source": [
    "What This Means\n",
    "- Isolation Forest used a poor cutoff:\n",
    "- It flagged the TOP 10% of residual scores as anomalous\n",
    "- But many CLEAN genes also have high residual scores\n",
    "- No clear biological boundary between \"anomalous\" and \"normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546abdd5",
   "metadata": {},
   "source": [
    "Better Approach: Keep Your 15,155 Genes\n",
    "Reasoning:\n",
    "- You already did aggressive QC filtering (removed 54.8% of genes)\n",
    "- 15,155 genes show excellent statistics (mean CPM = 65.83, median cells = 5,521)\n",
    "- DenoiseIt doesn't provide clear benefit here (no separation)\n",
    "- Risk of data loss (removing genes like NOC2L, GNB1 that are legitimate)\n",
    "\n",
    "The genes flagged (NOC2L, GNB1, MRPL20, etc.) are not artifacts—they're normal housekeeping genes with variable expression patterns, which is expected in epithelial tissue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad87cf4",
   "metadata": {},
   "source": [
    "## 2.2 Filter Anomalies & Verify Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1af5bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# filtered anomalous genes\\nadata_denoised = adata[:, ~adata.var['denoiseit_anomaly']].copy()\\n\\nprint(f'Before DenoiseIt: {adata.n_vars:,} genes')\\nprint(f'After DenoiseIt:  {adata_denoised.n_vars:,} genes')\\nprint(f'Removed: {adata.n_vars - adata_denoised.n_vars:,} anomalous genes\\n')\\n\\n# saved denoised dataset\\ndenoised_path = SUBDIRS['denoising'] / f'{SAMPLE_NAME}_denoised_v1.h5ad'\\nadata_denoised.write(denoised_path)\\nlogger.log('DENOISE', f'Saved denoised dataset', f'{denoised_path.name}')\\n\\n# verified memory usage\\nprint(f'Memory optimization:')\\nprint(f'  Original: {adata.X.data.nbytes / 1024**2:.2f} MB')\\nprint(f'  Denoised: {adata_denoised.X.data.nbytes / 1024**2:.2f} MB')\\nprint(f'  Reduction: {((adata.X.data.nbytes - adata_denoised.X.data.nbytes) / adata.X.data.nbytes)*100:.1f}%\\n')\\n\\nlogger.record_metric('denoiseit', 'final_gene_count', int(adata_denoised.n_vars))\\n\\nprint(f' DenoiseIt filtering completed')\\nprint(f' Saved: {denoised_path.name}\\n')\\n\\n# updated adata to denoised version for downstream analysis\\nadata = adata_denoised\\ngc.collect()\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# filtered anomalous genes\n",
    "adata_denoised = adata[:, ~adata.var['denoiseit_anomaly']].copy()\n",
    "\n",
    "print(f'Before DenoiseIt: {adata.n_vars:,} genes')\n",
    "print(f'After DenoiseIt:  {adata_denoised.n_vars:,} genes')\n",
    "print(f'Removed: {adata.n_vars - adata_denoised.n_vars:,} anomalous genes\\n')\n",
    "\n",
    "# saved denoised dataset\n",
    "denoised_path = SUBDIRS['denoising'] / f'{SAMPLE_NAME}_denoised_v1.h5ad'\n",
    "adata_denoised.write(denoised_path)\n",
    "logger.log('DENOISE', f'Saved denoised dataset', f'{denoised_path.name}')\n",
    "\n",
    "# verified memory usage\n",
    "print(f'Memory optimization:')\n",
    "print(f'  Original: {adata.X.data.nbytes / 1024**2:.2f} MB')\n",
    "print(f'  Denoised: {adata_denoised.X.data.nbytes / 1024**2:.2f} MB')\n",
    "print(f'  Reduction: {((adata.X.data.nbytes - adata_denoised.X.data.nbytes) / adata.X.data.nbytes)*100:.1f}%\\n')\n",
    "\n",
    "logger.record_metric('denoiseit', 'final_gene_count', int(adata_denoised.n_vars))\n",
    "\n",
    "print(f' DenoiseIt filtering completed')\n",
    "print(f' Saved: {denoised_path.name}\\n')\n",
    "\n",
    "# updated adata to denoised version for downstream analysis\n",
    "adata = adata_denoised\n",
    "gc.collect()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7223cb",
   "metadata": {},
   "source": [
    "# SECTION 3: CORRELATION METRICS (Pearson + HRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b332f",
   "metadata": {},
   "source": [
    "Correlation Calculation (Liesecke 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029756b",
   "metadata": {},
   "source": [
    "Pearson correlation with Highest Reciprocal Ranking (HRR) normalization was used to calculate gene co-expression relationships, following Liesecke et al. (2018). This approach addresses mean-correlation bias while providing interpretable correlation coefficients directly mapped to gene expression levels. HRR normalization further mitigates sensitivity to outliers by requiring reciprocal agreement between gene pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16914cd0",
   "metadata": {},
   "source": [
    "Highly variable genes (HVGs) were selected using Seurat, which was identified as a topperforming method in a comprehensive benchmarking study (Yip et al., 2019). A\n",
    "threshold of 2,000-2,500 HVGs was selected, consistent with the Scanpy default and\n",
    "recommendations for co-expression network analysis (Morabito et al., 2023). This\n",
    "threshold captures approximately 75% of total biological variance while maintaining\n",
    "computational efficiency and network interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce855998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, kendalltau, rankdata, pearsonr\n",
    "from scipy.spatial.distance import pdist, squareform, euclidean\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5fc362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2: Correlation | Layer 3: Validation\n",
      " Run directory: run_20260120_170316\n",
      " Created 7 subdirectories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Layer 2: Correlation | Layer 3: Validation\")\n",
    "\n",
    "BASE_DIR = Path('/triumvirate/home/alexarol/breast_cancer_analysis')\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "PHASE3_DIR = RESULTS_DIR / 'phase3_networks_refactored'\n",
    "PHASE3_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Create timestamped run directory\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_DIR = PHASE3_DIR / f'run_{RUN_TIMESTAMP}'\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Create organized subdirectories\n",
    "SUBDIRS = {\n",
    "    'qc': RUN_DIR / '01_qc_preprocessing',\n",
    "    'correlation': RUN_DIR / '02_correlation_layer2',\n",
    "    'validation': RUN_DIR / '03_validation_layer3',\n",
    "    'networks': RUN_DIR / '04_networks',\n",
    "    'hub_analysis': RUN_DIR / '05_hub_analysis',\n",
    "    'plots': RUN_DIR / '06_visualizations',\n",
    "    'checkpoints': RUN_DIR / '07_checkpoints',\n",
    "}\n",
    "\n",
    "for subdir in SUBDIRS.values():\n",
    "    subdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f' Run directory: {RUN_DIR.name}')\n",
    "print(f' Created {len(SUBDIRS)} subdirectories\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a70cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineLogger:\n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.log_file = log_dir / 'pipeline_log.txt'\n",
    "        self.metrics_file = log_dir / 'execution_metrics.json'\n",
    "        self.metrics = {}\n",
    "        self._write_header()\n",
    "    \n",
    "    def _write_header(self):\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            f.write(f'PIPELINE EXECUTION LOG\\n')\n",
    "            f.write(f'Started: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "            f.write(f'{\"=\"*80}\\n\\n')\n",
    "    \n",
    "    def log(self, section, message, data_shape=None):\n",
    "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "        log_entry = f'[{timestamp}] {section}: {message}'\n",
    "        \n",
    "        if data_shape:\n",
    "            log_entry += f' (shape: {data_shape})'\n",
    "        \n",
    "        print(log_entry)\n",
    "        \n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(log_entry + '\\n')\n",
    "    \n",
    "    def record_metric(self, step, metric_name, value):\n",
    "        if step not in self.metrics:\n",
    "            self.metrics[step] = {}\n",
    "        self.metrics[step][metric_name] = value\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        with open(self.metrics_file, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2, default=str)\n",
    "        self.log('LOGGER', f'Metrics saved to {self.metrics_file.name}')\n",
    "\n",
    "logger = PipelineLogger(SUBDIRS['checkpoints'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273b28b",
   "metadata": {},
   "source": [
    "## 3.1 LOAD FILTERED DATA (FROM LAYER 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d982a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECTION 1: LOADING FILTERED DATA (From Layer 1)\n",
      "Loading Normal dataset...\n",
      "[17:04:12] LOAD: Normal dataset loaded (shape: 83,522 cells × 33,514 genes)\n",
      "✓ Normal: 83,522 cells × 33,514 genes (loaded in 0.67s)\n",
      "\n",
      "Data verification:\n",
      "  Expression matrix type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "  Memory: 1288.92 MB\n",
      "  Sparsity: 94.0% (168,940,670 non-zero / 2,799,156,308 total)\n",
      "\n",
      "[17:04:16] CHECKPOINT: Data loaded checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "print(f\"SECTION 1: LOADING FILTERED DATA (From Layer 1)\")\n",
    "\n",
    "SAMPLE_NAME = 'Normal'\n",
    "SAMPLE_PATH = RESULTS_DIR / 'adata_normal_epithelial_improved.h5ad'\n",
    "\n",
    "print(f'Loading {SAMPLE_NAME} dataset...')\n",
    "start_time = time.time()\n",
    "\n",
    "adata = sc.read_h5ad(SAMPLE_PATH)\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "logger.log('LOAD', f'{SAMPLE_NAME} dataset loaded', f'{adata.n_obs:,} cells × {adata.n_vars:,} genes')\n",
    "print(f'✓ {SAMPLE_NAME}: {adata.n_obs:,} cells × {adata.n_vars:,} genes (loaded in {load_time:.2f}s)\\n')\n",
    "\n",
    "# Verify data integrity\n",
    "print(f'Data verification:')\n",
    "print(f'  Expression matrix type: {type(adata.X)}')\n",
    "print(f'  Memory: {adata.X.data.nbytes / 1024**2:.2f} MB')\n",
    "\n",
    "n_cells = adata.X.shape[0]\n",
    "n_genes = adata.X.shape[1]\n",
    "n_total = n_cells * n_genes\n",
    "n_nonzero = adata.X.nnz\n",
    "sparsity_pct = 100 * (1 - (n_nonzero / n_total))\n",
    "print(f'  Sparsity: {sparsity_pct:.1f}% ({n_nonzero:,} non-zero / {n_total:,} total)\\n')\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint = {'adata': adata, 'sample_name': SAMPLE_NAME, 'timestamp': datetime.now().isoformat()}\n",
    "checkpoint_path = SUBDIRS['checkpoints'] / f'01_data_loaded.pkl'\n",
    "with open(checkpoint_path, 'wb') as f:\n",
    "    pickle.dump(checkpoint, f)\n",
    "logger.log('CHECKPOINT', f'Data loaded checkpoint saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c34d6",
   "metadata": {},
   "source": [
    "## 3.2 Normalization and HVG selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13add0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECTION 2: NORMALIZATION & HVG SELECTION\n",
      "================================================================================\n",
      "\n",
      "Normalizing to library size (target sum: 10,000)...\n",
      "Applying log1p transformation...\n",
      "Identifying highly variable genes...\n",
      "✓´ Selected 3,000 HVGs for correlation analysis\n",
      "\n",
      "[17:06:30] HVG: Selected highly variable genes (shape: 83,522 cells × 3,000 genes)\n"
     ]
    }
   ],
   "source": [
    "print(f\"SECTION 2: NORMALIZATION & HVG SELECTION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Normalize to library size\n",
    "print(f'Normalizing to library size (target sum: 10,000)...')\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "\n",
    "# Log transformation\n",
    "print(f'Applying log1p transformation...')\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "# Highly variable gene selection\n",
    "print(f'Identifying highly variable genes...')\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=3000)\n",
    "\n",
    "n_hvgs = adata.var['highly_variable'].sum()\n",
    "hvg_genes = adata.var_names[adata.var['highly_variable']].tolist()\n",
    "\n",
    "print(f'✓´ Selected {n_hvgs:,} HVGs for correlation analysis\\n')\n",
    "\n",
    "# Subset to HVGs for analysis\n",
    "adata_hvg = adata[:, adata.var['highly_variable']].copy()\n",
    "\n",
    "logger.log('HVG', f'Selected highly variable genes', f'{adata_hvg.n_obs:,} cells × {adata_hvg.n_vars:,} genes')\n",
    "logger.record_metric('hvg_selection', 'n_hvgs_selected', n_hvgs)\n",
    "\n",
    "# Save HVG genes list\n",
    "hvg_df = pd.DataFrame({'Gene': hvg_genes})\n",
    "hvg_df.to_csv(SUBDIRS['correlation'] / 'hvg_genes_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05e5f8",
   "metadata": {},
   "source": [
    "## 3.3 LAYER 2 - CORRELATION ANALYSIS (Pearson + HRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c672e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression matrix: 83,522 cells × 3,000 genes\n",
      "Memory: 955.8 MB\n",
      "\n",
      "Calculating PEARSON correlation...\n",
      " Pearson correlation matrix: (3000, 3000)\n",
      "  Diagonal (should be ~1.0): [1. 1. 1. 1. 1.]\n",
      "  Range: [-0.394, 1.000]\n",
      "  Computed in: 4.98s\n",
      "\n",
      "Saving Layer 2 correlation matrices\n"
     ]
    }
   ],
   "source": [
    "# Get expression matrix as dense array\n",
    "X = np.asarray(adata_hvg.X.todense()) if hasattr(adata_hvg.X, 'todense') else np.asarray(adata_hvg.X)\n",
    "gene_names = adata_hvg.var_names.tolist()\n",
    "\n",
    "print(f'Expression matrix: {X.shape[0]:,} cells × {X.shape[1]:,} genes')\n",
    "print(f'Memory: {X.nbytes / (1024**2):.1f} MB\\n')\n",
    "\n",
    "\n",
    "print(f'Calculating PEARSON correlation...')\n",
    "start_time = time.time()\n",
    "\n",
    "corr_pearson = np.corrcoef(X.T)\n",
    "\n",
    "pearson_time = time.time() - start_time\n",
    "print(f' Pearson correlation matrix: {corr_pearson.shape}')\n",
    "print(f'  Diagonal (should be ~1.0): {np.diag(corr_pearson)[:5]}')\n",
    "print(f'  Range: [{corr_pearson.min():.3f}, {corr_pearson.max():.3f}]')\n",
    "print(f'  Computed in: {pearson_time:.2f}s\\n')\n",
    "\n",
    "logger.record_metric('layer2_pearson', 'computation_time_seconds', float(pearson_time))\n",
    "logger.record_metric('layer2_pearson', 'matrix_shape', str(corr_pearson.shape))\n",
    "logger.record_metric('layer2_pearson', 'value_range', f\"[{corr_pearson.min():.3f}, {corr_pearson.max():.3f}]\")\n",
    "\n",
    "print(f'Saving Layer 2 correlation matrices')\n",
    "np.save(SUBDIRS['correlation'] / 'corr_pearson.npy', corr_pearson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01dd61",
   "metadata": {},
   "source": [
    "Why the negative correlations (-0.394)?\n",
    "- Positive correlations (0 to 1.0): Genes move together\n",
    "- Negative correlations (-0.394): Genes move in OPPOSITE directions ✓\n",
    "\n",
    "Real biological examples:\n",
    "- Apoptosis genes ↓ when Proliferation genes ↑\n",
    "- Tumor suppressors ↓ in cancer samples\n",
    "- Differentiation genes ↓ when Stem cell genes ↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ffb46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating HRR (Hypergeometric Rank Rank) correlation...\n",
      "This is a rank-based method robust to outliers\n",
      "\n",
      "\n",
      "Calculating HRR Correlation\n",
      "  Cells: 83,522\n",
      "  Genes: 3,000\n",
      "  Bin size (top/bottom): 835 cells (1/100 of data)\n",
      "  Total gene pairs: 4,498,500\n",
      "\n",
      "Step 1: Ranking gene expression...\n",
      "  Ranking completed in 14.55s\n",
      "\n",
      "Step 2: Identifying top/bottom bin memberships...\n",
      "  Bin identification completed in 0.23s\n",
      "\n",
      "Step 3: Computing HRR for all gene pairs...\n",
      "  Progress (every 100,000 pairs):\n",
      "\n",
      "    100,000/4,498,500 pairs (2.2%) | ETA: 16.7m\n",
      "    200,000/4,498,500 pairs (4.4%) | ETA: 16.2m\n",
      "    300,000/4,498,500 pairs (6.7%) | ETA: 15.8m\n",
      "    400,000/4,498,500 pairs (8.9%) | ETA: 15.4m\n",
      "    500,000/4,498,500 pairs (11.1%) | ETA: 15.0m\n",
      "    600,000/4,498,500 pairs (13.3%) | ETA: 14.6m\n",
      "    700,000/4,498,500 pairs (15.6%) | ETA: 14.2m\n",
      "    800,000/4,498,500 pairs (17.8%) | ETA: 13.9m\n",
      "    900,000/4,498,500 pairs (20.0%) | ETA: 13.5m\n",
      "    1,000,000/4,498,500 pairs (22.2%) | ETA: 13.1m\n",
      "    1,100,000/4,498,500 pairs (24.5%) | ETA: 12.8m\n",
      "    1,200,000/4,498,500 pairs (26.7%) | ETA: 12.4m\n",
      "    1,300,000/4,498,500 pairs (28.9%) | ETA: 12.0m\n",
      "    1,400,000/4,498,500 pairs (31.1%) | ETA: 11.7m\n",
      "    1,500,000/4,498,500 pairs (33.3%) | ETA: 11.3m\n",
      "    1,600,000/4,498,500 pairs (35.6%) | ETA: 10.9m\n",
      "    1,700,000/4,498,500 pairs (37.8%) | ETA: 10.5m\n",
      "    1,800,000/4,498,500 pairs (40.0%) | ETA: 10.2m\n",
      "    1,900,000/4,498,500 pairs (42.2%) | ETA: 9.8m\n",
      "    2,000,000/4,498,500 pairs (44.5%) | ETA: 9.4m\n",
      "    2,100,000/4,498,500 pairs (46.7%) | ETA: 9.0m\n",
      "    2,200,000/4,498,500 pairs (48.9%) | ETA: 8.6m\n",
      "    2,300,000/4,498,500 pairs (51.1%) | ETA: 8.3m\n",
      "    2,400,000/4,498,500 pairs (53.4%) | ETA: 7.9m\n",
      "    2,500,000/4,498,500 pairs (55.6%) | ETA: 7.5m\n",
      "    2,600,000/4,498,500 pairs (57.8%) | ETA: 7.2m\n",
      "    2,700,000/4,498,500 pairs (60.0%) | ETA: 6.8m\n",
      "    2,800,000/4,498,500 pairs (62.2%) | ETA: 6.4m\n",
      "    2,900,000/4,498,500 pairs (64.5%) | ETA: 6.0m\n",
      "    3,000,000/4,498,500 pairs (66.7%) | ETA: 5.6m\n",
      "    3,100,000/4,498,500 pairs (68.9%) | ETA: 5.3m\n",
      "    3,200,000/4,498,500 pairs (71.1%) | ETA: 4.9m\n",
      "    3,300,000/4,498,500 pairs (73.4%) | ETA: 4.5m\n",
      "    3,400,000/4,498,500 pairs (75.6%) | ETA: 4.1m\n",
      "    3,500,000/4,498,500 pairs (77.8%) | ETA: 3.8m\n",
      "    3,600,000/4,498,500 pairs (80.0%) | ETA: 3.4m\n",
      "    3,700,000/4,498,500 pairs (82.2%) | ETA: 3.0m\n",
      "    3,800,000/4,498,500 pairs (84.5%) | ETA: 2.6m\n",
      "    3,900,000/4,498,500 pairs (86.7%) | ETA: 2.3m\n",
      "    4,000,000/4,498,500 pairs (88.9%) | ETA: 1.9m\n",
      "    4,100,000/4,498,500 pairs (91.1%) | ETA: 1.5m\n",
      "    4,200,000/4,498,500 pairs (93.4%) | ETA: 1.1m\n",
      "    4,300,000/4,498,500 pairs (95.6%) | ETA: 0.7m\n",
      "    4,400,000/4,498,500 pairs (97.8%) | ETA: 0.4m\n",
      "\n",
      "  HRR calculation completed in 1017.17s (17.0 minutes)\n",
      "  Rate: 4,423 pairs/second\n",
      "\n",
      "Step 4: Validation\n",
      "  Matrix shape: (3000, 3000)\n",
      "  Diagonal (should be ~1.0): [1. 1. 1. 1. 1.]\n",
      "  Range: [-0.762, 1.000]\n",
      "  Mean: 0.207\n",
      "  Median: 0.000\n",
      "  NaN values: 0\n",
      "  Inf values: 0\n",
      "  Positive values: 2,456,460 (27.3%)\n",
      "  Negative values: 7,238 (0.1%)\n",
      "  Zero values: 6,536,302 (72.6%)\n",
      "\n",
      "✓ HRR correlation matrix: (3000, 3000)\n",
      "  Diagonal (should be ~1.0): [1. 1. 1. 1. 1.]\n",
      "  Range: [-0.762, 1.000]\n",
      "  Total computation time: 1311.12s (21.9 minutes)\n",
      "\n",
      "Saving HRR correlation matrix...\n",
      "✓ Saved: corr_hrr.npy\n",
      "\n",
      "Saving Layer 2 correlation matrices...\n",
      "[18:39:30] LAYER2: Correlation matrices saved (Pearson + HRR)\n",
      " Saved: corr_pearson.npy, corr_hrr.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Calculating HRR (Hypergeometric Rank Rank) correlation...')\n",
    "print(f'This is a rank-based method robust to outliers\\n')\n",
    "\n",
    "def hrr_correlation_corrected(X, n_bins=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate HRR (Hypergeometric Rank-Rank) correlation matrix\n",
    "    \n",
    "    METHOD:\n",
    "    1. Rank each gene's expression across cells\n",
    "    2. Define top/bottom bins\n",
    "    3. For each gene pair: count hypergeometric overlap\n",
    "    4. Positive score if both in same bin (co-rank)\n",
    "    5. Negative score if in opposite bins (anti-rank)\n",
    "    \n",
    "    PARAMETERS:\n",
    "    - X: expression matrix (cells × genes), should be log-normalized\n",
    "    - n_bins: number of bins (100 = top 1% and bottom 1%)\n",
    "    - verbose: print progress\n",
    "    \n",
    "    OUTPUT:\n",
    "    - corr_hrr: correlation matrix (-1 to 1 scale)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_cells, n_genes = X.shape\n",
    "    bin_size = n_cells // n_bins  # Size of top and bottom bins\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\\nCalculating HRR Correlation')\n",
    "        print(f'  Cells: {n_cells:,}')\n",
    "        print(f'  Genes: {n_genes:,}')\n",
    "        print(f'  Bin size (top/bottom): {bin_size:,} cells (1/{n_bins} of data)')\n",
    "        print(f'  Total gene pairs: {n_genes * (n_genes - 1) // 2:,}\\n')\n",
    "    \n",
    "    # Step 1: Rank each gene's expression across cells\n",
    "    if verbose:\n",
    "        print(f'Step 1: Ranking gene expression...')\n",
    "    \n",
    "    start_time_rank = time.time()\n",
    "    ranks = rankdata(X, axis=0)  # Vectorized ranking\n",
    "    rank_time = time.time() - start_time_rank\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'  Ranking completed in {rank_time:.2f}s\\n')\n",
    "    \n",
    "    # Step 2: Initialize correlation matrix\n",
    "    corr_hrr = np.zeros((n_genes, n_genes))\n",
    "    \n",
    "    # Step 3: Calculate bin memberships for each gene\n",
    "    if verbose:\n",
    "        print(f'Step 2: Identifying top/bottom bin memberships...')\n",
    "    \n",
    "    start_time_bins = time.time()\n",
    "    \n",
    "    # For each gene, identify which cells are in top bin and bottom bin\n",
    "    top_bins = np.zeros((n_genes, n_cells), dtype=bool)\n",
    "    bottom_bins = np.zeros((n_genes, n_cells), dtype=bool)\n",
    "    \n",
    "    for i in range(n_genes):\n",
    "        # Top bin: highest ranked cells\n",
    "        top_bins[i, :] = ranks[:, i] >= (n_cells - bin_size)\n",
    "        # Bottom bin: lowest ranked cells\n",
    "        bottom_bins[i, :] = ranks[:, i] < bin_size\n",
    "    \n",
    "    bin_time = time.time() - start_time_bins\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'  Bin identification completed in {bin_time:.2f}s\\n')\n",
    "    \n",
    "    # Step 4: Calculate HRR for each gene pair\n",
    "    if verbose:\n",
    "        print(f'Step 3: Computing HRR for all gene pairs...')\n",
    "        print(f'  Progress (every 100,000 pairs):\\n')\n",
    "    \n",
    "    start_time_hrr = time.time()\n",
    "    pair_count = 0\n",
    "    total_pairs = n_genes * (n_genes - 1) // 2\n",
    "    \n",
    "    for i in range(n_genes):\n",
    "        for j in range(i + 1, n_genes):\n",
    "            # Count overlaps using hypergeometric principle\n",
    "            \n",
    "            # Same bin overlaps (concordant)\n",
    "            top_overlap = np.sum(top_bins[i, :] & top_bins[j, :])\n",
    "            bottom_overlap = np.sum(bottom_bins[i, :] & bottom_bins[j, :])\n",
    "            concordant = top_overlap + bottom_overlap\n",
    "            \n",
    "            # Opposite bin overlaps (discordant)\n",
    "            top_bottom_overlap = np.sum(top_bins[i, :] & bottom_bins[j, :])\n",
    "            bottom_top_overlap = np.sum(bottom_bins[i, :] & top_bins[j, :])\n",
    "            discordant = top_bottom_overlap + bottom_top_overlap\n",
    "            \n",
    "            # Calculate HRR score\n",
    "            # Positive if genes rank together, negative if rank apart\n",
    "            total_overlap = concordant + discordant\n",
    "            \n",
    "            if total_overlap > 0:\n",
    "                # Hypergeometric-based score\n",
    "                # Normalize by expected overlap under independence\n",
    "                expected_overlap = (2 * bin_size ** 2) / n_cells\n",
    "                \n",
    "                # Score: (observed - expected) / expected\n",
    "                if expected_overlap > 0:\n",
    "                    hrr_score = (concordant - discordant) / total_overlap\n",
    "                    # Normalize to [-1, 1]\n",
    "                    corr_hrr[i, j] = np.tanh(hrr_score)\n",
    "                else:\n",
    "                    corr_hrr[i, j] = 0.0\n",
    "            else:\n",
    "                corr_hrr[i, j] = 0.0\n",
    "            \n",
    "            # Symmetric matrix\n",
    "            corr_hrr[j, i] = corr_hrr[i, j]\n",
    "            \n",
    "            pair_count += 1\n",
    "            \n",
    "            # Progress reporting\n",
    "            if pair_count % 100000 == 0 and verbose:\n",
    "                elapsed = time.time() - start_time_hrr\n",
    "                rate = pair_count / elapsed\n",
    "                remaining = (total_pairs - pair_count) / rate if rate > 0 else 0\n",
    "                print(f'    {pair_count:,}/{total_pairs:,} pairs ({100*pair_count/total_pairs:.1f}%) | ETA: {remaining/60:.1f}m')\n",
    "    \n",
    "    # Step 5: Set diagonal to 1.0 (perfect self-correlation)\n",
    "    np.fill_diagonal(corr_hrr, 1.0)\n",
    "    \n",
    "    hrr_time = time.time() - start_time_hrr\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\\n  HRR calculation completed in {hrr_time:.2f}s ({hrr_time/60:.1f} minutes)')\n",
    "        print(f'  Rate: {pair_count / hrr_time:,.0f} pairs/second\\n')\n",
    "    \n",
    "    # Validation\n",
    "    if verbose:\n",
    "        print(f'Step 4: Validation')\n",
    "        print(f'  Matrix shape: {corr_hrr.shape}')\n",
    "        print(f'  Diagonal (should be ~1.0): {np.diag(corr_hrr)[:5]}')\n",
    "        print(f'  Range: [{corr_hrr.min():.3f}, {corr_hrr.max():.3f}]')\n",
    "        print(f'  Mean: {np.mean(corr_hrr):.3f}')\n",
    "        print(f'  Median: {np.median(corr_hrr):.3f}')\n",
    "        \n",
    "        # Check for NaN or Inf\n",
    "        n_nan = np.isnan(corr_hrr).sum()\n",
    "        n_inf = np.isinf(corr_hrr).sum()\n",
    "        print(f'  NaN values: {n_nan}')\n",
    "        print(f'  Inf values: {n_inf}')\n",
    "        \n",
    "        # Distribution check\n",
    "        positive_count = np.sum(corr_hrr > 0)\n",
    "        negative_count = np.sum(corr_hrr < 0)\n",
    "        zero_count = np.sum(corr_hrr == 0)\n",
    "        \n",
    "        print(f'  Positive values: {positive_count:,} ({100*positive_count/corr_hrr.size:.1f}%)')\n",
    "        print(f'  Negative values: {negative_count:,} ({100*negative_count/corr_hrr.size:.1f}%)')\n",
    "        print(f'  Zero values: {zero_count:,} ({100*zero_count/corr_hrr.size:.1f}%)')\n",
    "        print()\n",
    "    \n",
    "    return corr_hrr\n",
    "\n",
    "# Use the CORRECTED function\n",
    "corr_hrr = hrr_correlation_corrected(X, n_bins=100, verbose=True)\n",
    "\n",
    "hrr_time = time.time() - start_time\n",
    "\n",
    "print(f'✓ HRR correlation matrix: {corr_hrr.shape}')\n",
    "print(f'  Diagonal (should be ~1.0): {np.diag(corr_hrr)[:5]}')\n",
    "print(f'  Range: [{corr_hrr.min():.3f}, {corr_hrr.max():.3f}]')\n",
    "print(f'  Total computation time: {hrr_time:.2f}s ({hrr_time/60:.1f} minutes)\\n')\n",
    "\n",
    "logger.record_metric('layer2_hrr', 'computation_time_seconds', float(hrr_time))\n",
    "logger.record_metric('layer2_hrr', 'matrix_shape', str(corr_hrr.shape))\n",
    "logger.record_metric('layer2_hrr', 'value_range', f\"[{corr_hrr.min():.3f}, {corr_hrr.max():.3f}]\")\n",
    "\n",
    "# Save\n",
    "print(f'Saving HRR correlation matrix...')\n",
    "np.save(SUBDIRS['correlation'] / 'corr_hrr.npy', corr_hrr)\n",
    "print(f'✓ Saved: corr_hrr.npy\\n')\n",
    "\n",
    "\n",
    "# Save Layer 2 correlations\n",
    "print(f'Saving Layer 2 correlation matrices...')\n",
    "np.save(SUBDIRS['correlation'] / 'corr_hrr.npy', corr_hrr)\n",
    "\n",
    "with open(SUBDIRS['correlation'] / 'gene_names.pkl', 'wb') as f:\n",
    "    pickle.dump(gene_names, f)\n",
    "\n",
    "logger.log('LAYER2', 'Correlation matrices saved (Pearson + HRR)')\n",
    "print(f' Saved: corr_pearson.npy, corr_hrr.npy\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c917d4",
   "metadata": {},
   "source": [
    "## 3.4 LAYER 3 - VALIDATION (Consensus Clustering + BICOR + Distance Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "734c2e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 merge distances:\n",
      "[0.99945883 0.99946077 0.999464   0.99946638 0.99948317 0.99950479\n",
      " 0.99951223 0.99954472 0.9995601  0.99958596]\n",
      "\n",
      "Testing thresholds:\n",
      "\n",
      "Threshold 0.3: 2937 modules | Top sizes: [np.int64(5), np.int64(4), np.int64(3), np.int64(3), np.int64(3)]\n",
      "Threshold 0.4: 2911 modules | Top sizes: [np.int64(6), np.int64(6), np.int64(6), np.int64(4), np.int64(4)]\n",
      "Threshold 0.5: 2850 modules | Top sizes: [np.int64(8), np.int64(6), np.int64(6), np.int64(4), np.int64(4)]\n",
      "Threshold 0.6: 2756 modules | Top sizes: [np.int64(18), np.int64(10), np.int64(7), np.int64(7), np.int64(7)]\n",
      "Threshold 0.7: 2581 modules | Top sizes: [np.int64(29), np.int64(29), np.int64(17), np.int64(10), np.int64(7)]\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Look at dendrogram structure\n",
    "print(\"Last 10 merge distances:\")\n",
    "print(Z[-10:, 2])\n",
    "\n",
    "# Test different thresholds\n",
    "print(\"\\nTesting thresholds:\\n\")\n",
    "\n",
    "for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    clusters = fcluster(Z, threshold, criterion='distance')\n",
    "    n_mods = len(np.unique(clusters))\n",
    "    sizes = sorted([np.sum(clusters == i) for i in np.unique(clusters)], reverse=True)\n",
    "    print(f'Threshold {threshold}: {n_mods} modules | Top sizes: {sizes[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGNOSTIC: CHECK YOUR CORRELATIONS\n",
      "================================================================================\n",
      "\n",
      "Pearson: [-0.3940, 1.0000]\n",
      "  Mean: 0.0019, Median: -0.0002\n",
      "  Zeros: 0\n",
      "\n",
      "HRR: [-0.7616, 0.7616]\n",
      "  Mean: 0.2071, Median: 0.0000\n",
      "  Zeros: 3,268,151 (72.6%)\n",
      "\n",
      "60/40 Combined: [-0.5173, 0.9046]\n",
      "  Mean: 0.0840, Median: -0.0001\n",
      "\n",
      "Distance (1 - |corr|): [0.0954, 1.0000]\n",
      "  Mean: 0.9145\n",
      "  > 0.99: 3,246,477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC: CHECK CORRELATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Check Pearson\n",
    "pearson_vals = corr_pearson[np.triu_indices(corr_pearson.shape[0], k=1)]\n",
    "print(f\"Pearson: [{pearson_vals.min():.4f}, {pearson_vals.max():.4f}]\")\n",
    "print(f\"  Mean: {pearson_vals.mean():.4f}, Median: {np.median(pearson_vals):.4f}\")\n",
    "print(f\"  Zeros: {(pearson_vals == 0).sum():,}\")\n",
    "\n",
    "# Check HRR\n",
    "hrr_vals = corr_hrr[np.triu_indices(corr_hrr.shape[0], k=1)]\n",
    "print(f\"\\nHRR: [{hrr_vals.min():.4f}, {hrr_vals.max():.4f}]\")\n",
    "print(f\"  Mean: {hrr_vals.mean():.4f}, Median: {np.median(hrr_vals):.4f}\")\n",
    "print(f\"  Zeros: {(hrr_vals == 0).sum():,} ({100*(hrr_vals == 0).sum()/len(hrr_vals):.1f}%)\")\n",
    "\n",
    "# Test combined\n",
    "corr_test = 0.6 * corr_pearson + 0.4 * corr_hrr\n",
    "test_vals = corr_test[np.triu_indices(corr_test.shape[0], k=1)]\n",
    "print(f\"\\n60/40 Combined: [{test_vals.min():.4f}, {test_vals.max():.4f}]\")\n",
    "print(f\"  Mean: {test_vals.mean():.4f}, Median: {np.median(test_vals):.4f}\")\n",
    "\n",
    "# Check distance\n",
    "dist_test = 1 - np.abs(corr_test)\n",
    "dist_vals = dist_test[np.triu_indices(dist_test.shape[0], k=1)]\n",
    "print(f\"\\nDistance (1 - |corr|): [{dist_vals.min():.4f}, {dist_vals.max():.4f}]\")\n",
    "print(f\"  Mean: {dist_vals.mean():.4f}\")\n",
    "print(f\"  > 0.99: {(dist_vals > 0.99).sum():,}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a69205e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 83522 × 3000\n",
      "    obs: 'barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype'\n",
      "    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
      "    uns: 'log1p', 'hvg'\n",
      "X max value: 8.942034721374512\n"
     ]
    }
   ],
   "source": [
    "# Print adata info\n",
    "print(adata_hvg)\n",
    "print(f\"X max value: {adata_hvg.X.max()}\")  # If > 20, probably NOT log!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6285e525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING PEARSON CALCULATION\n",
      "\n",
      "Data Stats:\n",
      "  Shape: (83522, 3000)\n",
      "  Min: 0.0000, Max: 8.9420, Mean: 0.0709\n",
      "\n",
      "Test 1: Manual correlation (first 10 genes):\n",
      "  Mean: -0.000082\n",
      "  Range: [-0.0046, 0.0124]\n",
      "\n",
      "Test 2: Fresh Pearson calculation:\n",
      "  Mean: 0.001915\n",
      "  Range: [-0.3940, 1.0000]\n",
      "\n",
      "Test 3: Compare OLD vs FRESH:\n",
      "  Old mean: 0.001915\n",
      "  Fresh mean: 0.001915\n",
      "  Same? True\n",
      "\n",
      " Even fresh Pearson mean is 0.001915\n",
      "  Your genes may be genuinely uncorrelated!\n"
     ]
    }
   ],
   "source": [
    "print(\"DEBUGGING PEARSON CALCULATION\")\n",
    "print()\n",
    "\n",
    "# Get data\n",
    "X = np.asarray(adata_hvg.X.todense()) if hasattr(adata_hvg.X, 'todense') else np.asarray(adata_hvg.X)\n",
    "\n",
    "print(f\"Data Stats:\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  Min: {X.min():.4f}, Max: {X.max():.4f}, Mean: {X.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "# Test 1: Manual correlation on first 10 genes\n",
    "print(f\"Test 1: Manual correlation (first 10 genes):\")\n",
    "X_test = X[:, :10]\n",
    "corr_test = np.corrcoef(X_test.T)\n",
    "test_vals = corr_test[np.triu_indices(10, k=1)]\n",
    "print(f\"  Mean: {test_vals.mean():.6f}\")\n",
    "print(f\"  Range: [{test_vals.min():.4f}, {test_vals.max():.4f}]\")\n",
    "print()\n",
    "\n",
    "# Test 2: Calculate full Pearson fresh\n",
    "print(f\"Test 2: Fresh Pearson calculation:\")\n",
    "corr_fresh = np.corrcoef(X.T)\n",
    "fresh_vals = corr_fresh[np.triu_indices(corr_fresh.shape[0], k=1)]\n",
    "print(f\"  Mean: {fresh_vals.mean():.6f}\")\n",
    "print(f\"  Range: [{fresh_vals.min():.4f}, {fresh_vals.max():.4f}]\")\n",
    "print()\n",
    "\n",
    "# Test 3: Compare with old\n",
    "print(f\"Test 3: Compare OLD vs FRESH:\")\n",
    "print(f\"  Old mean: {corr_pearson[np.triu_indices(corr_pearson.shape[0], k=1)].mean():.6f}\")\n",
    "print(f\"  Fresh mean: {fresh_vals.mean():.6f}\")\n",
    "print(f\"  Same? {np.allclose(corr_pearson, corr_fresh)}\")\n",
    "print()\n",
    "\n",
    "# If fresh is better, use it\n",
    "if fresh_vals.mean() > 0.05:\n",
    "    print(f\" Fresh Pearson is GOOD! Using it.\")\n",
    "    corr_pearson = corr_fresh\n",
    "else:\n",
    "    print(f\" Even fresh Pearson mean is {fresh_vals.mean():.6f}\")\n",
    "    print(f\"  Your genes may be genuinely uncorrelated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "059740e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LAYER 3: STEP 1 - TEST DIFFERENT PEARSON/HRR RATIOS\n",
      "================================================================================\n",
      "\n",
      "Testing different Pearson/HRR ratios...\n",
      "\n",
      "Testing ratio: 100/0\n",
      "  Merge distances (last 3): [0.99954472 0.9995601  0.99958596]\n",
      "  Modules: 2850 | Imbalance:     8.00x | Top 5: [8, 6, 6, 4, 4]\n",
      "\n",
      "Testing ratio: 90/10\n",
      "  Merge distances (last 3): [0.99943166 0.99945815 0.99953973]\n",
      "  Modules: 2825 | Imbalance:     9.00x | Top 5: [9, 8, 6, 6, 5]\n",
      "\n",
      "Testing ratio: 80/20\n",
      "  Merge distances (last 3): [0.99935363 0.99937723 0.99953443]\n",
      "  Modules: 2793 | Imbalance:    11.00x | Top 5: [11, 10, 6, 6, 5]\n",
      "\n",
      "Testing ratio: 70/30\n",
      "  Merge distances (last 3): [0.99927561 0.9992963  0.99952914]\n",
      "  Modules: 2739 | Imbalance:    18.00x | Top 5: [18, 11, 10, 10, 8]\n",
      "\n",
      "Testing ratio: 60/40\n",
      "  Merge distances (last 3): [0.99919758 0.99921538 0.99952385]\n",
      "  Modules: 2632 | Imbalance:    25.00x | Top 5: [25, 17, 15, 10, 10]\n",
      "\n",
      "Testing ratio: 50/50\n",
      "  Merge distances (last 3): [0.99911955 0.99913445 0.99951856]\n",
      "  Modules: 2386 | Imbalance:    41.00x | Top 5: [41, 37, 25, 10, 10]\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Ratio        Modules      Imbalance   \n",
      "------------------------------------\n",
      "100/0        2850         8.00        x\n",
      "50/50        2386         41.00       x\n",
      "60/40        2632         25.00       x\n",
      "70/30        2739         18.00       x\n",
      "80/20        2793         11.00       x\n",
      "90/10        2825         9.00        x\n",
      "\n",
      "⚠ No good ratio found - using 60/40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LAYER 3: STEP 1 - TEST DIFFERENT PEARSON/HRR RATIOS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "ratios_to_test = [(1.0, 0.0), (0.9, 0.1), (0.8, 0.2), (0.7, 0.3), (0.6, 0.4), (0.5, 0.5)]\n",
    "\n",
    "ratio_results = {}\n",
    "\n",
    "print(\"Testing different Pearson/HRR ratios...\\n\")\n",
    "\n",
    "for w_pearson, w_hrr in ratios_to_test:\n",
    "    ratio_label = f\"{int(w_pearson*100)}/{int(w_hrr*100)}\"\n",
    "    print(f\"Testing ratio: {ratio_label}\")\n",
    "    \n",
    "    # ✅ CREATE NEW correlation matrix\n",
    "    corr_combined = w_pearson * corr_pearson + w_hrr * corr_hrr\n",
    "    \n",
    "    # ✅ CREATE NEW distance matrix\n",
    "    distance_combined = 1 - np.abs(corr_combined)\n",
    "    \n",
    "    # ✅ CREATE NEW Z matrix!\n",
    "    n_genes = distance_combined.shape[0]\n",
    "    condensed_dist = distance_combined[np.triu_indices(n_genes, k=1)]\n",
    "    Z = linkage(condensed_dist, method='average')\n",
    "    \n",
    "    # Test at threshold 0.5\n",
    "    clusters = fcluster(Z, 0.5, criterion='distance')\n",
    "    n_mods = len(np.unique(clusters))\n",
    "    sizes = sorted([int(np.sum(clusters == i)) for i in np.unique(clusters)], reverse=True)\n",
    "    imbalance = max(sizes) / min(sizes) if min(sizes) > 0 else float('inf')\n",
    "    \n",
    "    print(f\"  Merge distances (last 3): {Z[-3:, 2]}\")\n",
    "    print(f\"  Modules: {n_mods:4d} | Imbalance: {imbalance:8.2f}x | Top 5: {sizes[:5]}\\n\")\n",
    "    \n",
    "    ratio_results[ratio_label] = {\n",
    "        'Z': Z,\n",
    "        'corr_combined': corr_combined,\n",
    "        'n_modules': n_mods,\n",
    "        'imbalance': imbalance,\n",
    "        'sizes': sizes,\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Ratio':<12} {'Modules':<12} {'Imbalance':<12}\")\n",
    "print(\"-\"*36)\n",
    "\n",
    "best_ratio = None\n",
    "for ratio_label in sorted(ratio_results.keys()):\n",
    "    r = ratio_results[ratio_label]\n",
    "    is_good = 20 <= r['n_modules'] <= 50 and r['imbalance'] < 10\n",
    "    status = \" ✓ GOOD\" if is_good else \"\"\n",
    "    \n",
    "    print(f\"{ratio_label:<12} {r['n_modules']:<12} {r['imbalance']:<12.2f}x{status}\")\n",
    "    \n",
    "    if is_good and best_ratio is None:\n",
    "        best_ratio = ratio_label\n",
    "\n",
    "print()\n",
    "\n",
    "if best_ratio:\n",
    "    print(f\"✓ BEST RATIO: {best_ratio}\\n\")\n",
    "else:\n",
    "    print(\"⚠ No good ratio found - using 60/40\\n\")\n",
    "    best_ratio = \"60/40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Step 2/3: BICOR (Biweight Midcorrelation)')\n",
    "print(f'Robust correlation resistant to outliers\\n')\n",
    "\n",
    "def bicor_single_pair(x, y, k=0.9):\n",
    "    \"\"\"Calculate BICOR for a single gene pair\"\"\"\n",
    "    med_x = np.median(x)\n",
    "    mad_x = np.median(np.abs(x - med_x))\n",
    "    \n",
    "    med_y = np.median(y)\n",
    "    mad_y = np.median(np.abs(y - med_y))\n",
    "    \n",
    "    if mad_x == 0 or mad_y == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    u_x = (x - med_x) / (k * mad_x)\n",
    "    u_y = (y - med_y) / (k * mad_y)\n",
    "    \n",
    "    # Weight function: (1 - u^2)^2 for |u| < 1\n",
    "    w_x = np.where(np.abs(u_x) < 1, (1 - u_x**2)**2, 0)\n",
    "    w_y = np.where(np.abs(u_y) < 1, (1 - u_y**2)**2, 0)\n",
    "    \n",
    "    # Biweight midcorrelation\n",
    "    numerator = np.sum(w_x * w_y * (x - med_x) * (y - med_y))\n",
    "    denominator = np.sqrt(np.sum(w_x * (x - med_x)**2) * np.sum(w_y * (y - med_y)**2))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def bicor_matrix_fast(X, sample_genes=None, k=0.9):\n",
    "    \"\"\"\n",
    "    Calculate BICOR for all gene pairs\n",
    "    If sample_genes < n_genes, sample randomly for speed\n",
    "    \"\"\"\n",
    "    n_genes = X.shape[1]\n",
    "    \n",
    "    if sample_genes is None or sample_genes > n_genes:\n",
    "        genes_to_use = list(range(n_genes))\n",
    "    else:\n",
    "        # Sample random genes for faster computation\n",
    "        genes_to_use = np.random.choice(n_genes, sample_genes, replace=False).tolist()\n",
    "    \n",
    "    n_selected = len(genes_to_use)\n",
    "    corr_bicor = np.eye(n_selected)\n",
    "    \n",
    "    print(f'Computing BICOR for {n_selected} genes...')\n",
    "    \n",
    "    for i in range(n_selected):\n",
    "        for j in range(i + 1, n_selected):\n",
    "            bicor_val = bicor_single_pair(X[:, genes_to_use[i]], X[:, genes_to_use[j]], k)\n",
    "            corr_bicor[i, j] = bicor_val\n",
    "            corr_bicor[j, i] = bicor_val\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'  Processed {i+1}/{n_selected} genes...')\n",
    "    \n",
    "    return corr_bicor, genes_to_use\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use sampling for speed (select top 500 variable genes)\n",
    "top_variable_genes = np.argsort(X.var(axis=0))[-500:]\n",
    "X_sampled = X[:, top_variable_genes]\n",
    "gene_names_sampled = [gene_names[i] for i in top_variable_genes]\n",
    "\n",
    "corr_bicor, used_indices = bicor_matrix_fast(X_sampled, sample_genes=None, k=0.9)\n",
    "\n",
    "bicor_time = time.time() - start_time\n",
    "\n",
    "print(f' BICOR correlation matrix: {corr_bicor.shape}')\n",
    "print(f'  Diagonal: {np.diag(corr_bicor)[:5]}')\n",
    "print(f'  Range: [{corr_bicor.min():.3f}, {corr_bicor.max():.3f}]')\n",
    "print(f'  Computed in: {bicor_time:.2f}s\\n')\n",
    "\n",
    "logger.record_metric('layer3_bicor', 'computation_time_seconds', float(bicor_time))\n",
    "logger.record_metric('layer3_bicor', 'n_genes_used', len(gene_names_sampled))\n",
    "logger.record_metric('layer3_bicor', 'value_range', f\"[{corr_bicor.min():.3f}, {corr_bicor.max():.3f}]\")\n",
    "\n",
    "# Save BICOR\n",
    "np.save(SUBDIRS['validation'] / 'corr_bicor_sample.npy', corr_bicor)\n",
    "with open(SUBDIRS['validation'] / 'bicor_gene_names.pkl', 'wb') as f:\n",
    "    pickle.dump(gene_names_sampled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473dd4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Step 3/3: DISTANCE CORRELATION')\n",
    "print(f'Captures both linear and non-linear relationships\\n')\n",
    "\n",
    "def distance_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate distance correlation between two variables\n",
    "    Captures both linear and non-linear dependencies\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    x_centered = x - np.mean(x)\n",
    "    y_centered = y - np.mean(y)\n",
    "    \n",
    "    # Euclidean distances\n",
    "    dx = squareform(pdist([x_centered], metric='euclidean'))[0]\n",
    "    dy = squareform(pdist([y_centered], metric='euclidean'))[0]\n",
    "    \n",
    "    # Double-centered distances\n",
    "    dxc = dx - np.mean(dx, axis=0, keepdims=True) - np.mean(dx, axis=1, keepdims=True) + np.mean(dx)\n",
    "    dyc = dy - np.mean(dy, axis=0, keepdims=True) - np.mean(dy, axis=1, keepdims=True) + np.mean(dy)\n",
    "    \n",
    "    # Distance covariance\n",
    "    dcov_xy = np.sqrt(np.sum(dxc * dyc) / len(x)**2)\n",
    "    dcov_xx = np.sqrt(np.sum(dxc * dxc) / len(x)**2)\n",
    "    dcov_yy = np.sqrt(np.sum(dyc * dyc) / len(y)**2)\n",
    "    \n",
    "    # Distance correlation\n",
    "    if dcov_xx == 0 or dcov_yy == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dcov_xy / np.sqrt(dcov_xx * dcov_yy)\n",
    "\n",
    "def distance_correlation_matrix(X, sample_genes=500):\n",
    "    \"\"\"Calculate distance correlation for sampled genes\"\"\"\n",
    "    n_genes = X.shape[1]\n",
    "    \n",
    "    if sample_genes > n_genes:\n",
    "        genes_to_use = list(range(n_genes))\n",
    "    else:\n",
    "        genes_to_use = np.random.choice(n_genes, sample_genes, replace=False).tolist()\n",
    "    \n",
    "    n_selected = len(genes_to_use)\n",
    "    corr_dist = np.eye(n_selected)\n",
    "    \n",
    "    print(f'Computing distance correlation for {n_selected} genes...')\n",
    "    \n",
    "    for i in range(n_selected):\n",
    "        for j in range(i + 1, n_selected):\n",
    "            dist_corr = distance_correlation(X[:, genes_to_use[i]], X[:, genes_to_use[j]])\n",
    "            corr_dist[i, j] = dist_corr\n",
    "            corr_dist[j, i] = dist_corr\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'  Processed {i+1}/{n_selected} genes...')\n",
    "    \n",
    "    return corr_dist, genes_to_use\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "corr_dist, dist_used_indices = distance_correlation_matrix(X, sample_genes=500)\n",
    "\n",
    "dist_time = time.time() - start_time\n",
    "\n",
    "print(f' Distance correlation matrix: {corr_dist.shape}')\n",
    "print(f'  Diagonal: {np.diag(corr_dist)[:5]}')\n",
    "print(f'  Range: [{corr_dist.min():.3f}, {corr_dist.max():.3f}]')\n",
    "print(f'  Computed in: {dist_time:.2f}s\\n')\n",
    "\n",
    "logger.record_metric('layer3_distance', 'computation_time_seconds', float(dist_time))\n",
    "logger.record_metric('layer3_distance', 'value_range', f\"[{corr_dist.min():.3f}, {corr_dist.max():.3f}]\")\n",
    "\n",
    "# Save distance correlation\n",
    "np.save(SUBDIRS['validation'] / 'corr_distance_sample.npy', corr_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafd59e",
   "metadata": {},
   "source": [
    "## 3.5 Hub gene identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbe7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intramodular connectivity for each gene\n",
    "hub_genes_all = {}\n",
    "\n",
    "for module_id in sorted(np.unique(module_assignments)):\n",
    "    genes_in_module = [gene_names[i] for i in range(len(gene_names)) \n",
    "                       if module_assignments[i] == module_id]\n",
    "    gene_indices = [gene_names.index(g) for g in genes_in_module]\n",
    "    \n",
    "    # Get correlations within module (using Pearson)\n",
    "    module_corr = corr_pearson[np.ix_(gene_indices, gene_indices)]\n",
    "    \n",
    "    # Calculate connectivity (mean absolute correlation)\n",
    "    connectivity = np.mean(np.abs(module_corr), axis=1)\n",
    "    \n",
    "    # Sort by connectivity\n",
    "    sorted_genes = sorted(zip(genes_in_module, connectivity), key=lambda x: x[1], reverse=True)\n",
    "    hub_genes_all[module_id] = sorted_genes\n",
    "    \n",
    "    print(f'Module {module_id} ({len(genes_in_module)} genes) - Top 5 hub genes:')\n",
    "    for rank, (gene, conn) in enumerate(sorted_genes[:5], 1):\n",
    "        print(f'  {rank}. {gene:20} (connectivity: {conn:.3f})')\n",
    "    print()\n",
    "\n",
    "# Save hub genes\n",
    "hub_genes_df = []\n",
    "for module_id, genes in hub_genes_all.items():\n",
    "    for rank, (gene, connectivity) in enumerate(genes[:15], 1):\n",
    "        hub_genes_df.append({\n",
    "            'Module': module_id,\n",
    "            'Rank': rank,\n",
    "            'Gene': gene,\n",
    "            'Connectivity': connectivity,\n",
    "        })\n",
    "\n",
    "hub_genes_df = pd.DataFrame(hub_genes_df)\n",
    "hub_genes_df.to_csv(SUBDIRS['hub_analysis'] / 'hub_genes_top15_per_module.csv', index=False)\n",
    "logger.log('HUBS', 'Hub genes identified and saved')\n",
    "print(f' Hub genes saved: hub_genes_top15_per_module.csv\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19891609",
   "metadata": {},
   "source": [
    "## 3.6 Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SECTION 6: VISUALIZATIONS & NETWORK PLOTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Plot 1: Dendrogram\n",
    "print(f'Creating visualization 1/5: Dendrogram...')\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "dendrogram(Z, ax=ax, labels=gene_names, leaf_font_size=4, leaf_rotation=90)\n",
    "ax.set_title(f'{SAMPLE_NAME} Sample - Gene Dendrogram (Hierarchical Clustering)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Genes')\n",
    "ax.set_ylabel('Distance (1 - |Correlation|)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(SUBDIRS['plots'] / '01_dendrogram.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Module sizes\n",
    "print(f'Creating visualization 2/5: Module sizes...')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "module_ids = sorted(module_dict.keys())\n",
    "module_sizes = [len(module_dict[m]) for m in module_ids]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(module_ids)))\n",
    "ax.bar(module_ids, module_sizes, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Module ID', fontsize=12)\n",
    "ax.set_ylabel('Number of Genes', fontsize=12)\n",
    "ax.set_title(f'{SAMPLE_NAME} Sample - Module Sizes', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, (mod_id, size) in enumerate(zip(module_ids, module_sizes)):\n",
    "    ax.text(mod_id, size + 10, str(size), ha='center', fontsize=10, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(SUBDIRS['plots'] / '02_module_sizes.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Correlation method comparison (scatter)\n",
    "print(f'Creating visualization 4/5: Correlation method comparison...')\n",
    "\n",
    "# Sample gene pairs for comparison\n",
    "sample_pairs = np.random.choice(len(gene_names) * (len(gene_names) - 1) // 2, 5000, replace=False)\n",
    "\n",
    "pearson_vals = []\n",
    "hrr_vals = []\n",
    "pair_idx = 0\n",
    "\n",
    "for i in range(len(gene_names)):\n",
    "    for j in range(i + 1, len(gene_names)):\n",
    "        if pair_idx in sample_pairs:\n",
    "            pearson_vals.append(corr_pearson[i, j])\n",
    "            hrr_vals.append(corr_hrr[i, j])\n",
    "        pair_idx += 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Pearson distribution\n",
    "axes[0].hist(np.diag(corr_pearson), bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Correlation Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Pearson Correlation Distribution')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# HRR distribution\n",
    "axes[1].hist(np.diag(corr_hrr), bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Correlation Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('HRR Correlation Distribution')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Comparison scatter\n",
    "axes[2].scatter(pearson_vals, hrr_vals, alpha=0.3, s=10, color='purple')\n",
    "axes[2].set_xlabel('Pearson Correlation')\n",
    "axes[2].set_ylabel('HRR Correlation')\n",
    "axes[2].set_title('Method Comparison (5000 random pairs)')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SUBDIRS['plots'] / '04_correlation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5: Hub gene connectivity\n",
    "print(f'Creating visualization 5/5: Hub gene connectivity...')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(module_dict), figsize=(4*len(module_dict), 5))\n",
    "if len(module_dict) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (module_id, genes) in enumerate(hub_genes_all.items()):\n",
    "    top_hubs = [g[0] for g in genes[:10]]\n",
    "    connectivities = [g[1] for g in genes[:10]]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    colors_hub = plt.cm.viridis(np.linspace(0, 1, len(connectivities)))\n",
    "    ax.barh(range(len(top_hubs)), connectivities, color=colors_hub, edgecolor='black')\n",
    "    ax.set_yticks(range(len(top_hubs)))\n",
    "    ax.set_yticklabels(top_hubs, fontsize=9)\n",
    "    ax.set_xlabel('Connectivity', fontsize=10)\n",
    "    ax.set_title(f'Module {module_id}\\nTop 10 Hub Genes', fontsize=11, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SUBDIRS['plots'] / '05_hub_genes_connectivity.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "logger.log('PLOTS', 'All visualizations created and saved')\n",
    "print(f' All visualizations saved\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64712da1",
   "metadata": {},
   "source": [
    "## 3.7 Summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50680b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "GENE CO-EXPRESSION NETWORK ANALYSIS - SUMMARY REPORT\n",
    "{'='*80}\n",
    "\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Sample: {SAMPLE_NAME}\n",
    "Output Directory: {RUN_DIR}\n",
    "\n",
    "DATASET INFORMATION\n",
    "─────────────────────────────────────────────────────────────\n",
    "Total Cells (original): {n_cells:,}\n",
    "Total Genes (after filtering): {n_genes:,}\n",
    "HVGs Selected for Analysis: {n_hvgs:,}\n",
    "Data Sparsity: {sparsity_pct:.1f}%\n",
    "Expression Matrix Memory: {X.nbytes / (1024**2):.1f} MB\n",
    "\n",
    "LAYER 2: CORRELATION ANALYSIS\n",
    "─────────────────────────────────────────────────────────────\n",
    "✓ Pearson Correlation (Linear relationships)\n",
    "  - Computation time: {pearson_time:.2f}s\n",
    "  - Range: [{corr_pearson.min():.3f}, {corr_pearson.max():.3f}]\n",
    "  - Matrix size: {corr_pearson.shape}\n",
    "\n",
    "✓ HRR Correlation (Rank-based, robust)\n",
    "  - Computation time: {hrr_time:.2f}s\n",
    "  - Range: [{corr_hrr.min():.3f}, {corr_hrr.max():.3f}]\n",
    "  - Matrix size: {corr_hrr.shape}\n",
    "\n",
    "LAYER 3: VALIDATION ANALYSIS\n",
    "─────────────────────────────────────────────────────────────\n",
    "✓ Consensus Clustering (Hierarchical)\n",
    "  - Number of modules detected: {len(module_dict)}\n",
    "  - Clustering time: {clustering_time:.2f}s\n",
    "  - Method: Average linkage on 1-|Pearson|\n",
    "\n",
    "✓ BICOR Correlation (Robust to outliers)\n",
    "  - Genes analyzed: {len(gene_names_sampled)} (top variable)\n",
    "  - Computation time: {bicor_time:.2f}s\n",
    "  - Range: [{corr_bicor.min():.3f}, {corr_bicor.max():.3f}]\n",
    "\n",
    "✓ Distance Correlation (Non-linear)\n",
    "  - Genes analyzed: 500 (sampled)\n",
    "  - Computation time: {dist_time:.2f}s\n",
    "  - Range: [{corr_dist.min():.3f}, {corr_dist.max():.3f}]\n",
    "\n",
    "MODULE INFORMATION\n",
    "─────────────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "\n",
    "for module_id in sorted(module_dict.keys()):\n",
    "    genes = module_dict[module_id]\n",
    "    hub_genes = [f\"{g[0]} ({g[1]:.3f})\" for g in hub_genes_all[module_id][:3]]\n",
    "    summary_report += f\"\\nModule {module_id}:\\n\"\n",
    "    summary_report += f\"  Size: {len(genes):,} genes\\n\"\n",
    "    summary_report += f\"  Top 3 Hub Genes:\\n\"\n",
    "    for hub_gene in hub_genes:\n",
    "        summary_report += f\"    - {hub_gene}\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "OUTPUT FILES GENERATED\n",
    "─────────────────────────────────────────────────────────────\n",
    "Layer 2 (Correlation):\n",
    "  - corr_pearson.npy\n",
    "  - corr_hrr.npy\n",
    "  - hvg_genes_list.csv\n",
    "  - gene_names.pkl\n",
    "\n",
    "Layer 3 (Validation):\n",
    "  - gene_module_assignments.csv\n",
    "  - corr_bicor_sample.npy\n",
    "  - bicor_gene_names.pkl\n",
    "  - corr_distance_sample.npy\n",
    "\n",
    "Hub Analysis:\n",
    "  - hub_genes_top15_per_module.csv\n",
    "  \n",
    "Visualizations:\n",
    "  - 01_dendrogram.png\n",
    "  - 02_module_sizes.png\n",
    "  - 03_pearson_heatmap.png\n",
    "  - 04_correlation_comparison.png\n",
    "  - 05_hub_genes_connectivity.png\n",
    "  \n",
    "Logs & Checkpoints:\n",
    "  - pipeline_log.txt\n",
    "  - execution_metrics.json\n",
    "\n",
    "NEXT STEPS\n",
    "─────────────────────────────────────────────────────────────\n",
    "1. NORMAL sample analysis COMPLETE\n",
    "2. Setup NextFlow pipeline for other samples:\n",
    "   - ER+ sample\n",
    "   - HER2+ sample\n",
    "   - TNBC sample\n",
    "3. Run same workflow for each sample\n",
    "4. Comparative analysis across subtypes\n",
    "5. Pathway enrichment on hub genes\n",
    "6. Network visualization (Cytoscape/D3)\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_file = RUN_DIR / 'ANALYSIS_SUMMARY.txt'\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(summary_report)\n",
    "logger.log('REPORT', f'Summary report saved: {report_file.name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast_cancer_scrnaseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
