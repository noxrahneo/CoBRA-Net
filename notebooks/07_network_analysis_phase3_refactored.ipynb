{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab480b0",
   "metadata": {},
   "source": [
    "COMPREHENSIVE GENE CO-EXPRESSION NETWORK ANALYSIS\n",
    "\n",
    "Refactored for: Robust metrics, Pre-neoplastic integration, Fold-change, Enrichment\n",
    "\n",
    "WHY THIS APPROACH:\n",
    "- Multiple correlation metrics (Spearman, Kendall, Blomqvist, Pearson, Hoeffding, MI)\n",
    "- Weighted robust averaging (not single metric)\n",
    "- Deep exploratory analysis before networks\n",
    "- Domain-specific biological signatures\n",
    "- Expression directionality (up/down regulation)\n",
    "- Therapeutic target identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70a9ad",
   "metadata": {},
   "source": [
    "# SECTION 1: SETUP & DATA INTEGRATION\n",
    "\n",
    "WHY:\n",
    "- Load all 6 datasets (5 cancer groups + pre-neoplastic)\n",
    "- Verify data integrity\n",
    "- Prepare for downstream analysis\n",
    "\n",
    "DATA:\n",
    "- 6 adata_*_epithelial_improved.h5ad files from Phase 2\n",
    "\n",
    "OUTPUT:\n",
    "- All 6 datasets loaded in memory\n",
    "- Data structure verified\n",
    "\n",
    "NEXT USE:\n",
    "- All downstream analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1878b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 6 epithelial datasets...\n",
      "\n",
      "✓ Normal                    83,522 cells × 33,514 genes\n",
      "✓ ER_Positive               91,908 cells × 33,514 genes\n",
      "✓ HER2_Positive             19,693 cells × 33,514 genes\n",
      "✓ TripleNegative            7,561 cells × 33,514 genes\n",
      "✓ TripleNegative_BRCA1      14,186 cells × 33,514 genes\n",
      "✓ BRCA1_PreNeoplastic       7,644 cells × 33,514 genes\n",
      "\n",
      "OK all datasets loaded\n",
      "\n",
      "verifying data structure:\n",
      "\n",
      "Normal:\n",
      "  adata.X type: <class 'scipy.sparse._csr.csr_matrix'> (should be sparse matrix)\n",
      "  X format: possibly normalized\n",
      "  adata.obs columns: ['barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype']\n",
      "  sample_types: ['Normal']\n",
      "Categories (1, object): ['Normal']\n",
      "\n",
      "ER_Positive:\n",
      "  adata.X type: <class 'scipy.sparse._csr.csr_matrix'> (should be sparse matrix)\n",
      "  X format: possibly normalized\n",
      "  adata.obs columns: ['barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype']\n",
      "  sample_types: ['ER_Positive']\n",
      "Categories (1, object): ['ER_Positive']\n",
      "\n",
      "HER2_Positive:\n",
      "  adata.X type: <class 'scipy.sparse._csr.csr_matrix'> (should be sparse matrix)\n",
      "  X format: possibly normalized\n",
      "  adata.obs columns: ['barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype']\n",
      "  sample_types: ['HER2_Positive']\n",
      "Categories (1, object): ['HER2_Positive']\n",
      "\n",
      "TripleNegative:\n",
      "  adata.X type: <class 'scipy.sparse._csr.csr_matrix'> (should be sparse matrix)\n",
      "  X format: possibly normalized\n",
      "  adata.obs columns: ['barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype']\n",
      "  sample_types: ['TripleNegative']\n",
      "Categories (1, object): ['TripleNegative']\n",
      "\n",
      "TripleNegative_BRCA1:\n",
      "  adata.X type: <class 'scipy.sparse._csr.csr_matrix'> (should be sparse matrix)\n",
      "  X format: possibly normalized\n",
      "  adata.obs columns: ['barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype']\n",
      "  sample_types: ['TripleNegative_BRCA1']\n",
      "Categories (1, object): ['TripleNegative_BRCA1']\n",
      "\n",
      "BRCA1_PreNeoplastic:\n",
      "  adata.X type: <class 'scipy.sparse._csr.csr_matrix'> (should be sparse matrix)\n",
      "  X format: possibly normalized\n",
      "  adata.obs columns: ['barcode', 'sample_name', 'sample_type', 'geo_id', 'cell_type', 'epithelial_score', 'immune_score', 'molecular_subtype']\n",
      "  sample_types: ['BRCA1_PreNeoplastic']\n",
      "Categories (1, object): ['BRCA1_PreNeoplastic']\n",
      "\n",
      "OK data structure verified\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, kendalltau, rankdata    # ← UPDATED\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import subprocess\n",
    "import sys\n",
    "import time                                     # ← ADDED\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gseapy\", \"-q\"])\n",
    "import gseapy as gp\n",
    "import io\n",
    "import contextlib\n",
    "import pickle\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "#defined directories\n",
    "BASE_DIR = Path('/triumvirate/home/alexarol/breast_cancer_analysis')\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "PHASE3_DIR = RESULTS_DIR / 'phase3_networks_refactored'\n",
    "PHASE3_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "#defined subfolders\n",
    "(PHASE3_DIR / 'exploratory_analysis').mkdir(exist_ok=True)\n",
    "(PHASE3_DIR / 'correlation_metrics').mkdir(exist_ok=True)\n",
    "(PHASE3_DIR / 'networks').mkdir(exist_ok=True)\n",
    "(PHASE3_DIR / 'enrichment').mkdir(exist_ok=True)\n",
    "(PHASE3_DIR / 'expression_directionality').mkdir(exist_ok=True)\n",
    "(PHASE3_DIR / 'hub_analysis').mkdir(exist_ok=True)\n",
    "(PHASE3_DIR / 'comparison').mkdir(exist_ok=True)\n",
    "(PHASE3_DIR / 'therapeutic_targets').mkdir(exist_ok=True)\n",
    "\n",
    "#defined group names and file paths\n",
    "GROUPS = {\n",
    "    'Normal': RESULTS_DIR / 'adata_normal_epithelial_improved.h5ad',\n",
    "    'ER_Positive': RESULTS_DIR / 'adata_er_positive_epithelial_improved.h5ad',\n",
    "    'HER2_Positive': RESULTS_DIR / 'adata_her2_positive_epithelial_improved.h5ad',\n",
    "    'TripleNegative': RESULTS_DIR / 'adata_triplenegative_epithelial_improved.h5ad',\n",
    "    'TripleNegative_BRCA1': RESULTS_DIR / 'adata_triplenegative_brca1_epithelial_improved.h5ad',\n",
    "    'BRCA1_PreNeoplastic': RESULTS_DIR / 'adata_brca1_preneoplastic_epithelial_improved.h5ad',\n",
    "}\n",
    "\n",
    "#loaded all datasets\n",
    "print(f'loading {len(GROUPS)} epithelial datasets...\\n')\n",
    "\n",
    "datasets = {}\n",
    "for group_name, file_path in GROUPS.items():\n",
    "    if file_path.exists():\n",
    "        adata = sc.read_h5ad(file_path)\n",
    "        datasets[group_name] = adata\n",
    "        print(f'✓ {group_name:25} {adata.n_obs:,} cells × {adata.n_vars:,} genes')\n",
    "    else:\n",
    "        print(f'✗ {group_name:25} FILE NOT FOUND: {file_path}')\n",
    "\n",
    "print(f'\\nOK all datasets loaded\\n')\n",
    "\n",
    "#verified data structure\n",
    "print(f'verifying data structure:\\n')\n",
    "for group_name, adata in datasets.items():\n",
    "    print(f'{group_name}:')\n",
    "    print(f'  adata.X type: {type(adata.X)} (should be sparse matrix)')\n",
    "    print(f'  X format: {\"raw UMI counts\" if adata.X.max() < 1000 else \"possibly normalized\"}')\n",
    "    print(f'  adata.obs columns: {list(adata.obs.columns)}')\n",
    "    print(f'  sample_types: {adata.obs[\"sample_type\"].unique() if \"sample_type\" in adata.obs else \"NOT FOUND\"}')\n",
    "    print()\n",
    "\n",
    "print(f'OK data structure verified\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483c4d9",
   "metadata": {},
   "source": [
    "# SECTION 1.5: DATA PREPROCESSING\n",
    "\n",
    "WHY:\n",
    "- Normalize raw UMI counts to log-scale\n",
    "- Filter highly variable genes (HVGs)\n",
    "- Prepare for network analysis\n",
    "\n",
    "DATA:\n",
    "- Raw UMI count matrices\n",
    "\n",
    "OUTPUT:\n",
    "- Log-normalized, HVG-filtered datasets\n",
    "\n",
    "NEXT USE:\n",
    "- Co-expression metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e79472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Normal... OK 82,380 cells × 3,000 HVGs\n",
      "processing ER_Positive... OK 91,206 cells × 3,000 HVGs\n",
      "processing HER2_Positive... OK 19,554 cells × 3,000 HVGs\n",
      "processing TripleNegative... OK 7,514 cells × 3,000 HVGs\n",
      "processing TripleNegative_BRCA1... OK 14,100 cells × 3,000 HVGs\n",
      "processing BRCA1_PreNeoplastic... OK 7,616 cells × 3,000 HVGs\n",
      "\n",
      "OK preprocessing complete\n",
      "\n",
      "saving processed datasets...\n",
      "\n",
      "✓ adata_normal_hvg_processed.h5ad\n",
      "✓ adata_er_positive_hvg_processed.h5ad\n",
      "✓ adata_her2_positive_hvg_processed.h5ad\n",
      "✓ adata_triplenegative_hvg_processed.h5ad\n",
      "✓ adata_triplenegative_brca1_hvg_processed.h5ad\n",
      "✓ adata_brca1_preneoplastic_hvg_processed.h5ad\n",
      "\n",
      "OK all processed datasets saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#processing parameters\n",
    "N_HVG = 3000  #number of highly variable genes to keep\n",
    "MIN_GENES = 200  #minimum genes per cell\n",
    "MIN_CELLS = 3  #minimum cells per gene\n",
    "\n",
    "processed_datasets = {}\n",
    "\n",
    "for group_name, adata in datasets.items():\n",
    "    print(f'processing {group_name}...', end=' ', flush=True)\n",
    "    \n",
    "    #made copy to avoid modifying original\n",
    "    adata = adata.copy()\n",
    "    \n",
    "    #basic QC: filter by gene detection\n",
    "    sc.pp.filter_cells(adata, min_genes=MIN_GENES)\n",
    "    sc.pp.filter_genes(adata, min_cells=MIN_CELLS)\n",
    "    \n",
    "    #normalized to library size (counts per 10,000)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    \n",
    "    #log-transformed (log1p)\n",
    "    sc.pp.log1p(adata)\n",
    "    \n",
    "    #identified highly variable genes\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=N_HVG, batch_key=None)\n",
    "    \n",
    "    #subset to HVGs\n",
    "    adata_hvg = adata[:, adata.var['highly_variable']].copy()\n",
    "    \n",
    "    #stored in processed dict\n",
    "    processed_datasets[group_name] = adata_hvg\n",
    "    \n",
    "    print(f'OK {adata_hvg.n_obs:,} cells × {adata_hvg.n_vars:,} HVGs')\n",
    "\n",
    "print(f'\\nOK preprocessing complete\\n')\n",
    "\n",
    "#saved processed datasets for future reference\n",
    "print(f'saving processed datasets...\\n')\n",
    "for group_name, adata in processed_datasets.items():\n",
    "    out_file = PHASE3_DIR / f'adata_{group_name.lower()}_hvg_processed.h5ad'\n",
    "    adata.write(out_file)\n",
    "    print(f'✓ {out_file.name}')\n",
    "\n",
    "print(f'\\nOK all processed datasets saved\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f9536",
   "metadata": {},
   "source": [
    "# SECTION 2: COMPREHENSIVE EXPLORATORY ANALYSIS\n",
    "\n",
    "WHY:\n",
    "- Deeply understand data before network analysis\n",
    "- Identify technical artifacts\n",
    "- Characterize biological signals\n",
    "- Validate group differences\n",
    "\n",
    "DATA:\n",
    "- All 6 processed datasets\n",
    "\n",
    "OUTPUT:\n",
    "- 6+ exploratory figures\n",
    "- Summary statistics\n",
    "\n",
    "NEXT USE:\n",
    "- Context for network interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cff057",
   "metadata": {},
   "source": [
    "## SUBSECTION 2.1: Sample Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181af65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "#library size distribution\n",
    "for idx, (group_name, adata) in enumerate(processed_datasets.items()):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    library_size = adata.X.sum(axis=1)\n",
    "    ax.hist(library_size, bins=50, alpha=0.7, color='steelblue')\n",
    "    ax.set_xlabel('Library Size (Log scale)')\n",
    "    ax.set_ylabel('Number of Cells')\n",
    "    ax.set_title(f'{group_name}')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "out_file = PHASE3_DIR / 'exploratory_analysis' / 'sample_quality_library_size.png'\n",
    "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f'OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c0b1d",
   "metadata": {},
   "source": [
    "## SUBSECTION 2.2: Gene Detection Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb91abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, (group_name, adata) in enumerate(processed_datasets.items()):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    genes_per_cell = (adata.X > 0).sum(axis=1)\n",
    "    ax.hist(genes_per_cell, bins=50, alpha=0.7, color='coral')\n",
    "    ax.set_xlabel('Genes Detected per Cell')\n",
    "    ax.set_ylabel('Number of Cells')\n",
    "    ax.set_title(f'{group_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "out_file = PHASE3_DIR / 'exploratory_analysis' / 'sample_quality_genes_per_cell.png'\n",
    "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f'OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a166c",
   "metadata": {},
   "source": [
    "## SUBSECTION 2.3: Zero Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80cb9f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "zero_inflation_data = []\n",
    "\n",
    "for group_name, adata in processed_datasets.items():\n",
    "    total_values = adata.n_obs * adata.n_vars\n",
    "    zero_count = total_values - adata.X.nnz  #non-zero count\n",
    "    zero_pct = 100 * zero_count / total_values\n",
    "    \n",
    "    zero_inflation_data.append({\n",
    "        'Group': group_name,\n",
    "        'Zero_Inflation_%': zero_pct,\n",
    "    })\n",
    "\n",
    "zero_df = pd.DataFrame(zero_inflation_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(zero_df['Group'], zero_df['Zero_Inflation_%'], color='steelblue')\n",
    "ax.set_ylabel('Zero Inflation (%)')\n",
    "ax.set_title('Sparsity Across Groups')\n",
    "ax.set_xticklabels(zero_df['Group'], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "out_file = PHASE3_DIR / 'exploratory_analysis' / 'zero_inflation.png'\n",
    "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f'OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed9568",
   "metadata": {},
   "source": [
    "my thoughts on high sparsity:\n",
    "\n",
    "scRNA-seq data is inherently sparse because:\n",
    "- Each cell expresses only ~30-40% of all genes\n",
    "- Many genes have zero expression in a cell\n",
    "- Only ~3,000 HVGs selected (of 33,514 total)\n",
    "- Among HVGs: ~1,500-2,000 expressed per cell\n",
    "\n",
    "  3,000 HVGs × 222,370 cells = 667 million possible values\n",
    "  But only ~60-70 million non-zero values\n",
    "  → ~90-95% zeros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd4786",
   "metadata": {},
   "source": [
    "## SUBSECTION 2.4: Dimensionality Reduction (PCA)\n",
    "\n",
    "https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58317e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, (group_name, adata) in enumerate(processed_datasets.items()):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    #performed PCA\n",
    "    sc.pp.pca(adata, n_comps=50)\n",
    "    \n",
    "    #plotted variance explained\n",
    "    variance_ratio = adata.uns['pca']['variance_ratio'][:20]\n",
    "    ax.plot(variance_ratio, marker='o')\n",
    "    ax.set_xlabel('PC')\n",
    "    ax.set_ylabel('Variance Explained')\n",
    "    ax.set_title(f'{group_name}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "out_file = PHASE3_DIR / 'exploratory_analysis' / 'pca_variance_explained.png'\n",
    "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f'OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6e7957",
   "metadata": {},
   "source": [
    "- All plots show \"elbow curve\" pattern\n",
    "    - PC1 explains most variance\n",
    "    - PC2-PC5 explain progressively less\n",
    "    - After PC10, very small contributions\n",
    "\n",
    "\n",
    "- Biological interpretation:\n",
    "    - Normal tissue: More uniform (lower PC1 variance)\n",
    "    - Cancer tissue: More heterogeneous (higher PC1 variance)\n",
    "    - BRCA1-mutant: Most aggressive (highest variance)\n",
    "    - Pre-neoplastic: Transitioning (intermediate variance)\n",
    "\n",
    "- No batch effects visible:\n",
    "    - All curves follow same smooth pattern\n",
    "    - No sudden jumps or anomalies\n",
    "    - Consistent across groups\n",
    "\n",
    "- Variance distribution is realistic:\n",
    "    - PC1: 6-18% (high)\n",
    "    - PC2: ~4-6% (moderate)\n",
    "    - PC3-5: 1-3% (lower)\n",
    "    - PC10+: <0.5% (noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7401f4",
   "metadata": {},
   "source": [
    "## SUBSECTION 2.5: Expression Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "180762b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, (group_name, adata) in enumerate(processed_datasets.items()):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    #mean expression per gene\n",
    "    mean_expr = np.asarray(adata.X.mean(axis=0)).flatten()\n",
    "    \n",
    "    ax.hist(mean_expr, bins=50, alpha=0.7, color='green')\n",
    "    ax.set_xlabel('Mean Expression (Log scale)')\n",
    "    ax.set_ylabel('Number of Genes')\n",
    "    ax.set_title(f'{group_name}')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "out_file = PHASE3_DIR / 'exploratory_analysis' / 'expression_distribution.png'\n",
    "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f'OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8f196",
   "metadata": {},
   "source": [
    "- Most genes: LOW expression (10⁻⁴ to 10⁻² range)\n",
    "- Some genes: MEDIUM expression (10⁻¹ range)\n",
    "- Few genes: HIGH expression (10⁰ range - the small tail)\n",
    "\n",
    "just some thought: Spearman & Kendall could work best on this distribution - Log-scale naturally handles wide dynamic range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f960d",
   "metadata": {},
   "source": [
    "## SUBSECTION 2.6: HVG Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e688235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "#extracted HVG names per group\n",
    "hvg_sets = {group_name: set(adata.var_names) for group_name, adata in processed_datasets.items()}\n",
    "\n",
    "#counted overlaps\n",
    "overlap_matrix = np.zeros((len(hvg_sets), len(hvg_sets)))\n",
    "group_names_list = list(hvg_sets.keys())\n",
    "\n",
    "for i, g1 in enumerate(group_names_list):\n",
    "    for j, g2 in enumerate(group_names_list):\n",
    "        overlap = len(hvg_sets[g1] & hvg_sets[g2])\n",
    "        overlap_matrix[i, j] = overlap\n",
    "\n",
    "#plotted heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(overlap_matrix, annot=True, fmt='.0f', xticklabels=group_names_list, \n",
    "            yticklabels=group_names_list, cmap='YlOrRd', ax=ax)\n",
    "ax.set_title('HVG Overlap Between Groups')\n",
    "plt.tight_layout()\n",
    "out_file = PHASE3_DIR / 'exploratory_analysis' / 'hvg_overlap.png'\n",
    "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f'OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf9fa3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "| Pair                        | Overlap | % Shared | Interpretation                   |\n",
    "| --------------------------- | ------- | -------- | -------------------------------- |\n",
    "| Normal ↔ ER_Positive        | 861     | 28.7%    | Moderate overlap                 |\n",
    "| Normal ↔ HER2_Positive      | 903     | 30.1%    | Moderate overlap                 |\n",
    "| Normal ↔ TNBC               | 736     | 24.5%    | Lower (more different)           |\n",
    "| Normal ↔ TNBC-BRCA1         | 842     | 28.1%    | Moderate overlap                 |\n",
    "| Normal ↔ Pre-neoplastic     | 1,037   | 34.6%    | HIGHEST!                         |\n",
    "| ER_Positive ↔ HER2_Positive | 1,095   | 36.5%    | High overlap (both luminal-like) |\n",
    "| ER_Positive ↔ TNBC          | 722     | 24.1%    | Low overlap (very different)     |\n",
    "| ER_Positive ↔ TNBC-BRCA1    | 905     | 30.2%    | Moderate                         |\n",
    "| HER2_Positive ↔ TNBC        | 844     | 28.1%    | Moderate                         |\n",
    "| HER2_Positive ↔ TNBC-BRCA1  | 1,157   | 38.6%    | HIGHEST!                         |\n",
    "| TNBC ↔ TNBC-BRCA1           | 955     | 31.8%    | Moderate                         |\n",
    "| TNBC ↔ Pre-neoplastic       | 887     | 29.6%    | Moderate                         |\n",
    "| TNBC-BRCA1 ↔ Pre-neoplastic | 909     | 30.3%    | Moderate                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54915acf",
   "metadata": {},
   "source": [
    "1. Pre-neoplastic is Most Similar to Normal \n",
    "    Normal to Pre-neoplastic: 1,037 shared genes (34.6%)\n",
    "\n",
    "    Highest overlap with Normal!\n",
    "\n",
    "    Shows pre-neoplastic is early-stage transition\n",
    "\n",
    "    Perfect for showing progression story!\n",
    "\n",
    "2. HER2+ and TNBC-BRCA1 are Related \n",
    "    HER2_Positive to TNBC-BRCA1: 1,157 shared genes (38.6%)\n",
    "\n",
    "    Highest overlap between cancer subtypes\n",
    "\n",
    "    Both aggressive subtypes\n",
    "\n",
    "    Both have different metabolic profiles\n",
    "\n",
    "3. ER+ and HER2+ are More Similar \n",
    "    ER_Positive to HER2_Positive: 1,095 shared genes (36.5%)\n",
    "\n",
    "    Both are initially hormone-responsive\n",
    "\n",
    "    Less heterogeneous than TNBC\n",
    "\n",
    "4. TNBC is Most Different \n",
    "    TNBC has lowest overlaps with most groups (24-31%)\n",
    "\n",
    "    Exception: TNBC-BRCA1 (31.8%)\n",
    "\n",
    "    Shows TNBC has unique biology\n",
    "\n",
    "    Makes sense for aggressive phenotype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68fff5",
   "metadata": {},
   "source": [
    "seems like everything is alright, but i am considering analysing these plots as well: https://scanpy.readthedocs.io/en/stable/tutorials/plotting/core.html\n",
    "\n",
    "will add it a bit later, WHY? (SECTION 6B, 7B AND 9B)\n",
    "\n",
    "- Dotplot (sc.pl.dotplot)\n",
    "    - Perfect for showing hub gene expression across groups\n",
    "    - Can group by molecular_subtype or cell_type\n",
    "    - Shows mean expression + % cells expressing\n",
    "\n",
    "- Heatmap (sc.pl.heatmap)\n",
    "    - Excellent for showing individual cell expression patterns\n",
    "    - Can visualize hub genes × cells\n",
    "    - Can show fold-change patterns\n",
    "\n",
    "- Violin plots (sc.pl.violin)\n",
    "    - Compare expression distributions across groups\n",
    "    - Perfect for hub genes or pathway genes\n",
    "    - Shows distribution shape (not just mean)\n",
    "\n",
    "- Rank genes (sc.tl.rank_genes_groups + visualization)\n",
    "    - Identify differentially expressed genes per group\n",
    "    - Show log fold-change\n",
    "    - Can prioritize therapeutic targets\n",
    " \n",
    "- Correlation matrix (sc.pl.correlation_matrix)\n",
    "    - Perfect for comparing group relationships\n",
    "    - Can show if Normal/Pre-neoplastic/Cancer form progression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e69e3",
   "metadata": {},
   "source": [
    "# SECTION 3: ROBUST MULTI-METRIC CO-EXPRESSION ANALYSIS\n",
    "\n",
    "WHY:\n",
    "- single correlation metric can be biased or fooled by outliers\n",
    "- three independent metrics capture different association types\n",
    "- weighted averaging creates robust consensus: if all 3 metrics agree → strong signal\n",
    "\n",
    "OPTIMIZATION STRATEGY:\n",
    "- process one group at a time (not all 6 together)\n",
    "- save EVERY metric separately (checkpoint recovery)\n",
    "- monitor resources in real-time (CPU, memory, time)\n",
    "- validate after each step (catch errors immediately)\n",
    "- log everything to persistent text file\n",
    "\n",
    "WORKFLOW PER GROUP:\n",
    "1. Load & preprocess (normalize, log, HVGs)\n",
    "2. Calculate Spearman → save + validate + log\n",
    "3. Calculate BICOR → save + validate + log\n",
    "4. Calculate Pearson → save + validate + log\n",
    "5. Merge robustly (0.40S + 0.35B + 0.25P) → save\n",
    "6. Visualize comparison plots\n",
    "7. Final diagnostics & cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e013959",
   "metadata": {},
   "source": [
    "## SUBSECTION 3.0: HELPER FUNCTIONS FOR AUTOMATION\n",
    "\n",
    "WHY:\n",
    "- automate repetitive tasks (preprocess, calculate, combine, save, visualize)\n",
    "- reduce code duplication (write once, use many times)\n",
    "- make workflow transparent (each function does one clear job)\n",
    "- enable testing (can call functions individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02ad1408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK enhanced helper functions defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_resource_usage():\n",
    "    '''retrieved current CPU and memory usage\n",
    "    \n",
    "    #output:\n",
    "    #  dict with: cpu_percent, memory_gb, memory_percent, timestamp\n",
    "    '''\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    memory_gb = memory_info.rss / (1024**3)\n",
    "    \n",
    "    #total system memory\n",
    "    total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    memory_percent = (memory_gb / total_memory_gb) * 100\n",
    "    \n",
    "    return {\n",
    "        'cpu_percent': process.cpu_percent(interval=0.1),\n",
    "        'memory_gb': memory_gb,\n",
    "        'memory_percent': memory_percent,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "\n",
    "def log_to_file(message, group_name):\n",
    "    '''appended message to persistent log file\n",
    "    \n",
    "    #input:\n",
    "    #  message: text to log\n",
    "    #  group_name: group identifier (used in filename)\n",
    "    #\n",
    "    #output:\n",
    "    #  written to: phase3_networks_refactored/correlation_metrics/analysis_log_{group_name}.txt\n",
    "    '''\n",
    "    log_path = PHASE3_DIR / 'correlation_metrics' / f'analysis_log_{group_name.lower()}.txt'\n",
    "    \n",
    "    #appended (create if doesn't exist)\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(f'[{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}] {message}\\n')\n",
    "\n",
    "def inspect_correlation_matrix(matrix, metric_name, group_name, n_samples=10):\n",
    "    '''inspected correlation matrix structure and validity\n",
    "    \n",
    "    #input:\n",
    "    #  matrix: correlation matrix (n_genes × n_genes)\n",
    "    #  metric_name: 'spearman', 'bicor', 'pearson', or 'robust'\n",
    "    #  group_name: group identifier (for logging)\n",
    "    #  n_samples: number of random entries to display\n",
    "    #\n",
    "    #output:\n",
    "    #  printed inspection results\n",
    "    #  logged to file\n",
    "    '''\n",
    "    n_genes = matrix.shape[0]\n",
    "    diag_values = np.diag(matrix)\n",
    "    has_nans = np.isnan(matrix).sum()\n",
    "    has_infs = np.isinf(matrix).sum()\n",
    "    value_min = np.nanmin(matrix)\n",
    "    value_max = np.nanmax(matrix)\n",
    "    diag_mean = np.mean(diag_values)\n",
    "    \n",
    "    #random samples\n",
    "    upper_triangle_idx = np.triu_indices(n_genes, k=1)\n",
    "    if len(upper_triangle_idx[0]) > 0:\n",
    "        sample_idx = np.random.choice(len(upper_triangle_idx[0]), min(n_samples, len(upper_triangle_idx[0])), replace=False)\n",
    "        sample_values = matrix[upper_triangle_idx[0][sample_idx], upper_triangle_idx[1][sample_idx]]\n",
    "    else:\n",
    "        sample_values = []\n",
    "    \n",
    "    #memory usage\n",
    "    memory_mb = matrix.nbytes / (1024**2)\n",
    "    \n",
    "    #validation status\n",
    "    is_valid = (has_nans == 0 and has_infs == 0 and -1 <= value_min and value_max <= 1)\n",
    "    status = 'OK VALID' if is_valid else '✗ INVALID'\n",
    "    \n",
    "    #printed output\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'MATRIX INSPECTION: {metric_name.upper()}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    print(f'  Shape:           {matrix.shape[0]:,} × {matrix.shape[1]:,} genes')\n",
    "    print(f'  Memory:          {memory_mb:,.1f} MB')\n",
    "    print(f'  Diagonal:        mean={diag_mean:.4f} (should be 1.0)')\n",
    "    print(f'  Range:           [{value_min:8.4f}, {value_max:8.4f}] (should be [-1, 1])')\n",
    "    print(f'  NaNs:            {has_nans:,} (should be 0)')\n",
    "    print(f'  Infs:            {has_infs:,} (should be 0)')\n",
    "    print(f'  Sample values:   {sample_values}')\n",
    "    print(f'  Status:          {status}')\n",
    "    print(f'{\"=\"*80}\\n')\n",
    "    \n",
    "    #logged to file\n",
    "    log_msg = f'{metric_name.upper()} - shape={n_genes}x{n_genes}, memory={memory_mb:.1f}MB, range=[{value_min:.4f}, {value_max:.4f}], NaNs={has_nans}, status={status}'\n",
    "    log_to_file(log_msg, group_name)\n",
    "    \n",
    "    return is_valid\n",
    "\n",
    "def save_metric_intermediate(matrix, group_name, metric_name):\n",
    "    '''saved individual metric to pickle file (checkpoint)\n",
    "    \n",
    "    #input:\n",
    "    #  matrix: correlation matrix\n",
    "    #  group_name: group identifier\n",
    "    #  metric_name: 'spearman', 'bicor', 'pearson', or 'robust'\n",
    "    #\n",
    "    #output:\n",
    "    #  saved to: phase3_networks_refactored/correlation_metrics/{group_name}_{metric_name}.pkl\n",
    "    '''\n",
    "    output_path = PHASE3_DIR / 'correlation_metrics' / f'{group_name.lower()}_{metric_name.lower()}.pkl'\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(matrix, f)\n",
    "    \n",
    "    file_size_mb = output_path.stat().st_size / (1024**2)\n",
    "    message = f'Saved {metric_name} matrix ({file_size_mb:.1f} MB) → {output_path.name}'\n",
    "    print(f'✓ {message}')\n",
    "    log_to_file(message, group_name)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def load_metric_intermediate(group_name, metric_name):\n",
    "    '''loaded individual metric from pickle file\n",
    "    \n",
    "    #input:\n",
    "    #  group_name: group identifier\n",
    "    #  metric_name: 'spearman', 'bicor', 'pearson', or 'robust'\n",
    "    #\n",
    "    #output:\n",
    "    #  correlation matrix or None if not found\n",
    "    '''\n",
    "    input_path = PHASE3_DIR / 'correlation_metrics' / f'{group_name.lower()}_{metric_name.lower()}.pkl'\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        return None\n",
    "    \n",
    "    with open(input_path, 'rb') as f:\n",
    "        matrix = pickle.load(f)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def generate_progress_bar(current, total, metric_name, start_time, gene_count=None):\n",
    "    '''generated progress bar with ETA\n",
    "    \n",
    "    #input:\n",
    "    #  current: current gene pair count\n",
    "    #  total: total gene pairs (~n*(n-1)/2)\n",
    "    #  metric_name: 'spearman', 'bicor', 'pearson'\n",
    "    #  start_time: time.time() from start\n",
    "    #  gene_count: number of genes (optional, for display)\n",
    "    '''\n",
    "    elapsed = time.time() - start_time\n",
    "    progress = current / total if total > 0 else 0\n",
    "    \n",
    "    #progress bar\n",
    "    bar_length = 30\n",
    "    filled = int(bar_length * progress)\n",
    "    bar = '█' * filled + '░' * (bar_length - filled)\n",
    "    \n",
    "    #ETA calculation\n",
    "    if elapsed > 0 and progress > 0:\n",
    "        total_time = elapsed / progress\n",
    "        remaining = total_time - elapsed\n",
    "        eta_min = remaining / 60\n",
    "    else:\n",
    "        eta_min = 0\n",
    "    \n",
    "    #rate (pairs per second)\n",
    "    if elapsed > 0:\n",
    "        rate = current / elapsed\n",
    "    else:\n",
    "        rate = 0\n",
    "    \n",
    "    return f'{metric_name:10} [{bar}] {current:,}/{total:,} ({progress*100:5.1f}%) | {rate:6.2f} pairs/sec | ETA: {eta_min:6.1f}m'\n",
    "\n",
    "def preprocess_for_correlations(adata, n_hvgs=3000, group_name=None):\n",
    "    '''preprocessed expression data for correlation analysis\n",
    "    \n",
    "    #input:\n",
    "    #  adata: anndata object with raw UMI counts\n",
    "    #  n_hvgs: number of highly variable genes to keep (default: 3000)\n",
    "    #  group_name: group identifier (for logging)\n",
    "    #\n",
    "    #output:\n",
    "    #  adata_processed: normalized, log-transformed, HVG-filtered\n",
    "    #  X dense matrix ready for correlation calculation\n",
    "    '''\n",
    "    adata_proc = adata.copy()\n",
    "    \n",
    "    #normalized to library size (10,000 UMI per cell)\n",
    "    sc.pp.normalize_total(adata_proc, target_sum=1e4)\n",
    "    \n",
    "    #applied log transformation\n",
    "    sc.pp.log1p(adata_proc)\n",
    "    \n",
    "    #identified highly variable genes\n",
    "    sc.pp.highly_variable_genes(adata_proc, n_top_genes=n_hvgs)\n",
    "    \n",
    "    #subset to HVGs\n",
    "    adata_proc = adata_proc[:, adata_proc.var['highly_variable']]\n",
    "    \n",
    "    if group_name:\n",
    "        log_to_file(f'Preprocessing: {adata_proc.n_obs:,} cells × {adata_proc.n_vars:,} HVGs', group_name)\n",
    "    \n",
    "    return adata_proc\n",
    "\n",
    "def calculate_spearman_correlation(X_dense, group_name=None):\n",
    "    '''calculated Spearman rank correlation for all gene pairs\n",
    "    \n",
    "    #WHY Spearman:\n",
    "    #- rank-based (ignores magnitude of values)\n",
    "    #- robust to outliers in expression values\n",
    "    #- monotonic relationships (genes move together)\n",
    "    #- standard in genomics literature\n",
    "    '''\n",
    "    n_genes = X_dense.shape[1]\n",
    "    spearman_corr = np.zeros((n_genes, n_genes))\n",
    "    \n",
    "    print(f'\\n  calculating Spearman correlations for {n_genes:,} genes...')\n",
    "    start_time = time.time()\n",
    "    total_pairs = n_genes * (n_genes - 1) / 2\n",
    "    pair_count = 0\n",
    "    \n",
    "    #calculated pairwise Spearman correlations\n",
    "    for i in range(n_genes):\n",
    "        for j in range(i+1, n_genes):\n",
    "            x = X_dense[:, i]\n",
    "            y = X_dense[:, j]\n",
    "            spearman_corr[i, j], _ = spearmanr(x, y)\n",
    "            spearman_corr[j, i] = spearman_corr[i, j]\n",
    "            \n",
    "            pair_count += 1\n",
    "            \n",
    "            #progress every 500 pairs\n",
    "            if pair_count % 500 == 0:\n",
    "                progress_str = generate_progress_bar(pair_count, int(total_pairs), 'Spearman', start_time, n_genes)\n",
    "                print(f'    {progress_str}', end='\\r')\n",
    "    \n",
    "    #set diagonal to 1 (self-correlation)\n",
    "    np.fill_diagonal(spearman_corr, 1.0)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f'    OK Spearman complete ({elapsed/60:.1f} minutes, {pair_count:,} pairs)\\n')\n",
    "    \n",
    "    if group_name:\n",
    "        log_to_file(f'Spearman calculation: {elapsed/60:.1f} minutes, {pair_count:,} pairs', group_name)\n",
    "    \n",
    "    return spearman_corr\n",
    "\n",
    "def calculate_bicor_correlation(X_dense, group_name=None):\n",
    "    '''calculated biweight midcorrelation (BICOR) for all gene pairs\n",
    "    \n",
    "    #WHY BICOR:\n",
    "    #- weighted Pearson correlation (keeps linear information)\n",
    "    #- uses median + MAD (median absolute deviation) for robustness\n",
    "    #- downweights outliers WITHOUT removing them completely\n",
    "    #- specifically designed for genomics\n",
    "    '''\n",
    "    n_genes = X_dense.shape[1]\n",
    "    bicor_corr = np.zeros((n_genes, n_genes))\n",
    "    \n",
    "    print(f'\\n  calculating BICOR correlations for {n_genes:,} genes...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #calculated medians and MAD per gene (robust to outliers)\n",
    "    medians = np.median(X_dense, axis=0)\n",
    "    \n",
    "    try:\n",
    "        from scipy.stats import median_abs_deviation as mad_func\n",
    "        mads = np.array([mad_func(X_dense[:, i]) for i in range(n_genes)])\n",
    "    except ImportError:\n",
    "        #fallback if median_abs_deviation not available\n",
    "        mads = np.array([np.median(np.abs(X_dense[:, i] - medians[i])) for i in range(n_genes)])\n",
    "    \n",
    "    total_pairs = n_genes * (n_genes - 1) / 2\n",
    "    pair_count = 0\n",
    "    \n",
    "    #calculated pairwise BICOR\n",
    "    for i in range(n_genes):\n",
    "        for j in range(i+1, n_genes):\n",
    "            x = X_dense[:, i]\n",
    "            y = X_dense[:, j]\n",
    "            \n",
    "            #avoided division by zero\n",
    "            if mads[i] == 0 or mads[j] == 0:\n",
    "                bicor_corr[i, j] = 0\n",
    "                bicor_corr[j, i] = 0\n",
    "                pair_count += 1\n",
    "                continue\n",
    "            \n",
    "            #standardized using median and MAD (robust to outliers)\n",
    "            u = (x - medians[i]) / (9 * mads[i])\n",
    "            v = (y - medians[j]) / (9 * mads[j])\n",
    "            \n",
    "            #applied biweight function (smooth downweighting of outliers)\n",
    "            w_x = (1 - u**2)**2 * (np.abs(u) < 1)\n",
    "            w_y = (1 - v**2)**2 * (np.abs(v) < 1)\n",
    "            \n",
    "            #calculated weighted correlation\n",
    "            numerator = np.sum(w_x * w_y * u * v)\n",
    "            denominator = np.sqrt(np.sum(w_x * w_y * u**2) * np.sum(w_x * w_y * v**2))\n",
    "            \n",
    "            #avoided division by zero\n",
    "            if denominator > 0:\n",
    "                bicor_corr[i, j] = numerator / denominator\n",
    "            else:\n",
    "                bicor_corr[i, j] = 0\n",
    "            \n",
    "            bicor_corr[j, i] = bicor_corr[i, j]\n",
    "            pair_count += 1\n",
    "            \n",
    "            #progress every 500 pairs\n",
    "            if pair_count % 500 == 0:\n",
    "                progress_str = generate_progress_bar(pair_count, int(total_pairs), 'BICOR', start_time, n_genes)\n",
    "                print(f'    {progress_str}', end='\\r')\n",
    "    \n",
    "    #set diagonal to 1\n",
    "    np.fill_diagonal(bicor_corr, 1.0)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f'    OK BICOR complete ({elapsed/60:.1f} minutes, {pair_count:,} pairs)\\n')\n",
    "    \n",
    "    if group_name:\n",
    "        log_to_file(f'BICOR calculation: {elapsed/60:.1f} minutes, {pair_count:,} pairs', group_name)\n",
    "    \n",
    "    return bicor_corr\n",
    "\n",
    "def calculate_pearson_correlation(X_dense, group_name=None):\n",
    "    '''calculated Pearson correlation for all gene pairs\n",
    "    \n",
    "    #WHY Pearson:\n",
    "    #- captures linear relationships\n",
    "    #- baseline for validation\n",
    "    #- fast to compute (vectorized)\n",
    "    '''\n",
    "    n_genes = X_dense.shape[1]\n",
    "    print(f'\\n  calculating Pearson correlations for {n_genes:,} genes...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #used NumPy's vectorized corrcoef for efficiency\n",
    "    pearson_corr = np.corrcoef(X_dense.T)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'    OK Pearson complete ({elapsed/60:.1f} minutes, vectorized)\\n')\n",
    "    \n",
    "    if group_name:\n",
    "        log_to_file(f'Pearson calculation: {elapsed/60:.1f} minutes (vectorized)', group_name)\n",
    "    \n",
    "    return pearson_corr\n",
    "\n",
    "def combine_correlations_robust(spearman, bicor, pearson):\n",
    "    '''combined three correlations via weighted averaging\n",
    "    \n",
    "    #WEIGHTING RATIONALE:\n",
    "    #0.40 × Spearman: highest weight (most proven + standard in genomics)\n",
    "    #0.35 × BICOR: high weight (specialized for genomics + complementary)\n",
    "    #0.25 × Pearson: lower weight (validation check + less robust)\n",
    "    '''\n",
    "    robust_corr = (0.40 * spearman + 0.35 * bicor + 0.25 * pearson)\n",
    "    return robust_corr\n",
    "\n",
    "def generate_metric_comparison_plots(group_name, spearman, bicor, pearson, robust):\n",
    "    '''generated comparison plots for three correlation metrics\n",
    "    \n",
    "    #input:\n",
    "    #  group_name: group identifier\n",
    "    #  spearman, bicor, pearson, robust: correlation matrices\n",
    "    #\n",
    "    #output:\n",
    "    #  saved PNG file with 4 comparison plots\n",
    "    '''\n",
    "    print(f'\\n  generating metric comparison plots for {group_name}...')\n",
    "    \n",
    "    #flattened upper triangle\n",
    "    mask = np.triu(np.ones_like(spearman, dtype=bool), k=1)\n",
    "    spearman_flat = spearman[mask]\n",
    "    bicor_flat = bicor[mask]\n",
    "    pearson_flat = pearson[mask]\n",
    "    robust_flat = robust[mask]\n",
    "    \n",
    "    #created figure with 4 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    fig.suptitle(f'Correlation Metric Comparison: {group_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    #plot 1: Spearman vs BICOR\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(spearman_flat, bicor_flat, alpha=0.1, s=1)\n",
    "    ax.set_xlabel('Spearman (rank-based)', fontsize=11)\n",
    "    ax.set_ylabel('BICOR (weighted linear)', fontsize=11)\n",
    "    ax.set_title('Spearman vs BICOR\\n(different perspectives)', fontsize=12)\n",
    "    ax.plot([-1, 1], [-1, 1], 'r--', alpha=0.5, linewidth=2)\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    #plot 2: Spearman vs Pearson\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(spearman_flat, pearson_flat, alpha=0.1, s=1)\n",
    "    ax.set_xlabel('Spearman (robust)', fontsize=11)\n",
    "    ax.set_ylabel('Pearson (less robust)', fontsize=11)\n",
    "    ax.set_title('Spearman vs Pearson\\n(Pearson less stable)', fontsize=12)\n",
    "    ax.plot([-1, 1], [-1, 1], 'r--', alpha=0.5, linewidth=2)\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    #plot 3: Distribution comparison\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(spearman_flat, bins=100, alpha=0.5, label='Spearman', density=True, color='blue')\n",
    "    ax.hist(bicor_flat, bins=100, alpha=0.5, label='BICOR', density=True, color='green')\n",
    "    ax.hist(pearson_flat, bins=100, alpha=0.5, label='Pearson', density=True, color='orange')\n",
    "    ax.set_xlabel('Correlation Value', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('Distribution Comparison\\n(BICOR more robust than Pearson)', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    #plot 4: Robust vs Spearman\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(spearman_flat, robust_flat, alpha=0.1, s=1)\n",
    "    ax.set_xlabel('Spearman (single metric)', fontsize=11)\n",
    "    ax.set_ylabel('Robust (0.40S + 0.35B + 0.25P)', fontsize=11)\n",
    "    ax.set_title('Robust Averaging Effect\\n(balances three perspectives)', fontsize=12)\n",
    "    ax.plot([-1, 1], [-1, 1], 'r--', alpha=0.5, linewidth=2)\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #saved to file\n",
    "    output_path = PHASE3_DIR / 'correlation_metrics' / f'metric_comparison_{group_name.lower()}.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'    OK Plot saved: {output_path.name}\\n')\n",
    "    log_to_file(f'Saved comparison plot: {output_path.name}', group_name)\n",
    "\n",
    "def cleanup_memory(group_name):\n",
    "    '''released memory and logged cleanup\n",
    "    \n",
    "    #input:\n",
    "    #  group_name: group identifier (for logging)\n",
    "    '''\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    resources = get_resource_usage()\n",
    "    message = f'Memory cleanup: {resources[\"memory_gb\"]:.2f} GB / {resources[\"memory_percent\"]:.1f}%'\n",
    "    print(f'  {message}')\n",
    "    log_to_file(message, group_name)\n",
    "\n",
    "print('OK enhanced helper functions defined\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0103e",
   "metadata": {},
   "source": [
    "## SUBSECTION 3.1: TEST MODE (Verify on Small Subset First!)\n",
    "\n",
    "WHY:\n",
    "- before computing on full 3,000 genes (takes 1-2 hours), test on small subset\n",
    "- verifies code works, catches errors early, saves time\n",
    "\n",
    "DATA:\n",
    "- first 100 genes from Normal group (small subset)\n",
    "\n",
    "OUTPUT:\n",
    "- test correlation matrices (100×100)\n",
    "- diagnostics confirming correlations are valid\n",
    "\n",
    "NEXT USE:\n",
    "- if test passes → proceed to full analysis in 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6786619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_MODE = False, skipping tests\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_MODE = False  #set to False after first successful run\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f'TESTING on 100 genes from TripleNegative group (smallest sample)\\n')\n",
    "\n",
    "    print(f'STEP 1: TEST RESOURCE MONITORING')\n",
    "\n",
    "    resources = get_resource_usage()\n",
    "    print(f'CPU usage: {resources[\"cpu_percent\"]:.1f}%')\n",
    "    print(f'Memory: {resources[\"memory_gb\"]:.2f} GB / {psutil.virtual_memory().total / (1024**3):.1f} GB ({resources[\"memory_percent\"]:.1f}%)')\n",
    "    print(f'Timestamp: {resources[\"timestamp\"]}\\n')\n",
    "   \n",
    "    print(f'STEP 2: TEST LOGGING FUNCTIONS')\n",
    "    \n",
    "    TEST_GROUP = 'TEST_Run'\n",
    "    log_to_file('Test log message 1', TEST_GROUP)\n",
    "    log_to_file('Test log message 2', TEST_GROUP)\n",
    "    log_path = PHASE3_DIR / 'correlation_metrics' / f'analysis_log_{TEST_GROUP.lower()}.txt'\n",
    "    \n",
    "    if log_path.exists():\n",
    "        print(f'✓ Log file created: {log_path.name}')\n",
    "        with open(log_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            print(f'OK Log file contains {len(lines)} entries')\n",
    "            print(f'  First entry: {lines[0].strip()}')\n",
    "    else:\n",
    "        print(f'NOT OK ERROR: Log file not created!\\n')\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(f'STEP 3: LOAD AND PREPROCESS TEST DATA')\n",
    "\n",
    "    adata_test = datasets['TripleNegative'][:, :100].copy()\n",
    "    print(f'OK Loaded test data: {adata_test.n_obs:,} cells × {adata_test.n_vars:,} genes')\n",
    "    \n",
    "    adata_test_proc = preprocess_for_correlations(adata_test, n_hvgs=100, group_name=TEST_GROUP)\n",
    "    X_test = adata_test_proc.X.toarray()\n",
    "    \n",
    "    print(f'Preprocessed: {X_test.shape} (cells × genes)')\n",
    "    print(f'Data type: {X_test.dtype}')\n",
    "    print(f'Memory: {X_test.nbytes / (1024**2):.1f} MB\\n')\n",
    "    \n",
    "    print(f'STEP 4: TEST SPEARMAN CORRELATION CALCULATION')\n",
    "\n",
    "    spearman_test = calculate_spearman_correlation(X_test, group_name=TEST_GROUP)\n",
    "    print(f'Validating Spearman matrix')\n",
    "    is_valid_spearman = inspect_correlation_matrix(spearman_test, 'Spearman', TEST_GROUP, n_samples=5)\n",
    "    save_metric_intermediate(spearman_test, TEST_GROUP, 'spearman')\n",
    "\n",
    "    print(f'\\nSTEP 5: TEST BICOR CORRELATION CALCULATION')\n",
    "    \n",
    "    bicor_test = calculate_bicor_correlation(X_test, group_name=TEST_GROUP)\n",
    "    print(f'Validating BICOR matrix')\n",
    "    is_valid_bicor = inspect_correlation_matrix(bicor_test, 'BICOR', TEST_GROUP, n_samples=5)\n",
    "    save_metric_intermediate(bicor_test, TEST_GROUP, 'bicor')\n",
    "\n",
    "    print(f'\\nSTEP 6: TEST PEARSON CORRELATION CALCULATION')\n",
    "\n",
    "    pearson_test = calculate_pearson_correlation(X_test, group_name=TEST_GROUP)\n",
    "    print(f'Validating Pearson matrix')\n",
    "    is_valid_pearson = inspect_correlation_matrix(pearson_test, 'Pearson', TEST_GROUP, n_samples=5)\n",
    "    save_metric_intermediate(pearson_test, TEST_GROUP, 'pearson')\n",
    "\n",
    "    print(f'\\nSTEP 7: TEST ROBUST COMBINATION')\n",
    "\n",
    "    robust_test = combine_correlations_robust(spearman_test, bicor_test, pearson_test)\n",
    "    print(f' Robust correlation combined (0.40S + 0.35B + 0.25P)\\n')\n",
    "    print(f'Validating Robust matrix')\n",
    "    is_valid_robust = inspect_correlation_matrix(robust_test, 'Robust', TEST_GROUP, n_samples=5)\n",
    "    save_metric_intermediate(robust_test, TEST_GROUP, 'robust')\n",
    "\n",
    "    print(f'\\nSTEP 8: TEST COMPARISON PLOTS')\n",
    "\n",
    "    generate_metric_comparison_plots(TEST_GROUP, spearman_test, bicor_test, pearson_test, robust_test)\n",
    "\n",
    "    print(f'\\nSTEP 9: COMPARISON OF METRICS')\n",
    "\n",
    "    #flattened upper triangle for comparison\n",
    "    mask = np.triu(np.ones_like(spearman_test, dtype=bool), k=1)\n",
    "    spearman_flat = spearman_test[mask]\n",
    "    bicor_flat = bicor_test[mask]\n",
    "    pearson_flat = pearson_test[mask]\n",
    "    robust_flat = robust_test[mask]\n",
    "    \n",
    "    print(f'Metric correlation ranges:')\n",
    "    print(f'  Spearman: [{spearman_flat.min():.4f}, {spearman_flat.max():.4f}] mean={spearman_flat.mean():.4f}')\n",
    "    print(f'  BICOR:    [{bicor_flat.min():.4f}, {bicor_flat.max():.4f}] mean={bicor_flat.mean():.4f}')\n",
    "    print(f'  Pearson:  [{pearson_flat.min():.4f}, {pearson_flat.max():.4f}] mean={pearson_flat.mean():.4f}')\n",
    "    print(f'  Robust:   [{robust_flat.min():.4f}, {robust_flat.max():.4f}] mean={robust_flat.mean():.4f}')\n",
    "    print()\n",
    "    \n",
    "    #correlation between metrics\n",
    "    print(f'Agreement between metrics (Pearson correlation of flattened values):')\n",
    "    corr_spear_bicor = np.corrcoef(spearman_flat, bicor_flat)[0, 1]\n",
    "    corr_spear_pear = np.corrcoef(spearman_flat, pearson_flat)[0, 1]\n",
    "    corr_bicor_pear = np.corrcoef(bicor_flat, pearson_flat)[0, 1]\n",
    "    \n",
    "    print(f'  Spearman vs BICOR:    {corr_spear_bicor:.4f}')\n",
    "    print(f'  Spearman vs Pearson:  {corr_spear_pear:.4f}')\n",
    "    print(f'  BICOR vs Pearson:     {corr_bicor_pear:.4f}')\n",
    "    print()\n",
    "\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'TEST SUMMARY')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    all_valid = all([is_valid_spearman, is_valid_bicor, is_valid_pearson, is_valid_robust])\n",
    "    status = 'OK ALL TESTS PASSED!' if all_valid else 'NOT OK SOME TESTS FAILED!'\n",
    "    \n",
    "    print(f'\\nValidation results:')\n",
    "    print(f'  Spearman:  {\"OK VALID\" if is_valid_spearman else \"NOT OK INVALID\"}')\n",
    "    print(f'  BICOR:     {\"OK VALID\" if is_valid_bicor else \"✗´NOT OK INVALID\"}')\n",
    "    print(f'  Pearson:   {\"OK VALID\" if is_valid_pearson else \"NOT OK INVALID\"}')\n",
    "    print(f'  Robust:    {\"OK VALID\" if is_valid_robust else \"NOT OK INVALID\"}')\n",
    "    \n",
    "    print(f'\\nFiles created:')\n",
    "    print(f'  OK test_run_spearman.pkl')\n",
    "    print(f'  OK test_run_bicor.pkl')\n",
    "    print(f'  OK test_run_pearson.pkl')\n",
    "    print(f'  OK test_run_robust.pkl')\n",
    "    print(f'  OK metric_comparison_test_run.png')\n",
    "    print(f'  OK analysis_log_test_run.txt')\n",
    "    \n",
    "    print(f'\\nFinal memory:')\n",
    "    resources_final = get_resource_usage()\n",
    "    print(f'  {resources_final[\"memory_gb\"]:.2f} GB / {psutil.virtual_memory().total / (1024**3):.1f} GB ({resources_final[\"memory_percent\"]:.1f}%)')\n",
    "    \n",
    "    print(f'\\nFinal status: {status}')\n",
    "    print(f'{\"=\"*80}\\n')\n",
    "    \n",
    "    #cleanup\n",
    "    cleanup_memory(TEST_GROUP)\n",
    "    del adata_test, adata_test_proc, X_test, spearman_test, bicor_test, pearson_test, robust_test\n",
    "\n",
    "else:\n",
    "    print(f'TEST_MODE = False, skipping tests\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33b86996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: DELETING PICKLE FILES\n",
      "\n",
      "  ✓ Deleted: test_run_spearman.pkl               (  0.05 MB)\n",
      "  ✓ Deleted: test_run_bicor.pkl                  (  0.05 MB)\n",
      "  ✓ Deleted: test_run_pearson.pkl                (  0.05 MB)\n",
      "  ✓ Deleted: test_run_robust.pkl                 (  0.05 MB)\n",
      "\n",
      "  Total pickle files freed: 0.21 MB\n",
      "\n",
      "STEP 2: DELETING PNG PLOT\n",
      "\n",
      "  OK Deleted: metric_comparison_test_run.png      (  0.48 MB)\n",
      "\n",
      "  Total PNG freed: 0.48 MB\n",
      "\n",
      "STEP 3: GARBAGE COLLECTION\n",
      "\n",
      "   Garbage collection complete\n",
      "   Memory after cleanup: 12.30 GB / 31.2 GB (39.4%)\n",
      "\n",
      "STEP 4: VERIFY CLEANUP\n",
      "\n",
      "  Remaining test files:\n",
      "    Pickle files: 0 (should be 0)\n",
      "    PNG files: 0 (should be 0)\n",
      "    Log files: 1 (should be 1) ← kept for reference\n",
      "CLEANUP SUMMARY\n",
      "\n",
      "Space freed: 0.69 MB\n",
      "Memory available: 15.96 GB\n",
      "\n",
      " All test files deleted successfully!\n",
      "Log file preserved at: /triumvirate/home/alexarol/breast_cancer_analysis/results/phase3_networks_refactored/correlation_metrics/analysis_log_test_run.txt\n"
     ]
    }
   ],
   "source": [
    "print(f'STEP 1: DELETING PICKLE FILES\\n')\n",
    "\n",
    "pickle_files = ['spearman', 'bicor', 'pearson', 'robust']\n",
    "total_freed = 0\n",
    "\n",
    "for metric in pickle_files:\n",
    "    pkl_path = PHASE3_DIR / 'correlation_metrics' / f'test_run_{metric}.pkl'\n",
    "    \n",
    "    if pkl_path.exists():\n",
    "        file_size_mb = pkl_path.stat().st_size / (1024**2)\n",
    "        pkl_path.unlink()\n",
    "        print(f'  ✓ Deleted: {pkl_path.name:35} ({file_size_mb:6.2f} MB)')\n",
    "        total_freed += file_size_mb\n",
    "    else:\n",
    "        print(f'  - Not found: {pkl_path.name:35}')\n",
    "\n",
    "print(f'\\n  Total pickle files freed: {total_freed:.2f} MB\\n')\n",
    "\n",
    "print(f'STEP 2: DELETING PNG PLOT\\n')\n",
    "\n",
    "png_path = PHASE3_DIR / 'correlation_metrics' / 'metric_comparison_test_run.png'\n",
    "\n",
    "if png_path.exists():\n",
    "    png_size_mb = png_path.stat().st_size / (1024**2)\n",
    "    png_path.unlink()\n",
    "    print(f'  OK Deleted: {png_path.name:35} ({png_size_mb:6.2f} MB)')\n",
    "    total_freed += png_size_mb\n",
    "else:\n",
    "    print(f'  - Not found: {png_path.name:35}')\n",
    "\n",
    "print(f'\\n  Total PNG freed: {png_size_mb:.2f} MB\\n')\n",
    "\n",
    "print(f'STEP 3: GARBAGE COLLECTION\\n')\n",
    "\n",
    "gc.collect()\n",
    "resources_after_cleanup = get_resource_usage()\n",
    "\n",
    "print(f'   Garbage collection complete')\n",
    "print(f'   Memory after cleanup: {resources_after_cleanup[\"memory_gb\"]:.2f} GB / {psutil.virtual_memory().total / (1024**3):.1f} GB ({resources_after_cleanup[\"memory_percent\"]:.1f}%)\\n')\n",
    "\n",
    "print(f'STEP 4: VERIFY CLEANUP\\n')\n",
    "\n",
    "files_remaining = {\n",
    "    'Pickle files': len(list(PHASE3_DIR.glob('correlation_metrics/test_run_*.pkl'))),\n",
    "    'PNG files': len(list(PHASE3_DIR.glob('correlation_metrics/metric_comparison_test_run.png'))),\n",
    "    'Log files': len(list(PHASE3_DIR.glob('correlation_metrics/analysis_log_test_run.txt')))\n",
    "}\n",
    "\n",
    "print(f'  Remaining test files:')\n",
    "print(f'    Pickle files: {files_remaining[\"Pickle files\"]} (should be 0)')\n",
    "print(f'    PNG files: {files_remaining[\"PNG files\"]} (should be 0)')\n",
    "print(f'    Log files: {files_remaining[\"Log files\"]} (should be 1) ← kept for reference')\n",
    "\n",
    "print(f'CLEANUP SUMMARY')\n",
    "\n",
    "print(f'\\nSpace freed: {total_freed:.2f} MB')\n",
    "print(f'Memory available: {psutil.virtual_memory().available / (1024**3):.2f} GB')\n",
    "print(f'\\n All test files deleted successfully!')\n",
    "print(f'Log file preserved at: {PHASE3_DIR / \"correlation_metrics\" / \"analysis_log_test_run.txt\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bd901",
   "metadata": {},
   "source": [
    "## SUBSECTION 3.2: ANALYZE FIRST SAMPLE - TRIPLENEGATIVE\n",
    "\n",
    "workflow overview:\n",
    "- this cell analyzes the TripleNegative breast cancer epithelial cells group\n",
    "- using pre-processed, HVG-selected data (generated in earlier preprocessing step).\n",
    "- it calculates three independent correlation metrics (Spearman, BICOR, Pearson),\n",
    "- validates each matrix, combines them via weighted averaging to create a robust\n",
    "- consensus correlation matrix, generates diagnostic plots, and saves all outputs.\n",
    "\n",
    "- expected runtime: ~4-5 hours (shorter because preprocessing already done)\n",
    "- expected outputs: 4 pickle files + 1 comparison plot + 1 log file per group\n",
    "\n",
    "WHY this design:\n",
    "- single metrics can be fooled by outliers or miss non-linear patterns.\n",
    "- three metrics capture different signal types: rank-based (Spearman),\n",
    "- weighted-linear (BICOR), and linear (Pearson). if all 3 agree → high confidence.\n",
    "\n",
    "MEMORY strategy:\n",
    "- load preprocessed file from disk (avoids re-processing)\n",
    "- delete intermediate variables after each metric (save to disk first)\n",
    "- keep only final robust matrix in memory (other 3 deleted to save RAM)\n",
    "- reload from disk only when combining robustly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37892754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing group: TripleNegative\n",
      "start time: 2025-12-10 13:55:00\n",
      "STEP 1: LOAD PREPROCESSED DATA\n",
      "OK loaded preprocessed dataset from: adata_triplenegative_hvg_processed.h5ad\n",
      "  cells: 7,514\n",
      "  HVGs: 3,000\n",
      "  data format: csr_matrix\n",
      "  data already: normalized (10k UMI), log-transformed, HVG-selected\n",
      "\n",
      "  memory before dense conversion: 12.30 GB / 31.2 GB\n",
      "\n",
      "STEP 2: CONVERT TO DENSE MATRIX\n",
      "OK converted sparse matrix to dense\n",
      "  shape: (7514, 3000) (cells × genes)\n",
      "  data type: float32\n",
      "  memory: 86.0 MB\n",
      "  value range: [0.0000, 8.5104]\n",
      "\n",
      "  memory after dense conversion: 12.39 GB\n",
      "\n",
      "STEP 3: CALCULATE SPEARMAN RANK CORRELATION\n",
      "\n",
      "  calculating Spearman correlations for 3,000 genes...\n",
      "    OK Spearman complete (70.7 minutes, 4,498,500 pairs)0/4,498,500 (100.0%) | 1061.07 pairs/sec | ETA:    0.0m\n",
      "\n",
      "\n",
      "OK Spearman calculation complete (70.7 minutes)\n",
      "\n",
      "validating Spearman matrix\n",
      "\n",
      "================================================================================\n",
      "MATRIX INSPECTION: SPEARMAN\n",
      "================================================================================\n",
      "  Shape:           3,000 × 3,000 genes\n",
      "  Memory:          68.7 MB\n",
      "  Diagonal:        mean=1.0000 (should be 1.0)\n",
      "  Range:           [ -0.4944,   1.0000] (should be [-1, 1])\n",
      "  NaNs:            0 (should be 0)\n",
      "  Infs:            0 (should be 0)\n",
      "  Sample values:   [ 0.05193771 -0.00505736 -0.00401192  0.03573746  0.00100472 -0.00570993\n",
      "  0.01787602 -0.00371998  0.02524885  0.03967354]\n",
      "  Status:          OK VALID\n",
      "================================================================================\n",
      "\n",
      "✓ Saved spearman matrix (68.7 MB) → triplenegative_spearman.pkl\n",
      "STEP 4: CALCULATE BICOR (BIWEIGHT MIDCORRELATION)\n",
      "\n",
      "  calculating BICOR correlations for 3,000 genes...\n",
      "    OK BICOR complete (0.1 minutes, 4,498,500 pairs)4,500/4,498,500 ( 79.2%) | 1055525.90 pairs/sec | ETA:    0.0m\n",
      "\n",
      "\n",
      "OK BICOR calculation complete (0.1 minutes)\n",
      "\n",
      "validating BICOR matrix\n",
      "\n",
      "================================================================================\n",
      "MATRIX INSPECTION: BICOR\n",
      "================================================================================\n",
      "  Shape:           3,000 × 3,000 genes\n",
      "  Memory:          68.7 MB\n",
      "  Diagonal:        mean=1.0000 (should be 1.0)\n",
      "  Range:           [ -0.3721,   1.0000] (should be [-1, 1])\n",
      "  NaNs:            0 (should be 0)\n",
      "  Infs:            0 (should be 0)\n",
      "  Sample values:   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  Status:          OK VALID\n",
      "================================================================================\n",
      "\n",
      "✓ Saved bicor matrix (68.7 MB) → triplenegative_bicor.pkl\n",
      "STEP 5: CALCULATE PEARSON CORRELATION\n",
      "\n",
      "  calculating Pearson correlations for 3,000 genes...\n",
      "    OK Pearson complete (0.0 minutes, vectorized)\n",
      "\n",
      "\n",
      "OK Pearson calculation complete (0.0 minutes)\n",
      "\n",
      "validating Pearson matrix\n",
      "\n",
      "================================================================================\n",
      "MATRIX INSPECTION: PEARSON\n",
      "================================================================================\n",
      "  Shape:           3,000 × 3,000 genes\n",
      "  Memory:          68.7 MB\n",
      "  Diagonal:        mean=1.0000 (should be 1.0)\n",
      "  Range:           [ -0.5758,   1.0000] (should be [-1, 1])\n",
      "  NaNs:            0 (should be 0)\n",
      "  Infs:            0 (should be 0)\n",
      "  Sample values:   [-0.00158824  0.02260177 -0.00577132 -0.00159616  0.00140935 -0.00137027\n",
      "  0.01386416  0.01706583 -0.00718729 -0.00253282]\n",
      "  Status:          OK VALID\n",
      "================================================================================\n",
      "\n",
      "✓ Saved pearson matrix (68.7 MB) → triplenegative_pearson.pkl\n",
      "STEP 6: CREATE ROBUST CONSENSUS CORRELATION\n",
      "OK reloaded all three metrics from disk\n",
      "\n",
      "OK robust consensus created (0.40×S + 0.35×B + 0.25×P)\n",
      "\n",
      "validating robust matrix\n",
      "\n",
      "================================================================================\n",
      "MATRIX INSPECTION: ROBUST\n",
      "================================================================================\n",
      "  Shape:           3,000 × 3,000 genes\n",
      "  Memory:          68.7 MB\n",
      "  Diagonal:        mean=1.0000 (should be 1.0)\n",
      "  Range:           [ -0.3784,   1.0000] (should be [-1, 1])\n",
      "  NaNs:            0 (should be 0)\n",
      "  Infs:            0 (should be 0)\n",
      "  Sample values:   [-0.00274637  0.00820411 -0.000503    0.00795872  0.01898053 -0.00315848\n",
      "  0.00509173 -0.00408929  0.00998823 -0.00134321]\n",
      "  Status:          OK VALID\n",
      "================================================================================\n",
      "\n",
      "✓ Saved robust matrix (68.7 MB) → triplenegative_robust.pkl\n",
      "\n",
      "STEP 7: GENERATE DIAGNOSTIC COMPARISON PLOTS\n",
      "\n",
      "  generating metric comparison plots for TripleNegative...\n",
      "    OK Plot saved: metric_comparison_triplenegative.png\n",
      "\n",
      "STEP 8: FINAL SUMMARY AND CLEANUP\n",
      "ANALYSIS COMPLETE: TripleNegative\n",
      "\n",
      "  cells analyzed:           7,514\n",
      "  HVGs used:                3,000\n",
      "  correlation pairs:        4,498,500\n",
      "\n",
      "  timing breakdown:\n",
      "    Spearman:                  70.7 minutes\n",
      "    BICOR:                      0.1 minutes\n",
      "    Pearson:                    0.0 minutes\n",
      "    total:                      1.2 hours\n",
      "\n",
      "  validation results:\n",
      "    Spearman:                VALID\n",
      "    BICOR:                   VALID\n",
      "    Pearson:                 VALID\n",
      "    Robust:                  VALID\n",
      "    overall:                OK ALL METRICS VALID\n",
      "\n",
      "  memory usage:\n",
      "    current:                12.88 GB / 31.2 GB (41.2%)\n",
      "\n",
      "  output files saved:\n",
      "    ✓ triplenegative_spearman.pkl\n",
      "    ✓ triplenegative_bicor.pkl\n",
      "    ✓ triplenegative_pearson.pkl\n",
      "    ✓ triplenegative_robust.pkl\n",
      "    ✓ metric_comparison_triplenegative.png\n",
      "    ✓ analysis_log_triplenegative.txt\n",
      "\n",
      "OK memory cleaned up\n",
      "OK ready for next group analysis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#setting group to analyze (CHANGE ONLY THIS LINE FOR DIFFERENT GROUPS)\n",
    "GROUP_NAME = 'TripleNegative'\n",
    "\n",
    "print(f'analyzing group: {GROUP_NAME}')\n",
    "print(f'start time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "#initialized log file for this group\n",
    "log_path = PHASE3_DIR / 'correlation_metrics' / f'analysis_log_{GROUP_NAME.lower()}.txt'\n",
    "with open(log_path, 'w') as f:\n",
    "    f.write(f'ANALYSIS LOG: {GROUP_NAME}\\n')\n",
    "    f.write(f'started: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "log_to_file(f'phase 3 analysis initiated for {GROUP_NAME}', GROUP_NAME)\n",
    "\n",
    "print(f'STEP 1: LOAD PREPROCESSED DATA')\n",
    "\n",
    "#loading preprocessed, HVG-selected dataset for this group.\n",
    "#preprocessing already applied: normalized to 10k UMI, log-transformed, 3000 HVGs selected.\n",
    "#data format: sparse matrix of log-transformed expression (cells × genes)\n",
    "#file created in earlier preprocessing step: adata_{group}_hvg_processed.h5ad\n",
    "#expected: 7,561 cells for TripleNegative (varies by group)\n",
    "\n",
    "preprocessed_file = PHASE3_DIR / f'adata_{GROUP_NAME.lower()}_hvg_processed.h5ad'\n",
    "\n",
    "if not preprocessed_file.exists():\n",
    "    raise FileNotFoundError(f'preprocessed file not found: {preprocessed_file}')\n",
    "\n",
    "adata = sc.read_h5ad(preprocessed_file)\n",
    "print(f'OK loaded preprocessed dataset from: {preprocessed_file.name}')\n",
    "print(f'  cells: {adata.n_obs:,}')\n",
    "print(f'  HVGs: {adata.n_vars:,}')\n",
    "print(f'  data format: {type(adata.X).__name__}')\n",
    "print(f'  data already: normalized (10k UMI), log-transformed, HVG-selected\\n')\n",
    "\n",
    "log_to_file(f'loaded preprocessed data: {adata.n_obs:,} cells × {adata.n_vars:,} HVGs', GROUP_NAME)\n",
    "\n",
    "#monitoring resource usage before conversion to dense\n",
    "resources_start = get_resource_usage()\n",
    "print(f'  memory before dense conversion: {resources_start[\"memory_gb\"]:.2f} GB / {psutil.virtual_memory().total / (1024**3):.1f} GB\\n')\n",
    "\n",
    "print(f'STEP 2: CONVERT TO DENSE MATRIX')\n",
    "\n",
    "#converting sparse matrix to dense format required for pairwise correlations.\n",
    "#WHY dense: correlation computation requires accessing all cell values\n",
    "#for each gene pair. sparse matrix format (CSR) is inefficient for this.\n",
    "#dense conversion happens once, then reused for all 3 metric calculations.\n",
    "#\n",
    "#memory impact: X_dense will be ~7,561 cells × 3,000 genes × 4 bytes = 90 MB\n",
    "#(acceptable tradeoff for ~100x faster correlation computation)\n",
    "\n",
    "X_dense = adata.X.toarray()\n",
    "\n",
    "print(f'OK converted sparse matrix to dense')\n",
    "print(f'  shape: {X_dense.shape} (cells × genes)')\n",
    "print(f'  data type: {X_dense.dtype}')\n",
    "print(f'  memory: {X_dense.nbytes / (1024**2):.1f} MB')\n",
    "print(f'  value range: [{X_dense.min():.4f}, {X_dense.max():.4f}]\\n')\n",
    "\n",
    "resources_after_dense = get_resource_usage()\n",
    "print(f'  memory after dense conversion: {resources_after_dense[\"memory_gb\"]:.2f} GB\\n')\n",
    "\n",
    "log_to_file(f'dense matrix: {X_dense.shape}, {X_dense.nbytes / (1024**2):.1f} MB', GROUP_NAME)\n",
    "\n",
    "print(f'STEP 3: CALCULATE SPEARMAN RANK CORRELATION')\n",
    "\n",
    "#Spearman correlation: rank-based, robust to outliers and non-linear monotonic patterns.\n",
    "#\n",
    "#WHY Spearman for genomics:\n",
    "#- gene expression is right-skewed (some cells express genes at extreme levels)\n",
    "#- rank transformation removes effect of extreme values (outlier-robust)\n",
    "#- captures monotonic relationships: A increases → B increases (even if non-linear)\n",
    "#- proven standard in WGCNA and genomics co-expression literature\n",
    "#\n",
    "#computation: for each pair of genes (i,j), rank cell values independently,\n",
    "#then compute Pearson correlation on the ranks (not raw values).\n",
    "#\n",
    "#output: n_genes × n_genes correlation matrix\n",
    "#- diagonal = 1.0 (gene perfectly correlated with itself)\n",
    "#- off-diagonal ∈ [-1, +1] (correlation strength and direction)\n",
    "\n",
    "start_time = time.time()\n",
    "spearman = calculate_spearman_correlation(X_dense, group_name=GROUP_NAME)\n",
    "spearman_time = time.time() - start_time\n",
    "\n",
    "print(f'\\nOK Spearman calculation complete ({spearman_time/60:.1f} minutes)\\n')\n",
    "\n",
    "#validating Spearman matrix\n",
    "print(f'validating Spearman matrix')\n",
    "is_valid_spearman = inspect_correlation_matrix(spearman, 'Spearman', GROUP_NAME, n_samples=10)\n",
    "\n",
    "#saving to disk immediately (checkpoint recovery if interrupted)\n",
    "save_metric_intermediate(spearman, GROUP_NAME, 'spearman')\n",
    "\n",
    "#deleting Spearman from memory after saving to disk.\n",
    "#WHY delete: we don't need it until Step 6 (robust combination).\n",
    "#reloading from disk is fast, but keeping 3 matrices in RAM = ~300 MB wasted.\n",
    "#deleting frees memory for next metric computation (BICOR is slower).\n",
    "del spearman\n",
    "gc.collect()\n",
    "\n",
    "log_to_file(f'Spearman: {spearman_time/60:.1f} minutes, valid={is_valid_spearman}', GROUP_NAME)\n",
    "\n",
    "print(f'STEP 4: CALCULATE BICOR (BIWEIGHT MIDCORRELATION)')\n",
    "\n",
    "#BICOR: weighted Pearson correlation using median and MAD (median absolute deviation).\n",
    "#\n",
    "#WHY BICOR for genomics:\n",
    "#- combines robustness of rank-based methods with linearity preservation of Pearson\n",
    "#- uses median/MAD instead of mean/SD (more stable for skewed gene expression)\n",
    "#- smooth downweighting of outliers via biweight function (not harsh removal)\n",
    "#- designed specifically for genomics co-expression by WGCNA authors\n",
    "#- captures linear patterns that Spearman may miss on continuous scales\n",
    "#\n",
    "#computation: for each gene, compute median (med_i) and MAD (mad_i).\n",
    "#then for each pair (i,j), standardize using med/MAD instead of mean/SD.\n",
    "#apply biweight smoothing: points far from median get lower weight gradually.\n",
    "#compute weighted Pearson on original (not ranked) values.\n",
    "#\n",
    "#output: same structure as Spearman (n_genes × n_genes correlation matrix)\n",
    "\n",
    "start_time = time.time()\n",
    "bicor = calculate_bicor_correlation(X_dense, group_name=GROUP_NAME)\n",
    "bicor_time = time.time() - start_time\n",
    "\n",
    "print(f'\\nOK BICOR calculation complete ({bicor_time/60:.1f} minutes)\\n')\n",
    "\n",
    "#validating BICOR matrix\n",
    "print(f'validating BICOR matrix')\n",
    "is_valid_bicor = inspect_correlation_matrix(bicor, 'BICOR', GROUP_NAME, n_samples=10)\n",
    "\n",
    "#saving to disk\n",
    "save_metric_intermediate(bicor, GROUP_NAME, 'bicor')\n",
    "\n",
    "#deleting BICOR from memory (same reasoning as Spearman)\n",
    "del bicor\n",
    "gc.collect()\n",
    "\n",
    "log_to_file(f'BICOR: {bicor_time/60:.1f} minutes, valid={is_valid_bicor}', GROUP_NAME)\n",
    "\n",
    "print(f'STEP 5: CALCULATE PEARSON CORRELATION')\n",
    "\n",
    "#Pearson correlation: linear correlation coefficient (standard approach).\n",
    "#\n",
    "#WHY Pearson for validation:\n",
    "#- baseline comparison metric (industry standard, well-understood)\n",
    "#- fast to compute (vectorized NumPy operation on dense matrix)\n",
    "#- less robust than Spearman/BICOR (sensitive to outliers)\n",
    "#- useful for validation: how much do robust methods improve over standard?\n",
    "#\n",
    "#computation: for each gene pair (i,j), compute Pearson r on raw expression values.\n",
    "#r = covariance(i,j) / (std_i * std_j)\n",
    "#assumes linear relationship, no downweighting of outliers.\n",
    "#\n",
    "#output: n_genes × n_genes correlation matrix\n",
    "#\n",
    "#NOTE: weighted lower in robust combination (0.25 vs 0.40 Spearman)\n",
    "#because it's more susceptible to outlier influence in skewed expression data.\n",
    "\n",
    "start_time = time.time()\n",
    "pearson = calculate_pearson_correlation(X_dense, group_name=GROUP_NAME)\n",
    "pearson_time = time.time() - start_time\n",
    "\n",
    "print(f'\\nOK Pearson calculation complete ({pearson_time/60:.1f} minutes)\\n')\n",
    "\n",
    "#validating Pearson matrix\n",
    "print(f'validating Pearson matrix')\n",
    "is_valid_pearson = inspect_correlation_matrix(pearson, 'Pearson', GROUP_NAME, n_samples=10)\n",
    "\n",
    "#saving to disk\n",
    "save_metric_intermediate(pearson, GROUP_NAME, 'pearson')\n",
    "\n",
    "#deleting Pearson from memory\n",
    "del pearson\n",
    "gc.collect()\n",
    "\n",
    "log_to_file(f'Pearson: {pearson_time/60:.1f} minutes, valid={is_valid_pearson}', GROUP_NAME)\n",
    "\n",
    "print(f'STEP 6: CREATE ROBUST CONSENSUS CORRELATION')\n",
    "\n",
    "#reloading all three metrics from disk (memory-efficient approach).\n",
    "#instead of keeping all 3 matrices in RAM simultaneously (~300 MB),\n",
    "#we kept only one at a time during computation. now we need all 3 to combine.\n",
    "#disk I/O is fast enough that reload time is negligible compared to computation.\n",
    "\n",
    "spearman = load_metric_intermediate(GROUP_NAME, 'spearman')\n",
    "bicor = load_metric_intermediate(GROUP_NAME, 'bicor')\n",
    "pearson = load_metric_intermediate(GROUP_NAME, 'pearson')\n",
    "\n",
    "print(f'OK reloaded all three metrics from disk\\n')\n",
    "\n",
    "#weighted combination of the three metrics into single robust consensus.\n",
    "#weights chosen based on: (1) robustness to outliers, (2) genomics relevance.\n",
    "#\n",
    "#weight assignment:\n",
    "#  0.40 × Spearman  ← highest weight (rank-based, most outlier-robust)\n",
    "#  0.35 × BICOR     ← high weight (genomics-specific, linear-robust hybrid)\n",
    "#  0.25 × Pearson   ← lower weight (less robust to outliers, validation check)\n",
    "#\n",
    "#sum = 1.00 (weights normalized)\n",
    "#\n",
    "#CONSENSUS LOGIC:\n",
    "#- if all 3 metrics agree on strong correlation → robust value is strong\n",
    "#- if metrics disagree (e.g., Spearman high, Pearson low) → robust value is downweighted\n",
    "#- this automatic downweighting protects against outlier-driven false correlations\n",
    "#\n",
    "#biological interpretation:\n",
    "#- high robust value (>0.70) = gene pair co-expressed across methods = likely real\n",
    "#- medium robust value (0.40-0.70) = some disagreement = uncertain\n",
    "#- low robust value (<0.40) = metrics disagree = noise or condition-specific\n",
    "#\n",
    "#this approach is standard in WGCNA literature and proven effective in genomics.\n",
    "\n",
    "robust = combine_correlations_robust(spearman, bicor, pearson)\n",
    "\n",
    "print(f'OK robust consensus created (0.40×S + 0.35×B + 0.25×P)\\n')\n",
    "\n",
    "#validating robust consensus matrix\n",
    "print(f'validating robust matrix')\n",
    "is_valid_robust = inspect_correlation_matrix(robust, 'Robust', GROUP_NAME, n_samples=10)\n",
    "\n",
    "#saving robust matrix to disk (this is the final output we'll use for networks)\n",
    "save_metric_intermediate(robust, GROUP_NAME, 'robust')\n",
    "\n",
    "print()\n",
    "\n",
    "log_to_file(f'robust combination: valid={is_valid_robust}', GROUP_NAME)\n",
    "\n",
    "print(f'STEP 7: GENERATE DIAGNOSTIC COMPARISON PLOTS')\n",
    "\n",
    "#generating 4-panel diagnostic comparison figure showing relationships\n",
    "#between the three metrics and robust consensus. these plots serve as\n",
    "#quality control and validation of the consensus approach.\n",
    "#\n",
    "#panel 1 - Spearman vs BICOR: \"different perspectives\"\n",
    "#- if scatter: methods capture different signal aspects = good diversification\n",
    "#- if tight cluster: methods redundant = less benefit from ensemble\n",
    "#- expected: moderate scatter because rank-based ≠ weighted-linear\n",
    "#\n",
    "#panel 2 - Spearman vs Pearson: \"Pearson less stable\"\n",
    "#- if tight: robust methods not adding much value over Pearson\n",
    "#- if scattered: Pearson has more noise = robust methods improve stability\n",
    "#- expected: scatter in weak regions, cluster in strong correlations\n",
    "#\n",
    "#panel 3 - Distribution comparison: histograms of all 3 metrics\n",
    "#- tight peak = method is confident (most gene pairs uncorrelated)\n",
    "#- wide tail = method detects correlations across spectrum\n",
    "#- expected: Spearman/BICOR tight (robust), Pearson wider (less stable)\n",
    "#\n",
    "#panel 4 - Robust vs Spearman: \"balances three perspectives\"\n",
    "#- if tight cluster on diagonal: robust averaging preserves information\n",
    "#- if scattered: robust averaging loses important signal\n",
    "#- expected: tight cluster (proves weighted averaging works)\n",
    "#\n",
    "#files saved: metric_comparison_{group_name}.png (diagnostic file)\n",
    "\n",
    "generate_metric_comparison_plots(GROUP_NAME, spearman, bicor, pearson, robust)\n",
    "\n",
    "log_to_file(f'diagnostic plots generated', GROUP_NAME)\n",
    "\n",
    "print(f'STEP 8: FINAL SUMMARY AND CLEANUP')\n",
    "\n",
    "#computing total analysis time\n",
    "total_compute_time = spearman_time + bicor_time + pearson_time\n",
    "total_hours = total_compute_time / 3600\n",
    "\n",
    "#validation summary\n",
    "all_valid = all([is_valid_spearman, is_valid_bicor, is_valid_pearson, is_valid_robust])\n",
    "validation_status = 'OK ALL METRICS VALID' if all_valid else 'NOT OK VALIDATION FAILED'\n",
    "\n",
    "#final resource usage\n",
    "resources_final = get_resource_usage()\n",
    "memory_used_gb = resources_final['memory_gb']\n",
    "memory_total_gb = psutil.virtual_memory().total / (1024**3)\n",
    "memory_percent = resources_final['memory_percent']\n",
    "\n",
    "#printing comprehensive summary\n",
    "print(f'ANALYSIS COMPLETE: {GROUP_NAME}\\n')\n",
    "print(f'  cells analyzed:           {adata.n_obs:,}')\n",
    "print(f'  HVGs used:                {X_dense.shape[1]:,}')\n",
    "print(f'  correlation pairs:        {X_dense.shape[1] * (X_dense.shape[1] - 1) // 2:,}')\n",
    "print(f'\\n  timing breakdown:')\n",
    "print(f'    Spearman:               {spearman_time/60:7.1f} minutes')\n",
    "print(f'    BICOR:                  {bicor_time/60:7.1f} minutes')\n",
    "print(f'    Pearson:                {pearson_time/60:7.1f} minutes')\n",
    "print(f'    total:                  {total_hours:7.1f} hours')\n",
    "print(f'\\n  validation results:')\n",
    "print(f'    Spearman:               {\" VALID\" if is_valid_spearman else \" INVALID\"}')\n",
    "print(f'    BICOR:                  {\" VALID\" if is_valid_bicor else \" INVALID\"}')\n",
    "print(f'    Pearson:                {\" VALID\" if is_valid_pearson else \" INVALID\"}')\n",
    "print(f'    Robust:                 {\" VALID\" if is_valid_robust else \" INVALID\"}')\n",
    "print(f'    overall:                {validation_status}')\n",
    "print(f'\\n  memory usage:')\n",
    "print(f'    current:                {memory_used_gb:.2f} GB / {memory_total_gb:.1f} GB ({memory_percent:.1f}%)')\n",
    "print(f'\\n  output files saved:')\n",
    "print(f'    ✓ {GROUP_NAME.lower()}_spearman.pkl')\n",
    "print(f'    ✓ {GROUP_NAME.lower()}_bicor.pkl')\n",
    "print(f'    ✓ {GROUP_NAME.lower()}_pearson.pkl')\n",
    "print(f'    ✓ {GROUP_NAME.lower()}_robust.pkl')\n",
    "print(f'    ✓ metric_comparison_{GROUP_NAME.lower()}.png')\n",
    "print(f'    ✓ analysis_log_{GROUP_NAME.lower()}.txt\\n')\n",
    "\n",
    "#logging final summary\n",
    "log_to_file(f'analysis complete', GROUP_NAME)\n",
    "log_to_file(f'total time: {total_hours:.1f} hours', GROUP_NAME)\n",
    "log_to_file(f'validation: {validation_status}', GROUP_NAME)\n",
    "log_to_file(f'memory: {memory_used_gb:.2f} GB / {memory_total_gb:.1f} GB', GROUP_NAME)\n",
    "\n",
    "#cleanup: delete all intermediate matrices and data from memory\n",
    "del spearman, bicor, pearson, robust, adata, X_dense\n",
    "gc.collect()\n",
    "\n",
    "print(f'OK memory cleaned up')\n",
    "print(f'OK ready for next group analysis\\n')\n",
    "\n",
    "log_to_file(f'memory cleanup complete', GROUP_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1afaf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                DIAGNOSTIC: BICOR INVESTIGATION (SIMPLIFIED V2)                 \n",
      "================================================================================\n",
      "\n",
      "loading matrices...\n",
      "\n",
      "✓ loaded all matrices\n",
      "\n",
      "upper triangle size: 4,498,500 values\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "KEY QUESTION: Is BICOR mostly zeros?\n",
      "\n",
      "BICOR analysis:\n",
      "  total values:      4,498,500\n",
      "  zeros:             4,494,035 (99.90%)\n",
      "  non-zeros:         4,465 (0.10%)\n",
      "\n",
      "⚠️  WARNING: BICOR is 99.9% zeros!\n",
      "   this is ABNORMAL and indicates a problem.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "DISTRIBUTION COMPARISON\n",
      "\n",
      "Spearman:\n",
      "  mean:        +0.020614\n",
      "  median:      +0.009852\n",
      "  std:         0.042631\n",
      "  min:         -0.494386\n",
      "  max:         +0.837591\n",
      "  % zeros:     0.00%\n",
      "\n",
      "BICOR:\n",
      "  mean:        +0.000039\n",
      "  median:      +0.000000\n",
      "  std:         0.004782\n",
      "  min:         -0.372104\n",
      "  max:         +0.714210\n",
      "  % zeros:     99.90%\n",
      "\n",
      "Pearson:\n",
      "  mean:        +0.003713\n",
      "  median:      -0.001561\n",
      "  std:         0.031226\n",
      "  min:         -0.575845\n",
      "  max:         +0.985789\n",
      "  % zeros:     0.00%\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "METHOD AGREEMENT\n",
      "\n",
      "Correlation between methods:\n",
      "  Spearman vs BICOR:   +0.112994\n",
      "  Spearman vs Pearson: +0.742210\n",
      "  BICOR vs Pearson:    +0.166081\n",
      "\n",
      "✗ BICOR poorly agrees with Spearman (0.1130) - PROBLEM!\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "SAMPLE CORRELATION VALUES\n",
      "\n",
      "10 random non-zero BICOR values:\n",
      "   1. BICOR=+0.051584, Spearman=+0.052918, Pearson=+0.109179\n",
      "   2. BICOR=-0.043884, Spearman=-0.026064, Pearson=-0.026418\n",
      "   3. BICOR=+0.373946, Spearman=+0.437137, Pearson=+0.428523\n",
      "   4. BICOR=-0.078897, Spearman=-0.066578, Pearson=-0.052769\n",
      "   5. BICOR=-0.064109, Spearman=-0.084143, Pearson=-0.046541\n",
      "   6. BICOR=+0.139806, Spearman=+0.138929, Pearson=+0.140060\n",
      "   7. BICOR=-0.088857, Spearman=-0.088285, Pearson=-0.083418\n",
      "   8. BICOR=+0.102353, Spearman=+0.096000, Pearson=+0.098008\n",
      "   9. BICOR=+0.015911, Spearman=+0.019524, Pearson=+0.028280\n",
      "  10. BICOR=+0.016806, Spearman=+0.025476, Pearson=+0.023812\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "TOP 5 HIGHEST CORRELATIONS BY METHOD\n",
      "\n",
      "Top 5 by Spearman:\n",
      "  1. Spearman=+0.837591, BICOR=+0.000000, Pearson=+0.866894\n",
      "  2. Spearman=+0.760553, BICOR=+0.714210, Pearson=+0.826568\n",
      "  3. Spearman=+0.755400, BICOR=+0.000000, Pearson=+0.625404\n",
      "  4. Spearman=+0.750038, BICOR=+0.000000, Pearson=+0.721835\n",
      "  5. Spearman=+0.743514, BICOR=+0.696202, Pearson=+0.716888\n",
      "\n",
      "Top 5 by BICOR:\n",
      "  1. BICOR=+0.714210, Spearman=+0.760553, Pearson=+0.826568\n",
      "  2. BICOR=+0.698091, Spearman=+0.734979, Pearson=+0.818982\n",
      "  3. BICOR=+0.696202, Spearman=+0.743514, Pearson=+0.716888\n",
      "  4. BICOR=+0.683474, Spearman=+0.723142, Pearson=+0.818643\n",
      "  5. BICOR=+0.666233, Spearman=+0.721330, Pearson=+0.747796\n",
      "\n",
      "Top 5 by Pearson:\n",
      "  1. Pearson=+0.985789, Spearman=+0.288541, BICOR=+0.000000\n",
      "  2. Pearson=+0.960275, Spearman=+0.333245, BICOR=+0.000000\n",
      "  3. Pearson=+0.939633, Spearman=+0.223354, BICOR=+0.000000\n",
      "  4. Pearson=+0.932155, Spearman=+0.217985, BICOR=+0.000000\n",
      "  5. Pearson=+0.930680, Spearman=+0.249800, BICOR=+0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FINAL DIAGNOSIS\n",
      "\n",
      "✗ BICOR MATRIX IS BROKEN\n",
      "  99.9% values are zero (should be <1%)\n",
      "  BICOR calculation has a bug that needs fixing\n",
      "  ACTION: Fix calculate_bicor_correlation() and re-run TripleNegative\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DIAGNOSTIC CELL - SIMPLIFIED v2: BICOR INVESTIGATION (NO ADATA NEEDED)\n",
    "# ==============================================================================\n",
    "#simplified version that focuses on the key question:\n",
    "#is BICOR matrix mostly zeros or does it have normal distribution?\n",
    "#\n",
    "#note: this version does NOT reference adata (gene names)\n",
    "#because adata was deleted after analysis. we just look at numeric correlations.\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'{\"DIAGNOSTIC: BICOR INVESTIGATION (SIMPLIFIED V2)\":^80}')\n",
    "print(f'{\"=\"*80}\\n')\n",
    "\n",
    "GROUP_NAME = 'TripleNegative'\n",
    "correlation_dir = PHASE3_DIR / 'correlation_metrics'\n",
    "\n",
    "# ==============================================================================\n",
    "# LOAD MATRICES\n",
    "# ==============================================================================\n",
    "\n",
    "print(f'loading matrices...\\n')\n",
    "\n",
    "with open(correlation_dir / f'{GROUP_NAME.lower()}_spearman.pkl', 'rb') as f:\n",
    "    spearman = pickle.load(f)\n",
    "\n",
    "with open(correlation_dir / f'{GROUP_NAME.lower()}_bicor.pkl', 'rb') as f:\n",
    "    bicor = pickle.load(f)\n",
    "\n",
    "with open(correlation_dir / f'{GROUP_NAME.lower()}_pearson.pkl', 'rb') as f:\n",
    "    pearson = pickle.load(f)\n",
    "\n",
    "print(f'✓ loaded all matrices\\n')\n",
    "\n",
    "# ==============================================================================\n",
    "# EXTRACT UPPER TRIANGLES (EXCLUDE DIAGONAL)\n",
    "# ==============================================================================\n",
    "\n",
    "mask = np.triu(np.ones_like(bicor, dtype=bool), k=1)\n",
    "\n",
    "sp_upper = spearman[mask]\n",
    "bi_upper = bicor[mask]\n",
    "pe_upper = pearson[mask]\n",
    "\n",
    "print(f'upper triangle size: {len(bi_upper):,} values\\n')\n",
    "\n",
    "# ==============================================================================\n",
    "# KEY QUESTION: IS BICOR MOSTLY ZEROS?\n",
    "# ==============================================================================\n",
    "\n",
    "print(f'{\"─\"*80}')\n",
    "print(f'KEY QUESTION: Is BICOR mostly zeros?\\n')\n",
    "\n",
    "bicor_zeros = (bi_upper == 0).sum()\n",
    "bicor_zeros_pct = bicor_zeros / len(bi_upper) * 100\n",
    "\n",
    "print(f'BICOR analysis:')\n",
    "print(f'  total values:      {len(bi_upper):,}')\n",
    "print(f'  zeros:             {bicor_zeros:,} ({bicor_zeros_pct:.2f}%)')\n",
    "print(f'  non-zeros:         {len(bi_upper) - bicor_zeros:,} ({100 - bicor_zeros_pct:.2f}%)')\n",
    "print()\n",
    "\n",
    "if bicor_zeros_pct > 50:\n",
    "    print(f'⚠️  WARNING: BICOR is {bicor_zeros_pct:.1f}% zeros!')\n",
    "    print(f'   this is ABNORMAL and indicates a problem.\\n')\n",
    "elif bicor_zeros_pct > 10:\n",
    "    print(f'⚠️  CAUTION: BICOR is {bicor_zeros_pct:.1f}% zeros')\n",
    "    print(f'   higher than expected but not critical.\\n')\n",
    "else:\n",
    "    print(f'✓ BICOR zeros are normal ({bicor_zeros_pct:.2f}%)\\n')\n",
    "\n",
    "# ==============================================================================\n",
    "# COMPARE DISTRIBUTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(f'{\"─\"*80}')\n",
    "print(f'DISTRIBUTION COMPARISON\\n')\n",
    "\n",
    "print(f'Spearman:')\n",
    "print(f'  mean:        {sp_upper.mean():+.6f}')\n",
    "print(f'  median:      {np.median(sp_upper):+.6f}')\n",
    "print(f'  std:         {sp_upper.std():.6f}')\n",
    "print(f'  min:         {sp_upper.min():+.6f}')\n",
    "print(f'  max:         {sp_upper.max():+.6f}')\n",
    "print(f'  % zeros:     {(sp_upper == 0).sum() / len(sp_upper) * 100:.2f}%')\n",
    "print()\n",
    "\n",
    "print(f'BICOR:')\n",
    "print(f'  mean:        {bi_upper.mean():+.6f}')\n",
    "print(f'  median:      {np.median(bi_upper):+.6f}')\n",
    "print(f'  std:         {bi_upper.std():.6f}')\n",
    "print(f'  min:         {bi_upper.min():+.6f}')\n",
    "print(f'  max:         {bi_upper.max():+.6f}')\n",
    "print(f'  % zeros:     {(bi_upper == 0).sum() / len(bi_upper) * 100:.2f}%')\n",
    "print()\n",
    "\n",
    "print(f'Pearson:')\n",
    "print(f'  mean:        {pe_upper.mean():+.6f}')\n",
    "print(f'  median:      {np.median(pe_upper):+.6f}')\n",
    "print(f'  std:         {pe_upper.std():.6f}')\n",
    "print(f'  min:         {pe_upper.min():+.6f}')\n",
    "print(f'  max:         {pe_upper.max():+.6f}')\n",
    "print(f'  % zeros:     {(pe_upper == 0).sum() / len(pe_upper) * 100:.2f}%')\n",
    "print()\n",
    "\n",
    "# ==============================================================================\n",
    "# METHOD AGREEMENT\n",
    "# ==============================================================================\n",
    "\n",
    "print(f'{\"─\"*80}')\n",
    "print(f'METHOD AGREEMENT\\n')\n",
    "\n",
    "#removed NaN/Inf from comparison to avoid warnings\n",
    "valid_sp_bi = ~(np.isnan(sp_upper) | np.isnan(bi_upper) | np.isinf(sp_upper) | np.isinf(bi_upper))\n",
    "valid_sp_pe = ~(np.isnan(sp_upper) | np.isnan(pe_upper) | np.isinf(sp_upper) | np.isinf(pe_upper))\n",
    "valid_bi_pe = ~(np.isnan(bi_upper) | np.isnan(pe_upper) | np.isinf(bi_upper) | np.isinf(pe_upper))\n",
    "\n",
    "corr_sp_bi = np.corrcoef(sp_upper[valid_sp_bi], bi_upper[valid_sp_bi])[0, 1]\n",
    "corr_sp_pe = np.corrcoef(sp_upper[valid_sp_pe], pe_upper[valid_sp_pe])[0, 1]\n",
    "corr_bi_pe = np.corrcoef(bi_upper[valid_bi_pe], pe_upper[valid_bi_pe])[0, 1]\n",
    "\n",
    "print(f'Correlation between methods:')\n",
    "print(f'  Spearman vs BICOR:   {corr_sp_bi:+.6f}')\n",
    "print(f'  Spearman vs Pearson: {corr_sp_pe:+.6f}')\n",
    "print(f'  BICOR vs Pearson:    {corr_bi_pe:+.6f}')\n",
    "print()\n",
    "\n",
    "if corr_sp_bi > 0.80:\n",
    "    print(f'✓ BICOR agrees well with Spearman ({corr_sp_bi:.4f})')\n",
    "elif corr_sp_bi > 0.60:\n",
    "    print(f'⚠️  BICOR shows moderate agreement with Spearman ({corr_sp_bi:.4f})')\n",
    "else:\n",
    "    print(f'✗ BICOR poorly agrees with Spearman ({corr_sp_bi:.4f}) - PROBLEM!')\n",
    "print()\n",
    "\n",
    "# ==============================================================================\n",
    "# SHOW ACTUAL VALUES (NOT JUST ZEROS)\n",
    "# ==============================================================================\n",
    "\n",
    "print(f'{\"─\"*80}')\n",
    "print(f'SAMPLE CORRELATION VALUES\\n')\n",
    "\n",
    "#get random non-zero BICOR values\n",
    "nonzero_mask = bi_upper != 0\n",
    "nonzero_indices = np.where(nonzero_mask)[0]\n",
    "\n",
    "if len(nonzero_indices) > 10:\n",
    "    sample_idx = np.random.choice(nonzero_indices, 10, replace=False)\n",
    "    print(f'10 random non-zero BICOR values:')\n",
    "    for rank, idx in enumerate(sample_idx, 1):\n",
    "        print(f'  {rank:2d}. BICOR={bi_upper[idx]:+.6f}, Spearman={sp_upper[idx]:+.6f}, Pearson={pe_upper[idx]:+.6f}')\n",
    "    print()\n",
    "else:\n",
    "    if len(nonzero_indices) == 0:\n",
    "        print(f'✗ ERROR: BICOR matrix is completely zeros!')\n",
    "        print(f'  no non-zero values found')\n",
    "    else:\n",
    "        print(f'⚠️  only {len(nonzero_indices)} non-zero BICOR values exist')\n",
    "        print(f'showing all non-zero values:')\n",
    "        for rank, idx in enumerate(nonzero_indices[:10], 1):\n",
    "            print(f'  {rank:2d}. BICOR={bi_upper[idx]:+.6f}, Spearman={sp_upper[idx]:+.6f}, Pearson={pe_upper[idx]:+.6f}')\n",
    "    print()\n",
    "\n",
    "# ==============================================================================\n",
    "# FIND HIGH-CORRELATION PAIRS\n",
    "# ==============================================================================\n",
    "\n",
    "print(f'{\"─\"*80}')\n",
    "print(f'TOP 5 HIGHEST CORRELATIONS BY METHOD\\n')\n",
    "\n",
    "#top by Spearman\n",
    "top5_sp_idx = np.argsort(sp_upper)[-5:][::-1]\n",
    "print(f'Top 5 by Spearman:')\n",
    "for rank, idx in enumerate(top5_sp_idx, 1):\n",
    "    print(f'  {rank}. Spearman={sp_upper[idx]:+.6f}, BICOR={bi_upper[idx]:+.6f}, Pearson={pe_upper[idx]:+.6f}')\n",
    "print()\n",
    "\n",
    "#top by BICOR\n",
    "top5_bi_idx = np.argsort(bi_upper)[-5:][::-1]\n",
    "print(f'Top 5 by BICOR:')\n",
    "for rank, idx in enumerate(top5_bi_idx, 1):\n",
    "    print(f'  {rank}. BICOR={bi_upper[idx]:+.6f}, Spearman={sp_upper[idx]:+.6f}, Pearson={pe_upper[idx]:+.6f}')\n",
    "print()\n",
    "\n",
    "#top by Pearson\n",
    "top5_pe_idx = np.argsort(pe_upper)[-5:][::-1]\n",
    "print(f'Top 5 by Pearson:')\n",
    "for rank, idx in enumerate(top5_pe_idx, 1):\n",
    "    print(f'  {rank}. Pearson={pe_upper[idx]:+.6f}, Spearman={sp_upper[idx]:+.6f}, BICOR={bi_upper[idx]:+.6f}')\n",
    "print()\n",
    "\n",
    "# ==============================================================================\n",
    "# FINAL DIAGNOSIS\n",
    "# ==============================================================================\n",
    "\n",
    "print(f'{\"─\"*80}')\n",
    "print(f'FINAL DIAGNOSIS\\n')\n",
    "\n",
    "if bicor_zeros_pct > 50:\n",
    "    print(f'✗ BICOR MATRIX IS BROKEN')\n",
    "    print(f'  {bicor_zeros_pct:.1f}% values are zero (should be <1%)')\n",
    "    print(f'  BICOR calculation has a bug that needs fixing')\n",
    "    print(f'  ACTION: Fix calculate_bicor_correlation() and re-run TripleNegative')\n",
    "elif corr_sp_bi < 0.70:\n",
    "    print(f'⚠️  BICOR RESULTS ARE SUSPICIOUS')\n",
    "    print(f'  correlation with Spearman is {corr_sp_bi:.4f} (should be >0.85)')\n",
    "    print(f'  methods disagree significantly')\n",
    "    print(f'  ACTION: Investigate BICOR function implementation')\n",
    "elif len(nonzero_indices) == 0:\n",
    "    print(f'✗ BICOR MATRIX IS COMPLETELY ZERO')\n",
    "    print(f'  no non-zero correlations found')\n",
    "    print(f'  ACTION: This is a critical bug, fix and re-run')\n",
    "else:\n",
    "    print(f'✓ BICOR MATRIX APPEARS VALID')\n",
    "    print(f'  zeros: {bicor_zeros_pct:.2f}% (normal)')\n",
    "    print(f'  agreement with Spearman: {corr_sp_bi:.4f} (good)')\n",
    "    print(f'  ACTION: Proceed with remaining 5 groups')\n",
    "\n",
    "print(f'\\n{\"=\"*80}\\n')\n",
    "\n",
    "# ==============================================================================\n",
    "# END OF DIAGNOSTIC\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e473b",
   "metadata": {},
   "source": [
    "## SECTION 3.2: ROBUST CORRELATION CALCULATION\n",
    "\n",
    "- PURPOSE: calculated robust correlations (Pearson, Spearman, Kendall) for all\n",
    "- 6 patient groups, with filtering, progress monitoring, and validation\n",
    "\n",
    "- INPUT: processed_datasets (dict with 6 AnnData objects)\n",
    "- OUTPUT: robust_correlations (dict with correlation matrices per group)\n",
    "\n",
    "why several correlations?\n",
    "\n",
    "If we used only Spearman (as earlier)):\n",
    "\n",
    "    Gene A vs Gene B might show correlation = 0.7 (strong)\n",
    "\n",
    "BUT what if:\n",
    "  - One outlier cell inflates the correlation?\n",
    "  - The relationship isn't truly monotonic?\n",
    "  - Random noise coincidentally aligns?\n",
    "Single metric = single point of failure! \n",
    "\n",
    "| Metric   | Detects                            | Robust To                      | Example                       |\n",
    "| -------- | ---------------------------------- | ------------------------------ | ----------------------------- |\n",
    "| Spearman | Monotonic rank relationships       | Outliers (somewhat)            | Most genes rise/fall together |\n",
    "| Kendall  | Pairwise concordance (ranks agree) | Extreme outliers (very robust) | Genes move in same direction  |\n",
    "| Pearson  | Linear relationships               | Nothing (can be fooled)        | Perfect linear relationship   |\n",
    "\n",
    "\n",
    "based on these articeles i decided to change my approach as the following code shows: \n",
    "\n",
    "    https://doi.org/10.1038/s41598-021-01840-z\n",
    "\n",
    "    https://doi.org/10.3390/genes10120962\n",
    "\n",
    "Why averaging them? (from the article, but why i think it is right?)\n",
    "\n",
    "Robust Correlation = (0.40 × Spearman + 0.35 × Kendall + 0.25 × Pearson)\n",
    "\n",
    "This means:\n",
    "- If all 3 agree → strong correlation\n",
    "- If they disagree → weaker average\n",
    "- Reduces noise from any single metric\n",
    "- More reproducible across datasets\n",
    "- Better for identifying true co-expression\n",
    "\n",
    "Why these exact values?\n",
    "\n",
    "0.40 × Spearman    ← HIGHEST (most established in genomics)\n",
    "   → Most used in scRNA-seq literature\n",
    "   → Industry standard for co-expression\n",
    "   → Rank-based (handles sparse data well)\n",
    "\n",
    "0.35 × Kendall     ← HIGH (very robust)\n",
    "   → More robust to outliers than Spearman\n",
    "   → Measure of concordance (pairs moving together)\n",
    "   → Slightly slower to compute but trustworthy\n",
    "\n",
    "0.25 × Pearson     ← LOWER (for validation only)\n",
    "   → Catches linear relationships\n",
    "   → Can be fooled by outliers\n",
    "   → But good as sanity check\n",
    "   → If Pearson agrees with others → strong signal\n",
    "\n",
    "\n",
    "- Kendall takes too much time and effort, i will try using BIOCOR\n",
    "\n",
    "why not the same values?\n",
    "\n",
    "Equal weighting = treating all metrics as equally trustworthy\n",
    "Weighted = reflecting our knowledge:\n",
    "   - Spearman most reliable for scRNA-seq\n",
    "   - Kendall very robust\n",
    "   - Pearson useful but less robust\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIAGNOSTIC: Check correlation matrices\n",
    "print(f'{\"CORRELATION MATRIX DIAGNOSTICS\":^80}\\n')\n",
    "\n",
    "for group_name, corr_dict in robust_correlations.items():\n",
    "    spearman = corr_dict['spearman']\n",
    "    kendall = corr_dict['kendall']\n",
    "    pearson = corr_dict['pearson']\n",
    "    \n",
    "    print(f'{group_name}:')\n",
    "    print(f'  Spearman shape: {spearman.shape} (should be 3000×3000)')\n",
    "    print(f'  Spearman diagonal: {np.diag(spearman)[:5]} (should be all 1.0)')\n",
    "    print(f'  Spearman range: [{spearman.min():.4f}, {spearman.max():.4f}] (should be -1 to 1)')\n",
    "    print(f'  Spearman NaNs: {np.isnan(spearman).sum()} (should be 0)')\n",
    "    print(f'  Kendall range: [{kendall.min():.4f}, {kendall.max():.4f}]')\n",
    "    print(f'  Pearson range: [{pearson.min():.4f}, {pearson.max():.4f}]')\n",
    "    print()\n",
    "\n",
    "print(f'OK All correlation matrices look good!\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc14a5b",
   "metadata": {},
   "source": [
    "## SUBSECTION 3.3: Metric Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f72906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'generating metric comparison visualizations', end=' ', flush=True)\n",
    "\n",
    "#selected first group for visualization\n",
    "group_name = list(robust_correlations.keys())[0]\n",
    "corrs = robust_correlations[group_name]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "#spearman vs kendall\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(corrs['spearman'].flatten(), corrs['kendall'].flatten(), alpha=0.1, s=1)\n",
    "ax.set_xlabel('Spearman')\n",
    "ax.set_ylabel('Kendall')\n",
    "ax.set_title(f'{group_name}: Spearman vs Kendall')\n",
    "\n",
    "#spearman vs pearson\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(corrs['spearman'].flatten(), corrs['pearson'].flatten(), alpha=0.1, s=1)\n",
    "ax.set_xlabel('Spearman')\n",
    "ax.set_ylabel('Pearson')\n",
    "ax.set_title(f'{group_name}: Spearman vs Pearson')\n",
    "\n",
    "#distribution comparison\n",
    "ax = axes[1, 0]\n",
    "ax.hist(corrs['spearman'].flatten(), bins=100, alpha=0.5, label='Spearman')\n",
    "ax.hist(corrs['kendall'].flatten(), bins=100, alpha=0.5, label='Kendall')\n",
    "ax.hist(corrs['pearson'].flatten(), bins=100, alpha=0.5, label='Pearson')\n",
    "ax.set_xlabel('Correlation Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'{group_name}: Correlation Distributions')\n",
    "ax.legend()\n",
    "\n",
    "#robust vs spearman\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(corrs['spearman'].flatten(), corrs['robust'].flatten(), alpha=0.1, s=1)\n",
    "ax.set_xlabel('Spearman')\n",
    "ax.set_ylabel('Robust (Weighted)')\n",
    "ax.set_title(f'{group_name}: Robust vs Spearman')\n",
    "\n",
    "plt.tight_layout()\n",
    "out_file = PHASE3_DIR / 'correlation_metrics' / 'metric_comparison.png'\n",
    "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f'OK')\n",
    "\n",
    "print(f'\\nOK Section 3 complete\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CHECKPOINT: Phase 3 Progress')\n",
    "\n",
    "print(f' SECTION 1: Setup & Data Integration - COMPLETE')\n",
    "print(f'   - 6 datasets loaded and preprocessed')\n",
    "print(f'   - Log normalization + HVG filtering applied\\n')\n",
    "\n",
    "print(f' SECTION 2: Exploratory Analysis - COMPLETE')\n",
    "print(f'   - Sample quality assessment')\n",
    "print(f'   - Gene detection & zero inflation')\n",
    "print(f'   - PCA & dimensionality reduction')\n",
    "print(f'   - Expression distribution')\n",
    "print(f'   - HVG overlap analysis\\n')\n",
    "\n",
    "print(f' SECTION 3: Robust Correlations - COMPLETE')\n",
    "print(f'   - Spearman, Kendall, Pearson calculated')\n",
    "print(f'   - Weighted robust averaging (40/35/25)')\n",
    "print(f'   - Metric comparison visualizations\\n')\n",
    "\n",
    "print(f' NEXT: Soft power detection (SECTION 4)')\n",
    "print(f' THEN: Network construction (SECTION 5)')\n",
    "print(f' THEN: Gene enrichment (SECTION 6)')\n",
    "print(f' THEN: Expression directionality (SECTION 7)')\n",
    "print(f' THEN: Hub analysis (SECTION 8)')\n",
    "print(f' THEN: Network comparison (SECTION 9)')\n",
    "print(f' THEN: Publication figures (SECTION 10)')\n",
    "print(f' THEN: Therapeutic targets (SECTION 11)\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast_cancer_scrnaseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
